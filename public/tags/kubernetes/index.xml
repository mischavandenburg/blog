<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kubernetes on Mischa van den Burg</title>
    <link>https://mischavandenburg.com/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on Mischa van den Burg</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 14 Nov 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://mischavandenburg.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Talos Homelab Upgrade Guide October</title>
      <link>https://mischavandenburg.com/zet/talos-homelab-upgrade-guide-october/</link>
      <pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/talos-homelab-upgrade-guide-october/</guid>
      <description>[!IMPORTANT] Remember to create a custom image including iscsi-tools when using the Synology CSI driver
Set up CLI environment and set databases to maintenance mode export TALOS_CP=&amp;#34;192.168.100.107&amp;#34; export TALOS_W1=&amp;#34;192.168.100.245&amp;#34; export TALOS_W2=&amp;#34;192.168.100.60&amp;#34; export TALOS_W3=&amp;#34;192.168.100.228&amp;#34; # Set databases to maintenance mode k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters. Only needed for the control plane node.
Upgrade path from 1.7.5 to 1.7.7 from 1.7.7 to 1.</description>
      <content:encoded><![CDATA[<blockquote>
<p>[!IMPORTANT]
Remember to create a custom image including iscsi-tools when using the Synology CSI driver</p>
</blockquote>
<h3 id="set-up-cli-environment-and-set-databases-to-maintenance-mode">Set up CLI environment and set databases to maintenance mode</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_CP</span><span class="o">=</span><span class="s2">&#34;192.168.100.107&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_W1</span><span class="o">=</span><span class="s2">&#34;192.168.100.245&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_W2</span><span class="o">=</span><span class="s2">&#34;192.168.100.60&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_W3</span><span class="o">=</span><span class="s2">&#34;192.168.100.228&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Set databases to maintenance mode</span>
</span></span><span class="line"><span class="cl">k cnp maintenance <span class="nb">set</span> --all-namespaces
</span></span></code></pre></div><blockquote>
<p>[!WARNING]
Use the <code>--preserve</code> flag on single-node control plane clusters.
Only needed for the control plane node.</p>
</blockquote>
<h3 id="upgrade-path">Upgrade path</h3>
<ol>
<li>from 1.7.5 to 1.7.7</li>
<li>from 1.7.7 to 1.8.2</li>
</ol>
<h3 id="upgrade-1">Upgrade 1</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_IMAGE</span><span class="o">=</span><span class="s1">&#39;factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.7&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">talosctl upgrade --preserve --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --wait --debug --nodes <span class="nv">$TALOS_W1</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --wait --debug --nodes <span class="nv">$TALOS_W2</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --wait --debug --nodes <span class="nv">$TALOS_W3</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span></code></pre></div><h3 id="upgrade-2">Upgrade 2</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_IMAGE</span><span class="o">=</span><span class="s1">&#39;factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.8.2&#39;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">talosctl upgrade --preserve --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W1</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W2</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W3</span> -e <span class="nv">$TALOS_CP</span> --image <span class="nv">$TALOS_IMAGE</span>
</span></span></code></pre></div><h2 id="kubernetes-upgrade">Kubernetes Upgrade</h2>
<blockquote>
<p>[!WARNING]
Make sure to upgrade the Talos CLI before proceeding.
&ldquo;It is advisable to use the same version of talosctl as the version of the boot media used.&rdquo; - Talos Docs</p>
</blockquote>
<h3 id="upgrade-path-1">Upgrade Path</h3>
<ol>
<li>From 1.30.3 to 1.30.5</li>
<li>From 1.30.5 to 1.31.1</li>
</ol>
<blockquote>
<p>[!NOTE]
To trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to upgrade to, such as:
talosctl &ndash;nodes <!-- raw HTML omitted --> upgrade-k8s &ndash;to 1.30.0
Note that the &ndash;nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded.
To check what will be upgraded you can run talosctl upgrade-k8s with the &ndash;dry-run flag</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k cnp maintenance <span class="nb">set</span> --all-namespaces
</span></span><span class="line"><span class="cl">kubent
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.31.1 --dry-run
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.31.1
</span></span></code></pre></div><h2 id="links">Links</h2>
<p>202411140811</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Talos Linux Upgrade Guide July</title>
      <link>https://mischavandenburg.com/zet/talos-linux-upgrade-guide-july/</link>
      <pubDate>Wed, 31 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/talos-linux-upgrade-guide-july/</guid>
      <description>I upgraded my homelab cluster using Talos Linux today.
I made a mistake and forgot to use the custom-built image using iscsi-tools.
Learning the hard way.
Here are my notes:
Talos upgrade [!IMPORTANT] Remember to create a custom image including iscsi-tools
Set up CLI environment and set databases to maintenance mode export TALOS_CP=&amp;#34;192.168.100.107&amp;#34; export TALOS_W1=&amp;#34;192.168.100.245&amp;#34; export TALOS_W2=&amp;#34;192.168.100.60&amp;#34; k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters.</description>
      <content:encoded><![CDATA[<p>I upgraded my homelab cluster using Talos Linux today.</p>
<p>I made a mistake and forgot to use the custom-built image using iscsi-tools.</p>
<p>Learning the hard way.</p>
<p>Here are my notes:</p>
<h2 id="talos-upgrade">Talos upgrade</h2>
<blockquote>
<p>[!IMPORTANT]
Remember to create a custom image including iscsi-tools</p>
</blockquote>
<h3 id="set-up-cli-environment-and-set-databases-to-maintenance-mode">Set up CLI environment and set databases to maintenance mode</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_CP</span><span class="o">=</span><span class="s2">&#34;192.168.100.107&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_W1</span><span class="o">=</span><span class="s2">&#34;192.168.100.245&#34;</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">TALOS_W2</span><span class="o">=</span><span class="s2">&#34;192.168.100.60&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">k cnp maintenance <span class="nb">set</span> --all-namespaces
</span></span></code></pre></div><blockquote>
<p>[!WARNING]
Use the <code>--preserve</code> flag on single-node control plane clusters.
Only needed for the control plane node.</p>
</blockquote>
<h3 id="upgrade-path">Upgrade path:</h3>
<ol>
<li>from 1.6.4 to 1.6.8</li>
<li>from 1.6.8 to 1.7.5</li>
</ol>
<h3 id="upgrade-1">Upgrade 1</h3>
<blockquote>
<p>[!NOTE]
I made a mistake and did not update to the custom-built image including the iscsi-tools
This caused my cluster storage to break.</p>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">talosctl upgrade --preserve --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> --image ghcr.io/siderolabs/installer:v1.6.8
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W1</span> -e <span class="nv">$TALOS_CP</span> --image ghcr.io/siderolabs/installer:v1.6.8
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W2</span> -e <span class="nv">$TALOS_CP</span> --image ghcr.io/siderolabs/installer:v1.6.8
</span></span></code></pre></div><h3 id="upgrade-2">Upgrade 2</h3>
<p>I went to <a href="https://factory.talos.dev/">https://factory.talos.dev/</a> and created a new image including iscsi-tools.</p>
<p>Everything worked fine after that.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">talosctl upgrade --preserve --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W1</span> -e <span class="nv">$TALOS_CP</span> --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5
</span></span><span class="line"><span class="cl">talosctl upgrade --nodes <span class="nv">$TALOS_W2</span> -e <span class="nv">$TALOS_CP</span> --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5
</span></span></code></pre></div><h2 id="kubernetes-upgrade">Kubernetes Upgrade</h2>
<blockquote>
<p>[!WARNING]
Make sure to upgrade the Talos CLI before proceeding.
&ldquo;It is advisable to use the same version of talosctl as the version of the boot media used.&rdquo; - Talos Docs</p>
</blockquote>
<h3 id="upgrade-path-1">Upgrade Path</h3>
<ol>
<li>From 1.29.0 to 1.29.3</li>
<li>From 1.29.3 to 1.30.3</li>
</ol>
<blockquote>
<p>[!NOTE]</p>
</blockquote>
<blockquote>
<p>[!NOTE]
To trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to upgrade to, such as:
talosctl &ndash;nodes <!-- raw HTML omitted --> upgrade-k8s &ndash;to 1.30.0
Note that the &ndash;nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded.
To check what will be upgraded you can run talosctl upgrade-k8s with the &ndash;dry-run flag</p>
</blockquote>
<h3 id="round-1">Round 1</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k cnp maintenance <span class="nb">set</span> --all-namespaces
</span></span><span class="line"><span class="cl">kubent
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.29.3 --dry-run
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.29.3
</span></span></code></pre></div><h3 id="round-2">Round 2</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k cnp maintenance <span class="nb">set</span> --all-namespaces
</span></span><span class="line"><span class="cl">kubent
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.30.3 --dry-run
</span></span><span class="line"><span class="cl">talosctl --nodes <span class="nv">$TALOS_CP</span> -e <span class="nv">$TALOS_CP</span> upgrade-k8s --to 1.30.3
</span></span></code></pre></div><h2 id="links">Links:</h2>
<p>202407311407</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Got my CKS</title>
      <link>https://mischavandenburg.com/zet/got-my-cks/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/got-my-cks/</guid>
      <description>Just realized I never posted on the blog that I achieved the CKS a month ago.
I even created a study guide for it:
Links: 202406152006</description>
      <content:encoded><![CDATA[<p>Just realized I never posted on the blog that I achieved the CKS a month ago.</p>
<p>I even created a study guide for it:</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/_l232KiJHNA?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h2 id="links">Links:</h2>
<p>202406152006</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Alleviating Confusion About The To Field In Network Policies</title>
      <link>https://mischavandenburg.com/zet/alleviating-confusion-about-the-to-field-in-network-policies/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/alleviating-confusion-about-the-to-field-in-network-policies/</guid>
      <description>When solving a killercoda challenge I ran into some confusion. Even though my solution worked, there was a difference which I wanted to get clear on.
I wrote this:
apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np namespace: space1 spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: space2 - to: ports: - protocol: TCP port: 53 - protocol: UDP port: 53 But the provided course solution was this:</description>
      <content:encoded><![CDATA[<p>When solving a killercoda challenge I ran into some confusion. Even though my solution worked, there was a difference which I wanted to get clear on.</p>
<p>I wrote this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">NetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">np</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">space1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">Egress</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">to</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">namespaceSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">kubernetes.io/metadata.name</span><span class="p">:</span><span class="w"> </span><span class="l">space2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">to</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">53</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">53</span><span class="w">
</span></span></span></code></pre></div><p>But the provided course solution was this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">networking.k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">NetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">np</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">space1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">podSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">policyTypes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="l">Egress</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">to</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">     </span>- <span class="nt">namespaceSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">         </span><span class="nt">kubernetes.io/metadata.name</span><span class="p">:</span><span class="w"> </span><span class="l">space2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">53</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">TCP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">53</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span></code></pre></div><p>Do you see the difference? I used the <code>to:</code> field before the <code>ports:</code>  field.</p>
<p>My solution was correct, but it had me quite confused, why wouldn&rsquo;t they write <code>to:</code> here as well?</p>
<p>After some further digging I&rsquo;ve finally understood that the <code>to:</code> field is only necessary when you apply a selector. If the <code>to:</code> field is omitted, it means that it will apply to any destination. So in my solution, the <code>to:</code> field doesn&rsquo;t achieve anything because I&rsquo;m not applying any selector. It can therefore be omitted.</p>
<p>Each element under the <code>egress:</code> array is a separate rule and they are combined to reach the end result of allowing egress traffic to namespace space2, and UDP/TCP traffic to anywhere on port 53.</p>
<p>In this case, omitting the <code>to:</code> field makes sense because DNS is generally not limited to only one namespace. However, if you&rsquo;re only using the internal Kubernetes DNS you might want to limit it to the kube-system namespace. I&rsquo;ve done this in a CiliumNetworkPolicy in my homelab:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">commafeed-app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">commafeed</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">policy-type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;app&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">postgresql-operator-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">kube-dns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;53&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">dns</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span>- <span class="nt">matchPattern</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">world</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;80&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;7844&#34;</span><span class="w">
</span></span></span></code></pre></div><h2 id="links">Links:</h2>
<p><strong>part of</strong>:: [[Network Policies]]
[[CKS]]
[[networking-computers]]</p>
<p>202403250845</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>I&#39;m better at Network Policies than I thought</title>
      <link>https://mischavandenburg.com/zet/im-better-at-network-policies-than-i-thought/</link>
      <pubDate>Mon, 25 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/im-better-at-network-policies-than-i-thought/</guid>
      <description>There are two things which I&amp;rsquo;m dreading most for the CKS exam: writing Network Policies and configuring RBAC under heavy time pressure.
I&amp;rsquo;ve heard from colleagues that the exam has many questions of this nature, so it will be a good way of scoring points. However, regardless of the exam, the fact that I&amp;rsquo;m dreading these topics also shows that I&amp;rsquo;m not familiar enough with them and therefore this is a point where I should improve.</description>
      <content:encoded><![CDATA[<p>There are two things which I&rsquo;m dreading most for the CKS exam: writing Network Policies and configuring RBAC under heavy time pressure.</p>
<p>I&rsquo;ve heard from colleagues that the exam has many questions of this nature, so it will be a good way of scoring points. However, regardless of the exam, the fact that I&rsquo;m dreading these topics also shows that I&rsquo;m not familiar enough with them and therefore this is a point where I should improve. It&rsquo;s not only about the exam, but also my growth of expertise as a Kubernetes Engineer.</p>
<p>With the help of <a href="https://networkpolicy.io/">https://networkpolicy.io/</a> I&rsquo;ve created CiliumNetworkpolicies for my homelab and the publically available stuff is pretty well secured. It&rsquo;s not that I don&rsquo;t understand them, but I haven&rsquo;t practiced them enough to do them without having to spend a lot of time or relying on external tools. And the time pressure is the biggest challenge for the CKA / CKS exams.</p>
<p>For the weeks in my preparation of the CKS exam I will do Network Policy and RBAC exercises on Killercoda every day. They have multiple scenarios for the CKS, but also CKA and CKAD so there are a few things I can practice every day. I&rsquo;m told that when I can solve these without problems, I know enough to do the exam questions without problem.</p>
<h2 id="better-than-i-thought">Better than I thought</h2>
<p>I <a href="https://youtube.com/live/1NJTM2ocdtM?feature=share">streamed my first 3 hour study session live</a> and after I worked through the video module about Network Policies I went ahead and tackled the practical scenarios.</p>
<p>To my great surprise I was able to solve them without much trouble! I was slow and needed to talk myself through the code a few times, but I did manage to solve all of the challenges for the CKS on Killercoda. There was never a point where I was unsure of what to do, or a problem I couldn&rsquo;t fix.</p>
<p>This was quite an eye opening experience because I had expected some weeks of struggle with this. Perhaps I&rsquo;m better at this stuff than I give myself credit for. In any case, I am still going to do daily practice with these scenarios and possibly thinking of a few of my own when I run out of Killercoda practice.</p>
<p>Even though I managed to solve the challenges during this first round, being in an exam under heavy time pressure and feeling nervous is going to be a completely different experience. So I want to get this down to the point where I hardly have to think about it any more, I need to master these questions so I can spend my time on the big questions.</p>
<p>This experience has me wondering how RBAC is going to go. We&rsquo;ll find out soon enough!</p>
<h2 id="links">Links:</h2>
<p>202403250703</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting my CKS studies</title>
      <link>https://mischavandenburg.com/zet/starting-cks-studies/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-cks-studies/</guid>
      <description>It seems like I will have a couple of hours a day which I can allot to study in the coming month. This will be the perfect moment to start working on a goal I&amp;rsquo;ve had for a long time: getting my Certified Kubernetes Security Specialist certification.
t&amp;rsquo;s not going to be easy. I&amp;rsquo;m quite intimidated by all of the new things I&amp;rsquo;ll have to learn. But that&amp;rsquo;s a good sign.</description>
      <content:encoded><![CDATA[<p>It seems like I will have a couple of hours a day which I can allot to study in the coming month. This will be the perfect moment to start working on a goal I&rsquo;ve had for a long time: getting my Certified Kubernetes Security Specialist certification.</p>
<p>t&rsquo;s not going to be easy. I&rsquo;m quite intimidated by all of the new things I&rsquo;ll have to learn. But that&rsquo;s a good sign. It&rsquo;s the way it works. Everybody would be doing this if it was easy, and I can only grow by doing hard things that seem intimidating at first.</p>
<h2 id="game-plan">Game Plan</h2>
<ol>
<li>Work through this 11 hour course:</li>
</ol>
<p><a href="https://www.youtube.com/watch?v=d9xfB5qaOfg&amp;t=3921s&amp;pp=ygUKY2tzIGNvdXJzZQ%3D%3D">https://www.youtube.com/watch?v=d9xfB5qaOfg&amp;t=3921s&amp;pp=ygUKY2tzIGNvdXJzZQ%3D%3D</a></p>
<p>Now that I have a good homelab setup, I can also put things into practice much more conveniently. So, I&rsquo;ll make a rule for myself not to move to new chapters of the course until I have done extensive practice for the module and I feel I have it &ldquo;in the fingers&rdquo;.</p>
<ol start="2">
<li>Do practical exercises on RBAC and Network Policies every single day on Killercoda. These are my weakest points currently, and they will improve my security awareness on my own clusters but they will also be good ways to score points on the exam</li>
</ol>
<p>I&rsquo;m also planning to do live streams of my studies. One thing that kept me from streaming is that I cannot stream when I am watching courses. But as this course is published on YouTube, I won&rsquo;t get into copyright trouble. Secondly, I always thought that the chat would be distracting. But apparently non-interactive streaming is also a thing, and some people are content with just watching someone do their thing.</p>
<p>We&rsquo;ll see, if anything it will be an interesting way to document my own learning on my YouTube channel. And it may also lead to surprises. Sometimes people in the chat are very helpful and they might even help me understand things better.</p>
<p>Let&rsquo;s go!</p>
<h2 id="links">Links:</h2>
<p>202403241903</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Comparing akv2k8s with Azure Key Vault Provider for Secret Store CSI Driver</title>
      <link>https://mischavandenburg.com/zet/articles/akv2k8s-azure-key-vault-csi-analysis/</link>
      <pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/akv2k8s-azure-key-vault-csi-analysis/</guid>
      <description>In a recent analysis, I explored two notable solutions for synchronizing secrets from Azure Key Vaults to AKS (Azure Kubernetes Service) clusters: akv2k8s and the Azure Key Vault Provider for the Secret Store CSI Driver. Here, I present my findings and recommendations based on the functionality, maintenance requirements, and integration capabilities of these tools.
Akv2k8s, maintained by Sparebanken, is an open-source tool designed for the synchronization of secrets. Being dependent on an external tool for Kubernetes secrets synchronization is an undesirable situation and poses several challenges.</description>
      <content:encoded><![CDATA[<p>In a recent analysis, I explored two notable solutions for synchronizing secrets from Azure Key Vaults to AKS (Azure Kubernetes Service) clusters: akv2k8s and the Azure Key Vault Provider for the Secret Store CSI Driver. Here, I present my findings and recommendations based on the functionality, maintenance requirements, and integration capabilities of these tools.</p>
<p>Akv2k8s, maintained by Sparebanken, is an open-source tool designed for the synchronization of secrets. Being dependent on an external tool for Kubernetes secrets synchronization is an undesirable situation and poses several challenges. Notably, the latest version of akv2k8s has been problematic, especially concerning the deployment of Postgres databases on our AKS clusters using the EDB operator. Akv2k8s alters the SecurityContext of pods in a way that causes them to fail.</p>
<p>Furthermore, upgrading akv2k8s to the latest version necessitates a transition to Workload Identity due to the deprecation of aad-pod-identity, a move that promises to be complex. Conversely, the Azure Key Vault Provider, directly offered by Microsoft, allows continued use of Managed Identities for authentication to Key Vaults, simplifying the integration process.</p>
<p><a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/identity-access-modes/">https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/identity-access-modes/</a></p>
<h1 id="azure-key-vault-provider-for-secret-store-csi-driver">Azure Key Vault Provider for Secret Store CSI Driver</h1>
<p>The Azure Key Vault Provider for Secret Store CSI Driver is the solution for syncing secrets offered by Microsoft Azure. It is based on the <a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">kubernetes-sigs/secrets-store-csi-driver</a>.</p>
<h1 id="installation--maintenance">Installation &amp; Maintenance</h1>
<p>The addon is installed with an Azure CLI command or from the portal for existing clusters. The addon can also be enabled from code when deploying a new cluster.</p>
<p>The addon is automatically updated when an AKS upgrade to a new minor version is performed. <strong>Maintenance of the secret synching solution will not be required anymore and is handled by Azure Kubernetes Service.</strong></p>
<p>If an emergency update is necessary, outside of the normal upgrade schedule, it can be updated with the Azure CLI as follows:</p>
<p><code>az aks addon update --addon virtual-node --name MyManagedCluster --resource-group MyResourceGroup --subnet-name VirtualNodeSubnet</code></p>
<p>Addons can also be patched by upgrading to the latest AKS node image.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/integrations">https://learn.microsoft.com/en-us/azure/aks/integrations</a>
<a href="https://learn.microsoft.com/en-gb/azure/aks/node-image-upgrade">https://learn.microsoft.com/en-gb/azure/aks/node-image-upgrade</a></p>
<h1 id="comparison">Comparison</h1>
<h2 id="benefits-of-using-the-official-solution">Benefits Of Using The Official Solution</h2>
<ul>
<li>The tool is built and maintained by Microsoft and an integral part of the AKS offering</li>
<li>Authentication using a managed identity is supported out of the box</li>
<li>Supports RBAC based authentication on Key Vaults</li>
<li>Automatically updated when performing AKS upgrades</li>
<li>One less component for us to maintain</li>
<li>Can run alongside akv2k8s</li>
<li>We can implement automatic secret rotation</li>
<li>We are not dependent on an external project for our secrets management</li>
<li>Does not require an initcontainer to inject secrets</li>
</ul>
<h2 id="cons">Cons</h2>
<ul>
<li>Will take work to migrate all applications</li>
</ul>
<h1 id="migration-guide">Migration Guide</h1>
<p>Steps that describe how to migrate from the use of akv2k8s to the CSI drivers.</p>
<ul>
<li>enable the addon using the Azure CLI</li>
</ul>
<p><code>az aks enable-addons --addons azure-keyvault-secrets-provider --name aks-vo-dcp-dev --resource-group rg-vo-dcp-aks-dev-weu-001</code></p>
<p>This does not restart any existing pods or nodes, thus it does not impact the running setup. akv2k8s and the AKV Provider can run simultaneously on the cluster and the workloads can be migrated one by one.</p>
<p>Enabling the addon creates a Managed Identity. This Managed Identity needs to get the Key Vault Administrator role on the desired Key Vault to be able to retrieve Secrets.</p>
<p>To import the secrets, a SecretProviderClass resource is created:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.x-k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">SecretProviderClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">azure-sync</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l">azure</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">usePodIdentity</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;false&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">useVMManagedIdentity</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;true&#34;</span><span class="w"> </span><span class="c"># Set to true for using managed identity</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">userAssignedIdentityID</span><span class="p">:</span><span class="w"> </span><span class="l">ceb19a0a-941a-4f33-839d-aeace8ffe205</span><span class="w"> </span><span class="c"># Set the clientID of the user-assigned managed identity to use</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">KeyvaultName</span><span class="p">:</span><span class="w"> </span><span class="l">mischakv01</span><span class="w"> </span><span class="c"># Set to the name of your Key Vault</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">cloudName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;&#34;</span><span class="w"> </span><span class="c"># [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">objects</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      array:
</span></span></span><span class="line"><span class="cl"><span class="sd">        - |
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectName: ExampleSecret
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectType: secret              # object types: secret, key, or cert
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectVersion: &#34;&#34;               # [OPTIONAL] object versions, default to latest if empty
</span></span></span><span class="line"><span class="cl"><span class="sd">        - |
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectName: env-secret
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectType: secret              # object types: secret, key, or cert</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tenantId</span><span class="p">:</span><span class="w"> </span><span class="l">d62ada1b-ca42-4fe2-b9e7-ceb843af0ad</span><span class="w"> </span><span class="c"># The tenant ID of the key vault</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">secretObjects</span><span class="p">:</span><span class="w"> </span><span class="c"># [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">username</span><span class="w"> </span><span class="c"># data field to populate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">objectName</span><span class="p">:</span><span class="w"> </span><span class="l">env-secret</span><span class="w"> </span><span class="c"># name of the mounted content to sync; this could be the object name or the object alias</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">foosecret</span><span class="w"> </span><span class="c"># name of the Kubernetes secret object</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Opaque</span><span class="w"> </span><span class="c"># type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls)</span><span class="w">
</span></span></span></code></pre></div><p>The secret is then mounted in to the container during deployment.</p>
<p>See this pod example:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox-secrets-store-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">registry.k8s.io/e2e-test-images/busybox:1.29-1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;/bin/sleep&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;10000&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/mnt/secrets-store&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SECRET_USERNAME</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">foosecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">username</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">csi</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.k8s.io</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeAttributes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">secretProviderClass</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;azure-sync&#34;</span><span class="w">
</span></span></span></code></pre></div><p>In this example the secret is accessible as a text file at /mnt/secrets-store inside the container. It is also made available as an environment variable SECRET_USERNAME.</p>
<p>The applications on our platform are already using environment variables when using akv2k8s.</p>
<p>The application helm charts will need to be updated with the new SecredProviderClass, but the applications themselves don&rsquo;t need any changes. They will need to be redeployed with the updated configuration though.</p>
<p>Another note is that the secret must always be mounted even it is only used as an environment variable.</p>
<h1 id="conclusions">Conclusions</h1>
<p>Migrating to the Azure Key Vault Provider for Secret Store CSI Driver presents no disadvantages. There will be one less component to maintain, and it will always be compatible with the AKS offering.</p>
<p>It will require some work to convert the current workloads to use the CSI driver, but this is a one-time action which will not affect the applications while they are running. On the long term, this will pay itself back, because there is one less component to maintain.</p>
<h2 id="links">Links:</h2>
<p>202403111503</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Gateway API &amp; Azure Application Gateway for Containers</title>
      <link>https://mischavandenburg.com/zet/articles/gateway-api-application-gateway-for-containers/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/gateway-api-application-gateway-for-containers/</guid>
      <description>This document is the result of my research into the Gateway API. It aims to briefly describe the Gateway API for Kubernetes, a typical implementation of ingress traffic using NGINX in AKS and how this setup could benefit from implementing the Gateway API.
Introduction Gateway API is an official Kubernetes project focused on L4 and L7 routing in Kubernetes. This project represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs.</description>
      <content:encoded><![CDATA[<p>This document is the result of my research into the Gateway API. It aims to briefly describe the Gateway API for Kubernetes, a typical implementation of ingress traffic using NGINX in AKS and how this setup could benefit from implementing the Gateway API.</p>
<h1 id="introduction">Introduction</h1>
<p>Gateway API is an official Kubernetes project focused on L4 and L7 routing in Kubernetes. This project represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs. From the outset, it has been designed to be generic, expressive, and role-oriented.</p>
<p>The overall resource model focuses on 3 separate personas and corresponding resources that they are expected to manage:</p>
<p><img loading="lazy" src="/ga1.png" type="" alt=""  /></p>
<p>Up until now ingress traffic to Kubernetes clusters was handled by the Ingress resource. Although the Ingress API will not be deprecated, the Gateway API is where all the development and innovation happens in the Kubernetes project.</p>
<blockquote>
<p>Gateway API is the successor to the Ingress API.</p>
<p>-Kubernetes documentation</p>
</blockquote>
<h1 id="a-typical-nginx-ingress-solution">A Typical NGINX Ingress Solution</h1>
<p>Ingress traffic is typically implemented as follows.</p>
<ol>
<li>An AKS cluster is provisioned in its own resource group and added to an existing virtual network. Usually each subscription has its own virtual network.</li>
<li>We use the NGINX Ingress Controller which is deployed through ArgoCD</li>
<li>The NGINX Ingress controller creates a Kubernetes LoadBalancer resource and exposes the cluster on port 80 and 443, as you can see below:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Switched to context <span class="s2">&#34;aks-gwa-dev-admin&#34;</span>.
</span></span><span class="line"><span class="cl">mischa@mac-beast:~
</span></span><span class="line"><span class="cl"><span class="o">(</span>ins<span class="o">)</span>$ kn
</span></span><span class="line"><span class="cl">Context <span class="s2">&#34;aks-gwa-dev-admin&#34;</span> modified.
</span></span><span class="line"><span class="cl">Active namespace is <span class="s2">&#34;nginx-ingress&#34;</span>.
</span></span><span class="line"><span class="cl">mischa@mac-beast:~
</span></span><span class="line"><span class="cl"><span class="o">(</span>ins<span class="o">)</span>$ k get svc
</span></span><span class="line"><span class="cl">NAME                                                    TYPE           CLUSTER-IP        EXTERNAL-IP    PORT<span class="o">(</span>S<span class="o">)</span>                      AGE
</span></span><span class="line"><span class="cl">nginx-ingress-ingress-nginx-controller                  LoadBalancer   192.168.10.17     52.137.26.47   80:30974/TCP,443:30474/TCP   595d
</span></span><span class="line"><span class="cl">nginx-ingress-ingress-nginx-controller-admission        ClusterIP      192.168.51.96     &lt;none&gt;         443/TCP                      595d
</span></span><span class="line"><span class="cl">nginx-ingress-ingress-nginx-controller-metrics          ClusterIP      192.168.214.164   &lt;none&gt;         10254/TCP                    417d
</span></span><span class="line"><span class="cl">nginx-ingress-ingress-nginx-vo-custom-default-backend   ClusterIP      192.168.226.137   &lt;none&gt;         80/TCP                       595d
</span></span><span class="line"><span class="cl">mischa@mac-beast:~
</span></span><span class="line"><span class="cl"><span class="o">(</span>ins<span class="o">)</span>$
</span></span></code></pre></div><ol start="4">
<li>When a LoadBalancer resource is created in the cluster, AKS creates an Azure Load Balancer in the resource group of the AKS cluster. This Azure Load Balancer receives a public or internal IP address dependent on the Ingress Controller configuration. You can have a normal/external controller and an internal controller which doesn&rsquo;t receive a public IP address.</li>
<li>An Ingress resource is created in Kubernetes to configure the FQDN and TLS settings. In the Ingress resource, a label selector is configured to route the traffic to the correct backend service, which will route the traffic to the correct pods.</li>
<li>A DNS record is created and linked to the IP of the Azure Load Balancer which was provisioned by the Ingress Controller</li>
<li>When the FQDN is approached in the browser, the Azure DNS zone will forward the request to the Azure Load Balancer. The NGINX Ingress Controller will then route the traffic to the backend pods which were selected in the Ingress resource in Kubernetes.</li>
</ol>
<h2 id="disadvantages-of-this-situation">Disadvantages Of This Situation</h2>
<ul>
<li>Ingress API has less functionality</li>
<li>Azure Loadbalancer only works on Layer 4, no layer 7 routing</li>
<li>NGINX Ingress Controller maintenance and configuration</li>
</ul>
<h1 id="benefits-of-gateway-api">Benefits of Gateway API</h1>
<ul>
<li>natively supports traffic weighting, no extra annotations needed on ingress controller
<ul>
<li>example: sending 50% of the traffic to an application instance with a feature flag enabled</li>
</ul>
</li>
<li>natively supports header-based matching</li>
<li>traffic can be routed to other resources outside of the cluster or other clusters</li>
<li>extensible: allows for custom resources to be linked at various layers of the API, allowing customization</li>
<li>simplifies networking configuration</li>
<li>aids in transition towards service mesh</li>
<li>set up to support role based division of labour. The platform team sets up the infrastructure and Gateway itself, and the developer teams can add routing configuration that suits their needs</li>
<li>enables the developer teams to gain full control over their network configuration through GitOps</li>
</ul>
<p>Gateway API simplifies networking configuration by standardizing the way we define network rules. Thus, Gateway API aids in the transition from ingress towards service mesh. When the Gateway API is implemented, it will be much easier to adopt a service mesh networking architecture, or to migrate between service meshes.</p>
<p>Another advantage is that it can enable teams to implement specific header-based routing and traffic weighting between backend pools</p>
<p>A more specific list of Gateway API features:</p>
<ul>
<li>Header rewrite</li>
<li>HTTPS traffic management:</li>
<li>SSL termination</li>
<li>End to End SSL</li>
<li>Ingress and Gateway API support</li>
<li>Layer 7 HTTP/HTTPS request forwarding based on prefix/exact match on:
<ul>
<li>Hostname</li>
<li>Path</li>
<li>Header</li>
<li>Query string</li>
<li>Methods</li>
<li>Ports (80/443)</li>
</ul>
</li>
<li>Mutual Authentication (mTLS) to backend target</li>
<li>Traffic Splitting / weighted round robin</li>
<li>TLS Policies</li>
<li>URL rewrite</li>
</ul>
<h1 id="implementations">Implementations</h1>
<p>The Gateway API can be implemented <a href="https://gateway-api.sigs.k8s.io/implementations/">through various controllers</a>. Some notable implementations:</p>
<p><a href="https://github.com/nginxinc/nginx-gateway-fabric">NGINX Gateway Fabric (GA)</a></p>
<p><a href="https://doc.traefik.io/traefik/routing/providers/kubernetes-gateway/">Traefik (alpha)</a></p>
<p><a href="https://cilium.io/">Cilium (beta)</a></p>
<p>These will all require an extra controller or even a CNI to be installed.</p>
<p>However, with the release of <a href="https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/overview">Application Gateway for Containers</a>, Azure also offers a native implementation of the Gateway API in Azure Kubernetes Service.</p>
<h1 id="application-gateway-for-containers">Application Gateway for Containers</h1>
<p>The implementation of Gateway API through Azure&rsquo;s solution is rather straightforward since we are already running on the Azure platform.</p>
<p>The AG for Containers is a cluster-specific resource which deploys two resources in Azure: a frontend and an association. The frontend is an entry point where traffic is received in the form of an FQDN. The association is the entry to a virtual network. An association is a 1:1 mapping of an association resource to an Azure Subnet that has been delegated.</p>
<p>Each Application Gateway for Containers frontend provides a generated Fully Qualified Domain Name managed by Azure. The FQDN may be used as-is or customers may opt to mask the FQDN with a CNAME record.</p>
<p>Before a client sends a request to Application Gateway for Containers, the client resolves a CNAME that points to the frontend&rsquo;s FQDN; or the client may directly resolve the FQDN provided by Application Gateway for Containers by using a DNS server.</p>
<p>When the client initiates the request, the DNS name specified is passed as a host header to Application Gateway for Containers on the defined frontend.</p>
<p><img loading="lazy" src="/ga2.png" type="" alt=""  /></p>
<p>Image from Microsoft Documentation. I don&rsquo;t own this image.</p>
<h2 id="gateway-and-route-setup">Gateway and Route Setup</h2>
<p>A set of routing rules evaluates how the request for that hostname should be initiated to a defined backend target.</p>
<p>These resources are provided from within the cluster by creating a gateway with a Kubernetes manifest:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">kubectl apply -f - &lt;&lt;EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">gateway.networking.k8s.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Gateway</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">gateway-01</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">test-infra</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">alb.networking.azure.io/alb-namespace</span><span class="p">:</span><span class="w"> </span><span class="l">alb-test-infra</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">alb.networking.azure.io/alb-name</span><span class="p">:</span><span class="w"> </span><span class="l">alb-test</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">gatewayClassName</span><span class="p">:</span><span class="w"> </span><span class="l">azure-alb-external</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">listeners</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">http</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">80</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">HTTP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">allowedRoutes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">namespaces</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">from</span><span class="p">:</span><span class="w"> </span><span class="l">Same</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">EOF</span><span class="w">
</span></span></span></code></pre></div><p>When the gateway is created, the AG for Containers will create the required frontend and association.</p>
<p>Then we can create a route that connects the gateway to the backend pods of our application. This example shows how easy it is to weight traffic 50/50 to two backends:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="l">kubectl apply -f - &lt;&lt;EOF</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">gateway.networking.k8s.io/v1beta1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">HTTPRoute</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">traffic-split-route</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">test-infra</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">parentRefs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">gateway-01</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">backendRefs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">backend-v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">backend-v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="m">8080</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="m">50</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="l">EOF</span><span class="w">
</span></span></span></code></pre></div><p><img loading="lazy" src="/ga3.png" type="" alt=""  /></p>
<p>Image from Microsoft Documentation. I don&rsquo;t own this image.</p>
<h3 id="azure-load-balancer-controller">Azure Load Balancer Controller</h3>
<p>In order to enable these resources on the cluster, the Azure Load Balancer controller needs to be installed via Helm. When it is connected to a Managed Identity it is able to provision the required resources in Azure.</p>
<p>There is an alternative deployment mode where the Azure resources (frontends and associations) can be provisioned through Terraform or Bicep and referred to from within the cluster. However, I think that it is best to deploy these resources from within the cluster because it enables the application teams to get full control over their traffic based on the GitOps workflow that they are already used to working with.</p>
<h2 id="advantages-of-the-azure-application-gateway-for-containers">Advantages of the Azure Application Gateway for Containers</h2>
<ul>
<li>Natively supported for AKS on the Azure platform</li>
<li>Microsoft support and SLA</li>
<li>Increased performance, offering near real-time updates to add or move pods, routes, and probes</li>
<li>Does not need any configuration on the controller</li>
<li>Autoscaling</li>
<li>Availability zone resiliency</li>
<li>Azure resources can be provisioned from within the cluster</li>
<li>The FQDN can be linked to the DNS zone using <a href="https://github.com/kubernetes-sigs/external-dns">External DNS</a>, Azure Front Door or Azure DNS zones</li>
<li>Will likely be integrated into the AKS offering as an extension, which means we won&rsquo;t have to maintain the controller</li>
<li>Easier to maintain because configuration is tailored to AKS out of the box</li>
</ul>
<h2 id="disadvantages-of-the-azure-application-gateway-for-containers">Disadvantages of the Azure Application Gateway for Containers</h2>
<p>I don&rsquo;t see any disadvantages over using the AG for Containers in terms of technical implementation. In fact, implementing the AG for Containers will be the most straightforward and easiest option. However, we must consider the price. As the AG for containers does not currently have WAF functionality, the premium NGINX Ingress Controller offering could be considered in the price.</p>
<p>Using NGINX or Traefik would still require an Azure LoadBalancer resource to provision an internal and/or external IP. When making a pricing comparison, the following things should be considered when using a different solution:</p>
<ul>
<li>cost of Azure Load Balancer (and the reduncany of the load balancer)</li>
<li>cost of man hours needed to learn a new controller such as Traefik or NGINX</li>
<li>time required to upgrade the controller and reading up on release notes and changelogs</li>
<li>potential licence costs when adopting enterprise usage</li>
</ul>
<h2 id="lifecycle-management">Lifecycle management</h2>
<p>No matter which implementation we choose, the lifecycle management of the controller will at this point still be with us. When using the Azure Application Gateway for containers, it is installed with helm and updated through helm. We can use our ArgoCD setup for this. The same applies for different solutions such as traefik or nginx.</p>
<p>However, I expect that the ALB controller will be integrated within AKS in the future and that would remove the need for lifecycle management for the ALB controller.</p>
<h1 id="impact-analysis">Impact Analysis</h1>
<p>Since the Azure Application Gateway for Containers or the NGINX Gateway Fabric are deployed as separate controllers, these could co-exist with the current ingress setup. Normal Ingress resources should not be affected.</p>
<p>When adopting the Gateway API, all the existing Ingress resources will need to be converted to Gateway API resources in our code base. We will need to decide if this needs to be done by the platform team or the developer teams.</p>
<p>One solution could be that we configure the Gateway and let the teams know that this is now a posiblity and that this should be used for new application deployments. The teams can then configure the Routes themselves after some training by the platform team.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The Gateway API is the next iteration of the Ingress resource. It allows more functionality and aids in defining the roles between the platform and development teams. If one is in the process to revise the method of handling ingress traffic to the clusters, it would make sense to adopt the Gateway API in this process.</p>
<p>Although there are no plans for a service mesh at the time of writing, having the Gateway API implemented will make the transition to a service mesh much easier. In the meantime, the Gateway API already provides interesting functionality such as traffic weighting, which would be one of the reasons to implement a service mesh.</p>
<p>Since the Gateway API itself is a Kubernetes resource, it does not have feature parity and would work on any Kubernetes cluster that has a controller which allows the Gateway resource to be defined. The choice we need to make is which controller we will use for the implementation of the Gateway API on our AKS clusters.</p>
<p>Given the ease of implementation, native Azure support and simplified maintenance, the Application Gateway for Containers is the most logical choice.</p>
<h1 id="links">Links</h1>
<p><a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io/</a></p>
<p><a href="https://kubernetes.io/docs/concepts/services-networking/gateway/">https://kubernetes.io/docs/concepts/services-networking/gateway/</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/overview">https://learn.microsoft.com/en-us/azure/application-gateway/for-containers/overview</a></p>
<p><a href="https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/gateway-api.md">https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/gateway-api.md</a></p>
<h2 id="links-1">Links:</h2>
<p>202403101103</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>PersistentVolumeClaims Lifecycle in Kubernetes</title>
      <link>https://mischavandenburg.com/zet/kubernetes-storage-pvc-pv/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/kubernetes-storage-pvc-pv/</guid>
      <description>I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here&amp;rsquo;s what you need to know:
PVCs: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed.</description>
      <content:encoded><![CDATA[<p>I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here&rsquo;s what you need to know:</p>
<ul>
<li><strong>PVCs</strong>: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed.</li>
<li><strong>PVs</strong>: Provisioned by administrators or dynamically through Storage Classes, PVs provide the actual storage resources. Their lifecycle is not tied to any specific Pod.</li>
</ul>
<h1 id="understanding-reclaim-policies">Understanding Reclaim Policies</h1>
<p>The Reclaim Policy on a PV dictates its fate after a PVC is released. There are three policies to be aware of:</p>
<ol>
<li><strong>Delete</strong>: The PV and its underlying storage are deleted when the PVC is deleted. Importantly, if the PVC is not deleted, the PV remains.</li>
<li><strong>Retain</strong>: When a PVC is deleted, the PV is not. Instead, it transitions to a <code>Released</code> state, but the data remains intact until manually deleted or repurposed.</li>
<li><strong>Recycle (Deprecated)</strong>: The volume is scrubbed clean and made available for a new claim. Note: this policy is no longer recommended.</li>
</ol>
<h1 id="practical-implications">Practical Implications</h1>
<ul>
<li>PVCs won&rsquo;t cause PV deletion unless they themselves are deleted. This is crucial to understand, especially with a <code>Delete</code> Reclaim Policy, as it leads to the deletion of both the PV and the underlying storage, resulting in data loss.</li>
<li>Carefully manage PVCs, especially in production environments. Ensure you have backups and fully understand the implications of deleting PVCs, especially when using a <code>Delete</code> Reclaim Policy.</li>
</ul>
<h2 id="links">Links:</h2>
<p>202401180901</p>
<p>[[Kubernetes]]</p>
<p>[[storage]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Learning Flux and Installing To Homelab</title>
      <link>https://mischavandenburg.com/zet/video-homelab-learning-flux/</link>
      <pubDate>Wed, 17 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-homelab-learning-flux/</guid>
      <description>In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/BtuqzsyztBc?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.</p>
<p>Notes:</p>
<ul>
<li>
<p>kustomization resources live in the cluster</p>
</li>
<li>
<p>source resources also live in the cluster</p>
</li>
<li>
<p>To suspend updates for a kustomization, run the command flux suspend kustomization <!-- raw HTML omitted -->.</p>
</li>
<li>
<p>To resume updates run the command flux resume kustomization <!-- raw HTML omitted -->.</p>
</li>
<li>
<p>use <code>flux reconcile source git podinfo</code> to force a sync, nn waiting</p>
</li>
<li>
<p>love the fact that helm releases are still accessible and visible on the cluster</p>
</li>
<li>
<p>flux is just managing helm for you based on code</p>
</li>
<li>
<p>love the fact that weave dashboard is also part of the cluster manifests</p>
</li>
<li>
<p>learned about Kubernetes controllers</p>
</li>
<li>
<p>an operator is a controller</p>
</li>
<li>
<p>basically everything that enables CRD&rsquo;s to run is a controller</p>
</li>
</ul>
<p>&ldquo;A kubernetes controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state&rdquo;</p>
<h2 id="sources">Sources</h2>
<blockquote>
<p>A Source defines the origin of a repository containing the desired state of the system and the requirements to obtain it (e.g. credentials, version selectors). For example, the latest 1.x tag available from a Git repository over SSH.</p>
</blockquote>
<blockquote>
<p>All sources are specified as Custom Resources in a Kubernetes cluster, examples of sources are GitRepository, OCIRepository, HelmRepository and Bucket resources.</p>
</blockquote>
<p><a href="https://fluxcd.io/flux/concepts/#sources">https://fluxcd.io/flux/concepts/#sources</a></p>
<h2 id="weave-ui">Weave UI</h2>
<p>Follow this guide</p>
<p><a href="https://docs.gitops.weave.works/docs/installation/weave-gitops/">https://docs.gitops.weave.works/docs/installation/weave-gitops/</a></p>
<ul>
<li>sources are located in the clusters directory in a monorepo structure</li>
</ul>
<h2 id="repo">Repo</h2>
<p>Decided to fully commit to Flux and their practices.</p>
<p>Set up the repo according to this guide:</p>
<p><a href="https://fluxcd.io/flux/guides/repository-structure/">https://fluxcd.io/flux/guides/repository-structure/</a></p>
<p>And following this example:</p>
<p><a href="https://github.com/fluxcd/flux2-kustomize-helm-example">https://github.com/fluxcd/flux2-kustomize-helm-example</a></p>
<h2 id="links">Links:</h2>
<p>202312261612</p>
<p><a href="https://kubernetes.io/docs/concepts/architecture/controller/">https://kubernetes.io/docs/concepts/architecture/controller/</a></p>
<p><a href="https://youtu.be/BtuqzsyztBc">https://youtu.be/BtuqzsyztBc</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Learned Cilium Network Policies</title>
      <link>https://mischavandenburg.com/zet/cilium-network-policies/</link>
      <pubDate>Sun, 14 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/cilium-network-policies/</guid>
      <description>Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.
But now I&amp;rsquo;m exposing some apps to the internet in my homelab and I&amp;rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.</description>
      <content:encoded><![CDATA[<p>Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.</p>
<p>But now I&rsquo;m exposing some apps to the internet in my homelab and I&rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.</p>
<p>I installed Cilium as the Container Networking Interface in my homelab cluster. This also allows me to use the Hubble tool. This has proven to be of immmense value to see the traffic that is actually flowing around in the namespace you&rsquo;re working on.</p>
<p><img loading="lazy" src="/npolicy1.png" type="" alt=""  /></p>
<p>Even with a small application of a few microservices, as the image shows, the traffic that flows between them and which needs to come from other namespaces can get pretty complex. But in the Hubble UI you get a clear view of what&rsquo;s happening, and you can see exactly when packets are being dropped.</p>
<p>I also found the <a href="https://editor.networkpolicy.io/">Network Policy Editor</a> extremely valuable. It is such a relief to be able to compose the policy from a graphical interface, even though I&rsquo;m a CLI guy and want to do everything in and from code, this has been a gamechanger.</p>
<p>Now I have created the following policy for my linkding app, which allows the EDB operator to do its thing on my databases, Prometheus to monitor my application, but a hacker could not do anything outside of the namespace if they managed to break out.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">linkding-app</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">policy-type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;app&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">postgresql-operator-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">kube-dns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;53&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">dns</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span>- <span class="nt">matchPattern</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">world</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;80&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;7844&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">linkding-database</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">policy-type</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;database&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">fromEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">postgresql-operator-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEndpoints</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">io.kubernetes.pod.namespace</span><span class="p">:</span><span class="w"> </span><span class="l">kube-system</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">k8s-app</span><span class="p">:</span><span class="w"> </span><span class="l">kube-dns</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;53&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l">UDP</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">rules</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">dns</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span>- <span class="nt">matchPattern</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;*&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">kube-apiserver</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;6443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">toEntities</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="l">world</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">toPorts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">ports</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span>- <span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;443&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">cilium.io/v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">CiliumNetworkPolicy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">deny-all</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">linkding</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">endpointSelector</span><span class="p">:</span><span class="w"> </span>{}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">ingress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- {}<span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">egress</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- {}<span class="w">
</span></span></span></code></pre></div><h2 id="links">Links:</h2>
<p>202401142001</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video notes - Application Gateway for Containers</title>
      <link>https://mischavandenburg.com/zet/application-gateway-for-containers/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/application-gateway-for-containers/</guid>
      <description>App Gateway It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.
Resources Two types of resources. Azure resources and k8s resources.
The App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.
Frontend Azure Frontend is a public IP and fqdn.</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/slCjHV8z9Wk?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h1 id="app-gateway">App Gateway</h1>
<p>It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.</p>
<h1 id="resources">Resources</h1>
<p>Two types of resources. Azure resources and k8s resources.</p>
<p>The App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.</p>
<h1 id="frontend">Frontend</h1>
<h2 id="azure">Azure</h2>
<p>Frontend is a public IP and fqdn. Both are managed resources, you don&rsquo;t see them in your subscription.</p>
<p>You can have multiple frontends in one gateway to save money. Teams could share the app gateway but use different frontend IP addresses or FQDNs.</p>
<p>Control plane: Azure App Gateway for Containers</p>
<p>Data plane: association with kubernetes pods.</p>
<p>The association is made to the subnet in the Azure VNet.</p>
<p>Each association is in one subnet, and the subnet should at least have /24 or 256 addresses.</p>
<h1 id="kubernetes">Kubernetes</h1>
<p>ALB controller consists of two pods. Controller pod and a bootstrap pod.</p>
<p>Controller communicates to the Azure gateway resource. It talks directly to the App Gateway, not to the Azure Resource Manager, which is why you&rsquo;re able to have sub-second updates.</p>
<p>The bootstrap contains the CRDs etc, it does not do very much.</p>
<h2 id="creating-resources">Creating resources</h2>
<p>There is a managed option that will talk to ARM and create the resources for you. Or you can choose to deploy them yourself. It depends whether you want to control everything from Kubernetes. If you have all your Azure resources in Infrastructure as Code it probably makes more sense to create the App Gateway resources from there instead of from Kubernetes.</p>
<h1 id="association">Association</h1>
<p>Links frontend with a subnet in a VNet. This will typically be the same vnet that the AKS cluster is in. Could technically be a peered VNet but probably uncommon.</p>
<p>This is an Azure resource. It lives in the VNet and handles TLS and makes the connections to and from the pods and frontend IP. This is the data path.</p>
<p>The traffic is not routed within the cluster, but in Azure by the association.</p>
<p>Client talks to the front end, passes to the association, the association is doing the work, and then routing it to the cluster.</p>
<h1 id="support">Support</h1>
<p>Azure CNI. Does not support kubenet or Azure CNI overlay yet, but it will support in the future.</p>
<h1 id="kubernetes-resources">Kubernetes Resources</h1>
<p>Application Load Balancer: this name was chosen because k8s people don&rsquo;t know the concept of app gateway.</p>
<h1 id="benefits">Benefits</h1>
<ul>
<li>ssl offloading</li>
<li>traffic splitting</li>
<li>clearer separation between platform team and app team</li>
<li>platform team manages the gateway itself</li>
<li>automatic default health probes</li>
</ul>
<h1 id="benefits-of-azure">Benefits of Azure</h1>
<ul>
<li>not suing cluster resources for load balancing. this happens in azure</li>
<li>native azure metrics available</li>
</ul>
<h1 id="misc">misc</h1>
<ul>
<li>AGIC does not support ingress</li>
</ul>
<h2 id="links">Links:</h2>
<p>202309301009</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Homelab E2 - Setting Up Monitoring &#43; Studying k3s Networking  &amp; Configuring Ingress</title>
      <link>https://mischavandenburg.com/zet/video-homelab-3-k3s-networking/</link>
      <pubDate>Thu, 04 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-homelab-3-k3s-networking/</guid>
      <description>In this video I installed Prometheus and Grafana using helm and studied k3s networking.
My goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.
installed prometheus and grafana with kube-prometheus-stack helm chart reflected on why I use k3s gained understanding of k3s loadbalancing solution configured /etc/hosts file to resolve to fake domain configured k3s ingress to use fake local domain struggled with ingress but figured it out in the end successfully made grafana UI available on fake local domain grafana.</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/JjIB65CVXAo?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>In this video I installed Prometheus and Grafana using helm and studied k3s networking.</p>
<p>My goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.</p>
<ul>
<li>installed prometheus and grafana with kube-prometheus-stack helm chart</li>
<li>reflected on why I use k3s</li>
<li>gained understanding of k3s loadbalancing solution</li>
<li>configured /etc/hosts file to resolve to fake domain</li>
<li>configured k3s ingress to use fake local domain</li>
<li>struggled with ingress but figured it out in the end</li>
<li>successfully made grafana UI available on fake local domain grafana.homelab.nl</li>
</ul>
<h2 id="links">Links:</h2>
<p>202312261012</p>
<p><a href="https://youtu.be/JjIB65CVXAo">https://youtu.be/JjIB65CVXAo</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Homelab Episode 1</title>
      <link>https://mischavandenburg.com/zet/homelab-episode-1/</link>
      <pubDate>Sun, 31 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/homelab-episode-1/</guid>
      <description>This is the first video of my homelab series.
I set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.
Links: 202312301912
https://youtu.be/X40gNPZ2xP4
[[homelab]]</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/X40gNPZ2xP4?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>This is the first video of my homelab series.</p>
<p>I set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.</p>
<h2 id="links">Links:</h2>
<p>202312301912</p>
<p><a href="https://youtu.be/X40gNPZ2xP4">https://youtu.be/X40gNPZ2xP4</a></p>
<p>[[homelab]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Discovered a simple CLI to backup and upload Grafana dashboards</title>
      <link>https://mischavandenburg.com/zet/grafana-gdg-backup-cli/</link>
      <pubDate>Sat, 30 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/grafana-gdg-backup-cli/</guid>
      <description>This is a super useful tool to list, download and upload Grafana dashboards as json.
Currently using this to occasionally take extra-extra backups of my Grafana creations
https://software.es.net/gdg/
Links: 202312301612</description>
      <content:encoded><![CDATA[<p>This is a super useful tool to list, download and upload Grafana dashboards as json.</p>
<p>Currently using this to occasionally take extra-extra backups of my Grafana creations</p>
<p><a href="https://software.es.net/gdg/">https://software.es.net/gdg/</a></p>
<h2 id="links">Links:</h2>
<p>202312301612</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Homelab Secret Management With GitOps and Azure Key Vault</title>
      <link>https://mischavandenburg.com/zet/handling-secrets-kubernetes/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/handling-secrets-kubernetes/</guid>
      <description>In this blog post, I want to share with you how I set up secrets management for my home lab. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.</description>
      <content:encoded><![CDATA[<p>In this blog post, I want to share with you how I set up secrets management for my <a href="https://github.com/mischavandenburg/homelab/">home lab</a>. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.</p>
<p>Since Flux is the integrated solution for GitOps in Azure Kubernetes Service I also adopted it for my home lab. Flux supports and recommends encrypting secrets in git using SOPS, which is a tool that uses asymmetric encryption to protect secrets. This means that you can store encrypted secrets in your git repository and only decrypt them when they are applied to the cluster. This sounds like a convenient and secure way to manage secrets, right?</p>
<p>Well, not quite. After doing some research, I decided to use a different approach: storing my secrets in Azure Key Vault and syncing them to my cluster with the Azure Key Vault Provider. This is the recommended best practice by Microsoft, as explained in this article:</p>
<blockquote>
<p>TLDR: Referencing secrets in an external key vault is the recommended approach. It is easier to orchestrate secret rotation and more scalable with multiple clusters and/or teams.</p>
</blockquote>
<p><a href="https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/">https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/</a></p>
<p>SOPS has some drawbacks that made me choose the Azure Key Vault route for my home lab. First of all, SOPS requires you to encrypt each secret value manually in the command line, which can be tedious and error-prone. Secondly, SOPS relies on a single encryption key that is stored in your local machine or in a cloud KMS. If you lose this key, you will lose access to all your encrypted secrets in your Git repo. Thirdly, SOPS does not provide a backup or recovery mechanism for your secrets, nor does it offer fine-grained access control or auditing capabilities.</p>
<p>Here are some of the reasons why I chose this approach over SOPS:</p>
<ul>
<li>Secrets are stored in an external source, which means that they are not exposed in git, even if encrypted.</li>
<li>Azure Key Vault provides backup and recovery features, such as soft delete and restore, which can help in case of accidental deletion or corruption of secrets.</li>
<li>Azure Key Vault allows fine-grained authentication and authorization to the vaults, using Entra ID identities and policies. This means that I can control who can access or modify my secrets, and audit their actions.</li>
<li>I don&rsquo;t need to encrypt or decrypt values in the command line, which can be error-prone or leak information. I can use the Azure CLI or the Azure Portal to manage my secrets in the vault.</li>
<li>If I lose my encryption key, all my encrypted secrets in git are useless with SOPS. With Azure Key Vault, I can still access my secrets using my Entra ID or recover them from backup.</li>
<li>This approach mimics a solution that I would use for an enterprise production environment</li>
</ul>
<p>However, this does not mean that I will not use SOPS at all. Sometimes Helm charts or other configurations need values to be hardcoded due to incorrect implementation. This is where SOPS truly shines. You can safely check in your secret values to source code. Additionally, the SOPS keys can also be stored in Azure Key Vault. Tools should be used where they are most useful, so I&rsquo;m not saying I won&rsquo;t use SOPS at all, but I&rsquo;m choosing Azure Key Vaults and the accompanying CSI Store provider for my main solution.</p>
<h1 id="azure-key-vault-provider">Azure Key Vault Provider</h1>
<p>To sync my secrets from Azure Key Vault to my AKS cluster, I used the Azure Key Vault Provider for Secrets Store CSI Driver. This is an open source project that enables you to mount secrets from external sources (such as Azure Key Vault) as volumes in your pods using the Container Storage Interface (CSI) specification.</p>
<p>I followed <a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/">the provider documentation</a> to install the provider in my local cluster using a Flux HelmRelease.</p>
<p>With the provider installed I can use the following configuration to sync a secret named grafana-admin-password from my Key Vault and make it available as a volume and environment variable in an example busybox pod:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.x-k8s.io/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">SecretProviderClass</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">azure-kv-secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">provider</span><span class="p">:</span><span class="w"> </span><span class="l">azure</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">parameters</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">keyvaultName</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;mischa-homelab-k8s&#34;</span><span class="w"> </span><span class="c"># the name of the KeyVault</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">objects</span><span class="p">:</span><span class="w"> </span><span class="p">|</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">      array:
</span></span></span><span class="line"><span class="cl"><span class="sd">        - |
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectName: grafana-admin-password
</span></span></span><span class="line"><span class="cl"><span class="sd">          objectType: secret              # object types: secret, key, or cert</span><span class="w">      
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">tenantId</span><span class="p">:</span><span class="w"> </span><span class="l">6ddecc48-41b1-48de-bfde-2efd29fae9c7</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">secretObjects</span><span class="p">:</span><span class="w"> </span><span class="c"># [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">data</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">password</span><span class="w"> </span><span class="c"># data field to populate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">objectName</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-admin-password</span><span class="w"> </span><span class="c"># name of the mounted content to sync; this could be the object name or the object alias</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-custom-secret</span><span class="w"> </span><span class="c"># name of the Kubernetes secret object</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Opaque</span><span class="w"> </span><span class="c"># type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls)</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Pod</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox-secrets-store-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">busybox</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">registry.k8s.io/e2e-test-images/busybox:1.29-1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;/bin/sleep&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="s2">&#34;10000&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;/mnt/secrets-store&#34;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SECRET_USERNAME</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-custom-secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l">password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store01-inline</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">csi</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">driver</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store.csi.k8s.io</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">readOnly</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">volumeAttributes</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">secretProviderClass</span><span class="p">:</span><span class="w"> </span><span class="l">azure-kv-secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">nodePublishSecretRef</span><span class="p">:</span><span class="w"> </span><span class="c"># Only required when using service principal mode</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">secrets-store-creds</span><span class="w"> </span><span class="c"># Only required when using service principal mode. The name of the Kubernetes secret that contains the service principal credentials to access keyvault.</span><span class="w">
</span></span></span></code></pre></div><p>I hope you found this blog post useful and informative. If you have any questions or feedback, please feel free to leave a comment below or contact me on Twitter @mischa_vdburg</p>
<h2 id="links">Links:</h2>
<p>202312290912</p>
<p><a href="https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/">https://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/</a></p>
<p><a href="https://github.com/mischavandenburg/homelab/">https://github.com/mischavandenburg/homelab/</a></p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/key-vault/">Azure Key Vault documentation</a></li>
<li><a href="https://fluxcd.io/docs/">Flux documentation</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver">Azure Key Vault Provider for Secrets Store CSI Driver documentation</a></li>
</ul>
<p><a href="https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/">https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Viewing Strava Data In Grafana Dashboards</title>
      <link>https://mischavandenburg.com/zet/video-strava-grafana/</link>
      <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-strava-grafana/</guid>
      <description>In this video I&amp;rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:
creating an application in Strava installing the Strava plugin in Grafana adding the Strava data source importing dashboards exploring data and creating custom dashboard Links: 202312250812
https://youtu.be/CgP9hs9UDzA</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/CgP9hs9UDzA?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>In this video I&rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:</p>
<ul>
<li>creating an application in Strava</li>
<li>installing the Strava plugin in Grafana</li>
<li>adding the Strava data source</li>
<li>importing dashboards</li>
<li>exploring data and creating custom dashboard</li>
</ul>
<h2 id="links">Links:</h2>
<p>202312250812</p>
<p><a href="https://youtu.be/CgP9hs9UDzA">https://youtu.be/CgP9hs9UDzA</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: How To Install Prometheus &amp; Grafana In Your Homelab</title>
      <link>https://mischavandenburg.com/zet/video-install-prometheus-grafana-homelab/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-install-prometheus-grafana-homelab/</guid>
      <description>In this video I&amp;rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.
There are many options available out there but this is the easiest one I found to get up and running quickly.
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace
helm show values prometheus-community/kube-prometheus-stack &amp;gt; prometheus-default-values.yaml
Opening the Grafana UI k port-forward svc/prometheus-stack-grafana 3000:80</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/3AINqaBwOYs?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>In this video I&rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.</p>
<p>There are many options available out there but this is the easiest one I found to get up and running quickly.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</span></span><span class="line"><span class="cl">helm repo update
</span></span></code></pre></div><p><code>helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace</code></p>
<p><code>helm show values prometheus-community/kube-prometheus-stack &gt; prometheus-default-values.yaml</code></p>
<h2 id="opening-the-grafana-ui">Opening the Grafana UI</h2>
<p><code>k port-forward svc/prometheus-stack-grafana 3000:80</code></p>
<p>Then you can open it by entering <code>localhost:3000</code> in your browser.</p>
<p>The default credentials are admin:prom-operator</p>
<h2 id="links">Links:</h2>
<p>202312250812</p>
<p><a href="https://youtu.be/3AINqaBwOYs?si=maN1rfbg4pzBc3xI">https://youtu.be/3AINqaBwOYs?si=maN1rfbg4pzBc3xI</a></p>
<p><a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration">https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Talk: Avoiding Microservice Megadisasters by Jimmy Bogard</title>
      <link>https://mischavandenburg.com/zet/talk-avoiding-microservice-disasters/</link>
      <pubDate>Sat, 23 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/talk-avoiding-microservice-disasters/</guid>
      <description>Watched this very insightful talk on microservice architecture.
Some things I learned:
microservices should be autonomous: they should have minimal dependencies on other microservices if they have dependencies they should only be 1 layer deep a microservice should not be calling another microservice which calls another microservice dependencies can be reversed by pushing data towards the service for example: a pricing database can be dumped and pushed to a catalog service once a day data duplication is not a sin Another powerful point he made is that the architecture is a reflection from the organization&amp;rsquo;s structure.</description>
      <content:encoded><![CDATA[<p>Watched this very insightful talk on microservice architecture.</p>
<p>Some things I learned:</p>
<ul>
<li>microservices should be <strong>autonomous</strong>: they should have minimal dependencies on other microservices</li>
<li>if they have dependencies they should only be 1 layer deep
<ul>
<li>a microservice should not be calling another microservice which calls another microservice</li>
</ul>
</li>
<li>dependencies can be reversed by pushing data towards the service
<ul>
<li>for example: a pricing database can be dumped and pushed to a catalog service once a day</li>
<li>data duplication is not a sin</li>
</ul>
</li>
</ul>
<p>Another powerful point he made is that the architecture is a reflection from the organization&rsquo;s structure. He explained that the company was organized in teams and that managers of teams were promoted based on the amount of people they managed. This meant that managers were making up services which needed people to build them, which led to a proliferation of services and dependencies.</p>
<p>In order to have a correct software strucutre, the organization must be organized correctly first.</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/gfh-VCTwMw8?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h2 id="links">Links:</h2>
<p>202312231912</p>
<p><a href="https://youtu.be/gfh-VCTwMw8?si=QqHcbG_5ezx5qJX2">https://youtu.be/gfh-VCTwMw8?si=QqHcbG_5ezx5qJX2</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Ensure Cgroupsv2 compatiblity when containerizing old apps</title>
      <link>https://mischavandenburg.com/zet/ensure-cgroupsv2-compatibility/</link>
      <pubDate>Tue, 12 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/ensure-cgroupsv2-compatibility/</guid>
      <description>Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.
Please make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.</description>
      <content:encoded><![CDATA[<p>Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.</p>
<p>Please make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.</p>
<p><a href="https://github.com/Azure/AKS/releases/tag/2023-11-28">https://github.com/Azure/AKS/releases/tag/2023-11-28</a></p>
<h2 id="links">Links:</h2>
<p>202312120812</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What&#39;s so hard in Kubernetes?</title>
      <link>https://mischavandenburg.com/zet/whats-so-hard-about-kubernetes/</link>
      <pubDate>Sun, 10 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/whats-so-hard-about-kubernetes/</guid>
      <description>Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.
The real art is to build distributed systems on top of it, and it is dauntingly complex. That&amp;rsquo;s why I&amp;rsquo;m so attracted to it.
It does make clear just how great Azure&amp;rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I&amp;rsquo;m under the impression it is done really well.</description>
      <content:encoded><![CDATA[<p>Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.</p>
<p>The real art is to build distributed systems on top of it, and it is dauntingly complex. That&rsquo;s why I&rsquo;m so attracted to it.</p>
<p>It does make clear just how great Azure&rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I&rsquo;m under the impression it is done really well. Which is why I&rsquo;m on a mission to become an AKS expert.</p>
<p><a href="https://www.reddit.com/r/devops/comments/18e4nuw/whats_so_hard_in_kubernetes/?utm_source=share&amp;utm_medium=web2x&amp;context=3">https://www.reddit.com/r/devops/comments/18e4nuw/whats_so_hard_in_kubernetes/?utm_source=share&amp;utm_medium=web2x&amp;context=3</a></p>
<h2 id="links">Links:</h2>
<p>202312101012</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying Grafana Agent With Custom Secrets From Azure Key Vault Using Akv2k8s And K8s-Monitoring Helm Chart</title>
      <link>https://mischavandenburg.com/zet/grafana-agent-with-custom-secrets-akv2k8s/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/grafana-agent-with-custom-secrets-akv2k8s/</guid>
      <description>Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.
It contains:
kube-state-metrics, which gathers metrics about Kubernetes objects Node exporter, which gathers metrics about Kubernetes nodes OpenCost, which interprets the above to create cost metrics for the cluster, and Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.</description>
      <content:encoded><![CDATA[<p>Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.</p>
<p>It contains:</p>
<ul>
<li>kube-state-metrics, which gathers metrics about Kubernetes objects</li>
<li>Node exporter, which gathers metrics about Kubernetes nodes</li>
<li>OpenCost, which interprets the above to create cost metrics for the cluster, and</li>
<li>Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki</li>
</ul>
<p>The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.g. on Grafana Cloud).</p>
<p>For my current project I&rsquo;m setting it up to a Grafana Cloud stack, but as stated above it can also be used with a local or other remote Prometheus instance.</p>
<h1 id="akv2k8s">akv2k8s</h1>
<p>We use akv2k8s on our clusters to synch secrets from Azure Key Vaults to Kubernetes secrets or injecting them as environment variables. This way we can prevent checking in secrets to our code.</p>
<p>However, when I first tried to deploy this chart, external secrets were not supported. They have now been fixed by Pete Wall and I managed to deploy the k8s-monitoring by writing a wrapper Helm chart with the external secret objects.</p>
<h1 id="code">code</h1>
<p>All code is available in my <a href="https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets">lab repo</a>. I wrote the following Chart.yaml:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v2</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="m">1.0.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="l">This chart deploys the k8s-monitoring chart with custom secrets</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">dependencies</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="m">0.5.1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l">https://grafana.github.io/helm-charts/</span><span class="w">
</span></span></span></code></pre></div><p>Then I created a templates directory and added secrets.yaml:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">spv.no/v1alpha1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">AzureKeyVaultSecret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">vault</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span>{{<span class="w"> </span><span class="l">.Values.keyVault }}</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">object</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">secret</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">output</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">dataKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nn">---</span><span class="w">
</span></span></span></code></pre></div><p>Then, to use the external secrets in the values file, I used the following configuration:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">keyVault</span><span class="p">:</span><span class="w"> </span><span class="l">kv-123-hello</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">k8s-monitoring</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">cluster</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">aks-123-hello</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">externalServices</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">hostKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-host</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">basicAuth</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">usernameKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-user</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">passwordKey</span><span class="p">:</span><span class="w"> </span><span class="l">prometheus-password</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">grafana-agent-credentials-akv2k8s</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l">k8s-monitoring</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">extraConfig</span><span class="p">:</span><span class="w"> </span><span class="p">|-</span><span class="sd">
</span></span></span><span class="line"><span class="cl"><span class="sd">    logging {
</span></span></span><span class="line"><span class="cl"><span class="sd">      level  = &#34;info&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">      format = &#34;logfmt&#34;
</span></span></span><span class="line"><span class="cl"><span class="sd">    }</span><span class="w">    
</span></span></span></code></pre></div><p>This chart can be deployed using <code>helm install</code> but we are using GitOps like the gods intended. When this is configured in ArgoCD, the chart will be deployed with the secrets templates. These templates will retreive the secrets from the Azure Key Vault and synch them to the <code>grafana-agent-credentials-akv2k8s</code> secret.</p>
<h1 id="bug">bug</h1>
<p>Althoug this might have been a slight bug during the first implementation of this chart, I did run into the following error in my first attempts. The logs of the Grafana Agent showed:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">unsupported protocol scheme <span class="se">\&#34;\&#34;</span><span class="s2">&#34;
</span></span></span></code></pre></div><p>After a lot of debugging I decided to completely rebuild the deployment from scratch, and then I found out that I had put the externalServices.prometheus and externalServices.secret objects in a different order which messed up the rest of the values file. <strong>It is important to keep the order that is given in the values.yaml in the k8s-monitoring repo.</strong></p>
<h2 id="links">Links:</h2>
<p>Helm chart:</p>
<p><a href="https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring">https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring</a></p>
<p>GitHub issue:</p>
<p><a href="https://github.com/grafana/k8s-monitoring-helm/issues/81#issuecomment-1828771508">https://github.com/grafana/k8s-monitoring-helm/issues/81#issuecomment-1828771508</a></p>
<p>Link to example code in my lab repo:</p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets">https://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets</a></p>
<p>202311281011</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Implementing Pod Disruption Budgets</title>
      <link>https://mischavandenburg.com/zet/implementing-poddisruptionbudgets/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/implementing-poddisruptionbudgets/</guid>
      <description>When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I&amp;rsquo;m therefore starting a project to always run applications with multiple replicas.
However, running multiple replicas is not the only necessary improvement here.</description>
      <content:encoded><![CDATA[<p>When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I&rsquo;m therefore starting a project to always run applications with multiple replicas.</p>
<p>However, running multiple replicas is not the only necessary improvement here. Even though a pod is running with multiple replicas, that does not mean that Kubernetes will always keep them alive. When you do an AKS cluster upgrade, nodes are drained one by one and the pods are moved to a node with the higher k8s version. Technically, when draining a node, Kubernetes could kill both of the pods at the same time if they are running on the same node.</p>
<p>To prevent this we use <a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">pod disruption budgets</a></p>
<h1 id="helm">Helm</h1>
<p>We&rsquo;re using a Helm chart to deploy with ArgoCD. I&rsquo;m adding this to the templates directory:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">pyramid-backend-pdb</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">minAvailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">tier</span><span class="p">:</span><span class="w"> </span><span class="l">backend</span><span class="w">
</span></span></span></code></pre></div><p>I&rsquo;m selecing all the pods that have the label &ldquo;tier: backend&rdquo;.</p>
<p>To verify that I&rsquo;m targeting the right pods, I run:</p>
<p><code>kubectl get pods -l tier=backend</code></p>
<p>Currently my deployment has only two replicas:</p>
<pre tabindex="0"><code>(ins)$ k get deploy
NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
pyramid-deployment-backend   1/1     1            1           297d
</code></pre><p>So when I check my PDB it will show no allowed disruptions.</p>
<pre tabindex="0"><code>(ins)$ k get pdb
NAME                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
pyramid-backend-pdb   1               N/A               0                     13m
</code></pre><p>This is good, this is the expected behaviour. Kubernetes will now prevent me from draining the node because there are no allowed disruptions.</p>
<p>Then I scale up the deployment:</p>
<pre tabindex="0"><code>(ins)$ k scale deploy --replicas=2 pyramid-deployment-backend
deployment.apps/pyramid-deployment-backend scaled
</code></pre><p>And then I get an allowed disruption of 1:</p>
<pre tabindex="0"><code>(ins)$ k get pdb
NAME                  MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
pyramid-backend-pdb   1               N/A               1                     15m
</code></pre><p>With this configuration, Kubernetes will never kill all of the pods simultaneously in case a node needs to be drained. It will make sure that one of the pods keeps running when it is rescheduling pods for an upgrade.</p>
<h2 id="links">Links:</h2>
<p><a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a></p>
<p>202311271411</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>KEDA went GA for AKS!</title>
      <link>https://mischavandenburg.com/zet/keda-ga/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/keda-ga/</guid>
      <description>Great news! KEDA finally went GA for AKS.
https://learn.microsoft.com/en-us/azure/aks/keda-about
Links: 202311081711</description>
      <content:encoded><![CDATA[<p>Great news! KEDA finally went GA for AKS.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/keda-about">https://learn.microsoft.com/en-us/azure/aks/keda-about</a></p>
<h2 id="links">Links:</h2>
<p>202311081711</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Some notes on upgrading AKS clusters running EnterpriseDB</title>
      <link>https://mischavandenburg.com/zet/edb-cluster-notes/</link>
      <pubDate>Tue, 17 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/edb-cluster-notes/</guid>
      <description>Upgrading clusters with databases When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.
The EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.
In order to be able to run the AKS upgrade you will need to set the following:
k cnp maintenance set --all-namespaces --reusePVC
cnp is a kubectl plugin which you need to install to manage the EDB operator --reusePVC can only be used if all the nodes are in the same availability zone use this command to list all the zones: kubectl describe nodes | grep -e &amp;quot;Name:&amp;quot; -e &amp;quot;topology.</description>
      <content:encoded><![CDATA[<h1 id="upgrading-clusters-with-databases">Upgrading clusters with databases</h1>
<p>When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.</p>
<p>The EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.</p>
<p>In order to be able to run the AKS upgrade you will need to set the following:</p>
<p><code>k cnp maintenance set --all-namespaces --reusePVC</code></p>
<ul>
<li>cnp is a kubectl plugin which you need to install to manage the EDB operator</li>
<li><code>--reusePVC</code> can only be used if all the nodes are in the same availability zone</li>
<li>use this command to list all the zones: <code>kubectl describe nodes | grep -e &quot;Name:&quot; -e &quot;topology.kubernetes.io/zone&quot;</code></li>
</ul>
<p>When the upgrade is complete, run <code>k cnp maintenance set --all-namespaces</code></p>
<h2 id="getting-the-status">Getting the status</h2>
<p>You can get all information about the database cluster:</p>
<p><code>k cnp status pyramid-cluster-database -n mycommodity</code></p>
<p>Most deployments have the same name pyramid-cluster-database.</p>
<h2 id="problems">Problems</h2>
<p>Sometimes a database replica will not come up properly and you can get the following error message:</p>
<p><code>(FATAL: the database system is starting up (SQLSTATE 57P03))</code></p>
<p>To fix this, first disable reusing PVC by running:</p>
<p><code>k cnp maintenance set -n problemdatabase</code></p>
<p>&ldquo;problemdatabase&rdquo; is the namespace where the database cluster is deployed.</p>
<p>Then delete the PVC of the problematic pod. This should trigger a new replica to be created from scratch. Depending on the size of the DB it can take a lot of time.</p>
<h2 id="links">Links:</h2>
<p>202310171410</p>
<p>[[Kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Backing up AKS Clusters with Azure Backup is now in preview</title>
      <link>https://mischavandenburg.com/zet/aks-azure-backup-preview/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/aks-azure-backup-preview/</guid>
      <description>You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.
https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup
Links: 202310130610
[[AKS]]
[[Kubernetes]]
[[Azure]]</description>
      <content:encoded><![CDATA[<p>You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup">https://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup</a></p>
<h2 id="links">Links:</h2>
<p>202310130610</p>
<p>[[AKS]]</p>
<p>[[Kubernetes]]</p>
<p>[[Azure]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Fixed an issue with akv2k8s overriding security context</title>
      <link>https://mischavandenburg.com/zet/akv2k8s-edb-initcontainer-securitycontext/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/akv2k8s-edb-initcontainer-securitycontext/</guid>
      <description>This week I&amp;rsquo;ve been tackling an issue at work with akv2k8s.
We are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:
Error: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: &amp;#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)&amp;#34;, container: bootstrap-controller) Pods have a security context where you specify how the containers in the pod should be run.</description>
      <content:encoded><![CDATA[<p>This week I&rsquo;ve been tackling an issue at work with akv2k8s.</p>
<p>We are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:</p>
<pre tabindex="0"><code>Error: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: &#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)&#34;, container: bootstrap-controller)
</code></pre><p>Pods have a security context where you specify how the containers in the pod should be run.</p>
<pre tabindex="0"><code>securityContext:
    runAsUser: 998
    runAsGroup: 996
    runAsNonRoot: true
    fsGroup: 996
</code></pre><p>Here we are telling the pod that we may not run as the root user and that we will run as user 998.</p>
<p>My error states that it cannot verify whether it is running as root or not because we are using a non-numeric user.</p>
<p>We were dumbfounded by this because the deployment and ReplicaSet neatly had their securitycontexts configured and the pods should inherit them. After a lot of searching I figured out that the securitycontext of the pods was actually empty, so somehow it must be overridden somewhere.</p>
<p>The culprit was the akv2k8s envinjector. This tool injects the secrets into the environment of the pods. It took a lot of experimentation but we managed to narrow it down to the akv2k8s upgrade and eventually we found this GitHub issue:</p>
<p><a href="https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605">https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605</a></p>
<p>For some reason the envinjector is overriding the securitycontext of the pods. The issue was fixed by downgrading the helm chart back to 2.4.2</p>
<p>I did learn a lot in the process though!</p>
<h2 id="links">Links:</h2>
<p>202310130610</p>
<p><a href="https://www.enterprisedb.com/docs/postgres_for_kubernetes/latest/installation_upgrade/">https://www.enterprisedb.com/docs/postgres_for_kubernetes/latest/installation_upgrade/</a></p>
<p><a href="https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605">https://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605</a></p>
<p><a href="https://stackoverflow.com/questions/53949329/kubernetes-runasnonroot-failing-createcontainerconfigerror">https://stackoverflow.com/questions/53949329/kubernetes-runasnonroot-failing-createcontainerconfigerror</a></p>
<p><a href="https://github.com/cloudnative-pg/cloudnative-pg">https://github.com/cloudnative-pg/cloudnative-pg</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Forcing a new cert from letsencrypt when too many have been issued</title>
      <link>https://mischavandenburg.com/zet/forcing-new-cert-letsencrypt-timeout/</link>
      <pubDate>Thu, 12 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/forcing-new-cert-letsencrypt-timeout/</guid>
      <description>Learned a new trick to force a new cert from letsencrypt. Had this error message:
The certificate request has failed to complete and will be retried: Failed to wait for order resource &amp;#34;vcs-secret-j8gnj-3121219202&amp;#34; to become ready: order is in &amp;#34;errored&amp;#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.</description>
      <content:encoded><![CDATA[<p>Learned a new trick to force a new cert from letsencrypt. Had this error message:</p>
<pre tabindex="0"><code>The certificate request has failed to complete and will be retried: Failed to wait for order resource &#34;vcs-secret-j8gnj-3121219202&#34; to become ready: order is in &#34;errored&#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.org/docs/duplicate-certificate-limit/
</code></pre><p>I asked for too many certificates during a 168 hour period. However, there is a really easy fix. Just add a new subdomain and LetsEncrypt will treat it as an entirely new certificate!</p>
<p>For example, when using cert-manager:</p>
<pre tabindex="0"><code>apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  creationTimestamp: &#39;2023-10-12T11:09:36Z&#39;
  generation: 2
  labels:
    argocd.argoproj.io/instance: vcs-dev
  name: vcs-secret
  namespace: vcs
  ownerReferences:
    - apiVersion: networking.k8s.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: Ingress
      name: vcs-ingress
      uid: xxxxx
      resourceVersion: &#39;226558404&#39;
  uid: xxxx
spec:
  dnsNames:
    - vcs.mischavandenburg.com
    - test.mischavandenburg.com
  issuerRef:
    group: cert-manager.io
    kind: ClusterIssuer
    name: letsencrypt-prod
  secretName: vcs-secret
  usages:
    - digital signature
    - key encipherment
</code></pre><p>Just add another domain under spec.dnsNames and it will treat it as a new cert.</p>
<pre tabindex="0"><code>spec:
  dnsNames:
    - vcs.mischavandenburg.com
    - test.mischavandenburg.com
    - hello.mischavandenburg.com
</code></pre><h2 id="links">Links:</h2>
<p>202310121310</p>
<p>[[Kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Some Interesting Features went GA on Azure</title>
      <link>https://mischavandenburg.com/zet/azure-updates-sept-23/</link>
      <pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-updates-sept-23/</guid>
      <description>Yesterday a few interesting AKS related features became Generally Available on Azure.
KEDA add-on makes it easier to scale your applications on AKS cluster.
https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/
You can have a flexible and customized strategy for node-level OS security updates.
https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/
Use Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads
https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/
Preview Public preview: AKS support for Kubernetes version 1.28
https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/
Links: 202309281009
[[Azure]]
[[Kubernetes]]</description>
      <content:encoded><![CDATA[<p>Yesterday a few interesting AKS related features became Generally Available on Azure.</p>
<p>KEDA add-on makes it easier to scale your applications on AKS cluster.</p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/">https://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/</a></p>
<p>You can have a flexible and customized strategy for node-level OS security updates.</p>
<p><a href="https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/">https://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/</a></p>
<p>Use Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads</p>
<p><a href="https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/">https://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/</a></p>
<h1 id="preview">Preview</h1>
<p>Public preview: AKS support for Kubernetes version 1.28</p>
<p><a href="https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/">https://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/</a></p>
<h2 id="links">Links:</h2>
<p>202309281009</p>
<p>[[Azure]]</p>
<p>[[Kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Pod Disruption Budgets Can Mess With Your AKS Updates</title>
      <link>https://mischavandenburg.com/zet/pod-disruption-budget-aks/</link>
      <pubDate>Wed, 27 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/pod-disruption-budget-aks/</guid>
      <description>Past week we&amp;rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.
If a deployment has a pod disruption budget which is incorrectly configured, it might show up as ALLOWED DISRUPTIONS: 0. When this happens, the node cannot be drained and you will get an error message in your events.</description>
      <content:encoded><![CDATA[<p>Past week we&rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.</p>
<p>If a deployment has a pod disruption budget which is incorrectly configured, it might show up as <code>ALLOWED DISRUPTIONS: 0</code>. When this happens, the node cannot be drained and you will get an error message in your events.</p>
<p><code>k get poddisruptionbudgets.policy</code></p>
<p><code>k get events</code></p>
<p>The error message will say something like &ldquo;Too man eviction attempts, usually a pdb&rdquo; (I lost the shell output so can&rsquo;t copy atm).</p>
<p>Kubernetes is in a situation where it needs to schedule the pod on another node, but it is unable to do do so because we are telling Kubernetes that it is not allowed to have any disruptions on the deployment.</p>
<p>Kubernetes is logical, it&rsquo;s doing like it&rsquo;s told. But it&rsquo;s frustrating because you can be sitting there waiting for 20 minutes wondering why your node isn&rsquo;t draining.</p>
<p>At my current gig we are not responsible for the content on the clusters and we should not meddle with the application teams&rsquo; namespaces.</p>
<p>However, one solution is to either kill the pod manually or scale up the deployment to more replicas so there will be a higher amount of allowed disruptions.</p>
<h2 id="links">Links:</h2>
<p>202309271809</p>
<p>[[Kubernetes]]</p>
<p>[[Azure]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Deploying AKS Cluster With Azure CNI Using Bicep</title>
      <link>https://mischavandenburg.com/zet/video-deploy-aks-with-azure-cni/</link>
      <pubDate>Sun, 09 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-deploy-aks-with-azure-cni/</guid>
      <description>In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.
Azure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.
I use Neovim and the Azure CLI for my coding and deployment.
You will learn how to:
Implement dev/test prefix to create multiple clusters with the same template Plan a VNet range for an Azure CNI cluster and be mindful of overlaps Deploy a VNet and subnet for the cluster using Bicep Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node Validate your Bicep template and troubleshoot errors Explore the results of your deployment in the Azure portal Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/_U3HichIJ0Q?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<p>In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.</p>
<p>Azure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.</p>
<p>I use Neovim and the Azure CLI for my coding and deployment.</p>
<p>You will learn how to:</p>
<ul>
<li>Implement dev/test prefix to create multiple clusters with the same template</li>
<li>Plan a VNet range for an Azure CNI cluster and be mindful of overlaps</li>
<li>Deploy a VNet and subnet for the cluster using Bicep</li>
<li>Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node</li>
<li>Validate your Bicep template and troubleshoot errors</li>
<li>Explore the results of your deployment in the Azure portal</li>
<li>Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps</li>
</ul>
<p>This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.</p>
<h1 id="excalidraw">Excalidraw</h1>
<p><img loading="lazy" src="/excni.png" type="" alt=""  /></p>
<h1 id="bullet-points">Bullet Points</h1>
<ul>
<li>Introduction to Azure CNI</li>
<li>Implement dev/test prefix</li>
<li>Deploy VNET and one subnet for the cluster</li>
<li>Deploy cluster with Azure CNI enabled</li>
</ul>
<h1 id="vnet-planning">VNet planning</h1>
<p>VNet cidr: 10.108.0.0/16</p>
<p>Subnet cidr:</p>
<p>10.108.0.0/16 - 65,536 addresses</p>
<p>Service cidr 10.0.0.0/16
DNS service ip addres 10.0.0.10</p>
<h2 id="links">Links:</h2>
<p>202307071807</p>
<p><a href="https://youtu.be/_U3HichIJ0Q">https://youtu.be/_U3HichIJ0Q</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni">https://learn.microsoft.com/en-us/azure/aks/configure-azure-cni</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Finishing Pipeline Setup  &amp; Working on KeyVault Template - Azure Kubernetes Lab Series</title>
      <link>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/</guid>
      <description>Finish deploying keyvault using pipeline Get the random name generation to work Lessons Learned Subscriptions need to be registered with resource providers, apparently https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli
acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline Achieved Setting up connection between pipeline and Azure subscription Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources) Learned about provider registrations Made progress on creating unique names for resources Successfully deployed new resource group and key vault from the pipeline Next time: Look into random string creation with utcNow</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/eooZ3OHl5Mc?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<ul>
<li>Finish deploying keyvault using pipeline</li>
<li>Get the random name generation to work</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Subscriptions need to be registered with resource providers, apparently</li>
</ul>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli</a></p>
<ul>
<li>acccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled</li>
<li>Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline</li>
</ul>
<h1 id="achieved">Achieved</h1>
<ul>
<li>Setting up connection between pipeline and Azure subscription</li>
<li>Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources)</li>
<li>Learned about provider registrations</li>
<li>Made progress on creating unique names for resources</li>
<li>Successfully deployed new resource group and key vault from the pipeline</li>
</ul>
<h1 id="next-time">Next time:</h1>
<p>Look into random string creation with utcNow</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow</a></p>
<p>or newGuid</p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid</a></p>
<p>Links:</p>
<p>202306281806</p>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/eooZ3OHl5Mc">https://youtu.be/eooZ3OHl5Mc</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Setting Up A Simple Azure Pipeline To Deploy A Keyvault</title>
      <link>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/</guid>
      <description>Write KeyVault template Write pipeline code set up Azure DevOps pipeline Lessons Learned Always make sure to use az deployment group instead of az group deployment Because it has older Bicep version and will be deprecated Make sure to be in correct Directory to be able to sync subscriptions for service connection Links: 202306302206
https://youtu.be/WnA8V3uq7P8</description>
      <content:encoded><![CDATA[

    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/WnA8V3uq7P8?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<ul>
<li>Write KeyVault template</li>
<li>Write pipeline code</li>
<li>set up Azure DevOps pipeline</li>
</ul>
<h1 id="lessons-learned">Lessons Learned</h1>
<ul>
<li>Always make sure to use <code>az deployment group</code> instead of <code>az group deployment</code></li>
<li>Because it has older Bicep version and will be deprecated</li>
<li>Make sure to be in correct Directory to be able to sync subscriptions for service connection</li>
</ul>
<h2 id="links">Links:</h2>
<p>202306302206</p>
<p><a href="https://youtu.be/WnA8V3uq7P8">https://youtu.be/WnA8V3uq7P8</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Deploying an AKS Cluster with Bicep, GitHub Copilot and Neovim</title>
      <link>https://mischavandenburg.com/zet/video-deploying-aks-cluster-bicep-github-copilot/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-deploying-aks-cluster-bicep-github-copilot/</guid>
      <description>Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn&amp;rsquo;t disappointed!
Links: 202306271706
https://www.youtube.com/watch?v=l0B65FUfNBU
[[AKS]] [[Kubernetes]] [[Neovim]] [[bicep]] [[coding]]</description>
      <content:encoded><![CDATA[<p>Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn&rsquo;t disappointed!</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/l0B65FUfNBU?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h2 id="links">Links:</h2>
<p>202306271706</p>
<p><a href="https://www.youtube.com/watch?v=l0B65FUfNBU">https://www.youtube.com/watch?v=l0B65FUfNBU</a></p>
<p>[[AKS]]
[[Kubernetes]]
[[Neovim]]
[[bicep]]
[[coding]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Video: Introducing New Bicep Parameter Files - .bicepparam - No more JSON!</title>
      <link>https://mischavandenburg.com/zet/video-bicep-bicepparam/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/video-bicep-bicepparam/</guid>
      <description>The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:
More readable and editor friendly Cleaner and less lines of code VSCode integration Quickly convert from JSON or template file using VSCode In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .</description>
      <content:encoded><![CDATA[<p>The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:</p>
<ul>
<li>More readable and editor friendly</li>
<li>Cleaner and less lines of code</li>
<li>VSCode integration</li>
<li>Quickly convert from JSON or template file using VSCode</li>
</ul>
<p>In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .bicepparam files.</p>
<p>Yes, you read that right, you&rsquo;ll be seeing a hardcore vim user switch to VSCode for this particular task!</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/6Gav1JpGAzo?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h2 id="links">Links:</h2>
<p>202306271706</p>
<p><a href="https://youtu.be/6Gav1JpGAzo">https://youtu.be/6Gav1JpGAzo</a></p>
<p><a href="https://github.com/Azure/bicep/releases/tag/v0.18.4">https://github.com/Azure/bicep/releases/tag/v0.18.4</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep">https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep</a></p>
<p><a href="https://github.com/mischavandenburg/lab/tree/main/bicep/keyvault-parameters">https://github.com/mischavandenburg/lab/tree/main/bicep/keyvault-parameters</a></p>
<p>[[AKS]]
[[bicep]]
[[coding]]
[[Kubernetes]]
[[Azure]]
[[Neovim]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>What is Azure CNI Overlay for AKS?</title>
      <link>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</link>
      <pubDate>Wed, 14 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/azure-aks-cni-overlay/</guid>
      <description>CNI? CNI stands for Container Network Interface. It allows communication between pods and services.
Current Azure CNI limitations Let&amp;rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</description>
      <content:encoded><![CDATA[<h1 id="cni">CNI?</h1>
<p>CNI stands for Container Network Interface. It allows communication between pods and services.</p>
<h2 id="current-azure-cni-limitations">Current Azure CNI limitations</h2>
<p>Let&rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.</p>
<p>Let&rsquo;s say Azure is assigned the following ranges:</p>
<p>10.60.0.0/16</p>
<p>10.61.0.0/16</p>
<p>10.62.0.0/16</p>
<p>This means that the networks in each of these ranges would have a maximum possible amount of 65534 addresses per range.</p>
<p>With the current Azure CNI (i.e. the non-overlay version), all pods are assigned an IP address from one of these ranges. It also uses direct VNet routing.  Since the pods use VNet IP&rsquo;s, there is a maximum of 65.000 pods per cluster. In other words, there is a risk for IP exhaustion, which limits the scalability of your workloads. Moreover, pod subnets cannot be shared across clusters.</p>
<p>It is crucial to carefully plan the number of pods you expect to deploy. If the required number of IP addresses exceeds the available addresses in the subnet, you will not be able to run your pods.</p>
<p>Now, these ranges are large and you can anticipate the growth of your resources. For now we are fine. But to design an infrastructure which is truly scalable and extendable, you will need to look into different options. This is where the Azure CNI Overlay comes in.</p>
<h2 id="benefits-of-azure-cni-overlay">Benefits of Azure CNI Overlay</h2>
<p>An Overlay network is an abstracted, virtual network which is put on top of your current network infrastructure. Nodes are assigned IP addresses from the VNets that they are deployed in, but pods get assigned IP addresses from the Overlay network.</p>
<p>Pods are assigned addresses from a private CIDR which is logically separate from the VNet hosting the nodes. They do not use up the IP addressess of the VNets, which means that your workloads become nearly infinitely scalable within your assigned IP address ranges when you are operating in this type of corporate networking infrastructure with IP range limitations. You can scale up to literally thousands of nodes without worrying about IP exhaustion.</p>
<p><img loading="lazy" src="/cnioverlay.png" type="" alt=""  /></p>
<p>Additionally, the Overlay network can also span across multiple AKS clusters. This opens up a whole world of possibilities where pods from separate workloads on separate clusters could communicate with each other directly using the high speed native direct routing of the Azure network.</p>
<h2 id="limitations">Limitations</h2>
<p>Azure CNI Overlay also comes with some limitations. A big one is that you cannot use Application Gateway as an Ingress Controller (AGIC) for an Overlay cluster.</p>
<p>Other notable limitations:</p>
<ul>
<li>Windows support is still in Preview</li>
<li>Virtual Machine Availability Sets (VMAS) are not supported for Overlay</li>
<li>Dualstack networking is not supported in Overlay</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, the Azure CNI Overlay provides a powerful solution to address the challenges of IP exhaustion and scalability in Azure AKS. By implementing the overlay network, organizations can overcome the limitations of the non-overlay version of Azure CNI and achieve a truly scalable and manageable infrastructure.</p>
<p>Azure CNI Overlay is currently in preview for Windows and GA for Linux nodes, but I&rsquo;m very excited about the developments. I&rsquo;ll be following them closely and I hope to be a part of its implementation at my current contract.</p>
<h1 id="links">Links:</h1>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay">https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay</a></p>
<p><a href="https://www.youtube.com/watch?v=kLBLaCC_dNs">https://www.youtube.com/watch?v=kLBLaCC_dNs</a></p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/">https://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/</a></p>
<p>202306131506</p>
<p>[[aks-networking-essentials]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Deploying Simple Applications to AKS with Draft</title>
      <link>https://mischavandenburg.com/zet/deploy-draft-azure/</link>
      <pubDate>Fri, 09 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/deploy-draft-azure/</guid>
      <description>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&amp;rsquo;m curious to see how far Microsoft will take this!
Links: 202306092006</description>
      <content:encoded><![CDATA[<p>Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I&rsquo;m curious to see how far Microsoft will take this!</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/PqhdX8-SZYw?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<h2 id="links">Links:</h2>
<p>202306092006</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>You Can Abort Operations on AKS Clusters Now</title>
      <link>https://mischavandenburg.com/zet/aks-abort-operation/</link>
      <pubDate>Thu, 27 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/aks-abort-operation/</guid>
      <description>It&amp;rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.
For example:
az aks operation-abort --name myAKSCluster --resource-group myResourceGroup
Links: 202304270704
https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/
https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli</description>
      <content:encoded><![CDATA[<p>It&rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.</p>
<p>For example:</p>
<p><code>az aks operation-abort --name myAKSCluster --resource-group myResourceGroup</code></p>
<h2 id="links">Links:</h2>
<p>202304270704</p>
<p><a href="https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/">https://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/</a></p>
<p><a href="https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli">https://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Starting My Homelab</title>
      <link>https://mischavandenburg.com/zet/starting-my-homelab/</link>
      <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/starting-my-homelab/</guid>
      <description>This week I started a project which I&amp;rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I&amp;rsquo;ve been collecting hardware here and there, and I&amp;rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein&amp;rsquo;s Homelab Init playlist on YouTube which I&amp;rsquo;m working on.
There are a few reasons why I haven&amp;rsquo;t started up until now:</description>
      <content:encoded><![CDATA[<p>This week I started a project which I&rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I&rsquo;ve been collecting hardware here and there, and I&rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein&rsquo;s <a href="https://www.youtube.com/playlist?list=PLrK9UeDMcQLpjUGg5z9Z6Un-axVx06-2J">Homelab Init</a> playlist on YouTube which I&rsquo;m working on.</p>
<p>There are a few reasons why I haven&rsquo;t started up until now:</p>
<ul>
<li>Focused on switching jobs and certifications</li>
<li>Not knowing what to run on the cluster</li>
<li>High electricity costs</li>
</ul>
<p>Now that I found a new job with a great employer, I&rsquo;ve changed my focus towards doing more hands-on learning in my free time by learning Go and diving deeper into Cloud Native technology. Energy prices have come down in the meantime as well.</p>
<p>I&rsquo;ve reached a stage in my Go learning journey where I&rsquo;m actually able to start making small deployable applications, and I want an environment where I can do that without high costs or without fearing to break something. I want to learn more about databases on Kubernetes, and I want to start writing small microservices and API&rsquo;s that are able to query these databases.</p>
<p>My lab is going to be my playground, where I can deploy whatever I want and learn the technologies that interest me at that particular moment.</p>
<h1 id="hardware-parts">Hardware: Parts</h1>
<p>For some reason the Raspberry Pi has become synonymous with homelabs. I get that it&rsquo;s fun to run a cluster on something that is not much larger than a 1kg pack of sugar. But I never really caught on to that whole scene yet. Maybe it&rsquo;s because I&rsquo;m a bit late to the party and the Pi&rsquo;s have been scarce and very expensive lately?</p>
<p>In any case, I&rsquo;ve been thankfully accepting old computers that friends were going to get rid of, and I&rsquo;ve been keeping some of my own old hardware as well. I have enough motherboards and other parts to assemble around 3 nodes, which will probably have around 8GB RAM each, but possibilities to attach storage.</p>
<p>This is also what has been keeping me back for a while I think. There is quite a bit of work that I need to do to get these machines going, and probably I&rsquo;ll have to purchase a couple of other parts. However, I also have some functional hardware.</p>
<h2 id="gaming-desktop">Gaming Desktop</h2>
<p>I have an old gaming desktop with 16GB RAM, an Intel 6700K Skylake, 1070 video card and a couple of TB of storage.</p>
<p>This has been my Arch Linux desktop for the past year, but now that I switched to my new MacBook, I&rsquo;m not using it as much. I want to keep it as it is right now, but I could run a few Virtual Machines on there, and maybe consider turning it into a ProxMox server.</p>
<h2 id="old-laptops">Old Laptops</h2>
<p>I have two old laptops. One Asus with 4GB of RAM and a Thinkpad T430 with 8GB RAM. The Thinkpad is actually surprisingly powerful. As a weekend project I installed Arch on it and I fitted it with a refurbished keyboard, and it is a very pleasant machine to work with. However, now that I have a very powerful laptop that I use as a desktop and portable device, it has become redundant.</p>
<h1 id="old-laptops-as-raspberri-pis">Old Laptops as Raspberri Pi&rsquo;s</h1>
<p>Having these two old laptops lying around, it occurred to me that these machines were basically Raspberri Pi&rsquo;s with a large form factor and a higher power usage. Why would I need to spend hundreds of euros on these smaller computers if I could just use these laptops as a starting point for my lab?</p>
<p>Using laptops has the following advantages:</p>
<ul>
<li>No additional costs</li>
<li>No building needed</li>
<li>Easy to install Linux on them</li>
<li>Built-in screen and keyboard for quick access when SSH does not work out</li>
<li>Built-in batteries to handle short power disruptions (rare but possible)</li>
<li>Can get going very quickly</li>
</ul>
<h1 id="choices-and-goals">Choices and Goals</h1>
<h2 id="kubernetes">Kubernetes</h2>
<p>The first goal is to get a Kubernetes cluster running. I will do bare metal kubeadm installs, and later I want to learn more about Talos. Fortunately I feel very comfortable installing Kubernetes with kubeadm. I did plenty of practice for my CKA, and I recently installed it on free Oracle VM&rsquo;s. I&rsquo;ve experimented a bit with K9S earlier, but I want to learn how to maintain on-prem Kubernetes.</p>
<p>My goals is to learn to maintain production-grade clusters properly.</p>
<h2 id="linux">Linux</h2>
<p>Naturally I&rsquo;ll be using Linux as my base OS. After some consideration I chose to use Ubuntu Server 22. Some notes on that choice:</p>
<ul>
<li>I already have years of Ubuntu Server experience</li>
<li>Good to keep building on what I have</li>
<li>Working with managed Kubernetes on my day job requires me to keep Linux admin skills fresh</li>
<li>Still the <a href="https://www.enterpriseappstoday.com/stats/linux-statistics.html">most popular Linux distro</a></li>
<li>Google uses Ubuntu Server</li>
<li>Well documented and plenty of questions on StackOverflow</li>
</ul>
<h2 id="infrastructure-as-code--gitops">Infrastructure as Code &amp; GitOps</h2>
<p>Initially I&rsquo;ll configure the servers by hand, but I want to have the server configuration as code as Ansible playbooks eventually. However, I&rsquo;ll be using ArgoCD for all of my deployments on Kubernetes itself, so the server configuration is only a very small part of the setup. Just get Kubernetes running and do the rest with ArgoCD.</p>
<p>Perhaps I will expand with larger servers that run multiple VM&rsquo;s. Then it will be very relevant to start provisioning these with Ansible.</p>
<h2 id="networking">Networking</h2>
<p>I want to learn more about networking and use static IP addresses for my servers. I need to figure out how my home network works exactly. Surprisingly, I&rsquo;ve never taken the effort to actually know how the devices on my network get their IP addresses and how they communicate, even though I&rsquo;ve learned plenty about it for my day job and do networking in an enterprise environment daily.</p>
<p>For Kubernetes I&rsquo;ll use Flannel to start out with, but I want to learn more about Cilium, Istio and other service mesh implementations.</p>
<p>Another goal is to host my own DNS server for internal name resolution, probably CoreDNS.</p>
<h2 id="deployment">Deployment</h2>
<p>I want to host my own container registry (Harbor) and use Tekton pipelines to for CI/CD, and I&rsquo;m playing with the thought to host my own GitLab instance as well.</p>
<h1 id="lets-go">Let&rsquo;s Go!</h1>
<p>Another realization was that I don&rsquo;t need to have everything figured out before I begin. The beauty of cluster computing is that you can add to it as you go. I can start with a small cluster of two nodes and build it out as my needs grow. I don&rsquo;t expect to need more than a few GB of RAM in the foreseeable future, so these two laptops will be plenty to get going.</p>
<p><img loading="lazy" src="/cluster-laptops.png" type="" alt=""  /></p>
<h2 id="links">Links:</h2>
<p>2023041213</p>
<p>[[homelab]]</p>
<p>[[homelab-network]]</p>
<p>[[Linux]]</p>
<p>[[homelab-ubuntu-server]]</p>
<p>[[Kubernetes]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Outlining My First Go Project</title>
      <link>https://mischavandenburg.com/zet/go-twitter-cli-project/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/go-twitter-cli-project/</guid>
      <description>When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.
Over the past few months I&amp;rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.
https://github.com/mischavandenburg/twitter-cli
https://twitter.com/mischa_vdburg
Twitter CLI Programs should solve a problem.</description>
      <content:encoded><![CDATA[<p>When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.</p>
<p>Over the past few months I&rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.</p>
<p><a href="https://github.com/mischavandenburg/twitter-cli">https://github.com/mischavandenburg/twitter-cli</a></p>
<p><a href="https://twitter.com/mischa_vdburg">https://twitter.com/mischa_vdburg</a></p>
<h1 id="twitter-cli">Twitter CLI</h1>
<p>Programs should solve a problem. My problem has to do with Twitter. I recently created a Twitter account, and I want to make a tweet whenever I publish something new on my website. I&rsquo;m currently doing this by hand, and that needs to stop, obviously.</p>
<p>There are bots out there for this, but I want to build it myself. I&rsquo;ve created the following user stories for my project.</p>
<h2 id="user-story-1">User Story 1</h2>
<blockquote>
<p>As a user, I need a command that I can run from a bash shell that will post the standard input to my Twitter account</p>
</blockquote>
<h2 id="user-story-2">User Story 2</h2>
<blockquote>
<p>As a user, I need a command that I can run from a bash shell that will take the latest post from the RSS feed generated by my blog and post it to Twitter</p>
</blockquote>
<h2 id="concepts">Concepts</h2>
<p>By writing this program I&rsquo;ll need to figure out the following problems in Go:</p>
<ul>
<li>Taking input from the command line</li>
<li>Authenticating to the Twitter API</li>
<li>Making a POST request to the Twitter API</li>
<li>Curling an RSS feed</li>
<li>Looping over / reading XML / HTML data</li>
<li>Transforming that data to a suitable format to post to Twitter</li>
</ul>
<h1 id="expansion">Expansion</h1>
<p>This will be a good start for my project and will keep me busy for a while. When I solved the previous problems I can use the result and expand further. Some thoughts about further expansion:</p>
<h2 id="rss-feeds">RSS Feeds</h2>
<p>I can use the skills I learn to start crawling Reddit feeds and filter them for keywords. I can automatically generate a curated selection from Reddit which will be easier to consume and will save me time by only serving me content that I might think is interesting to me, based on keywords.</p>
<h2 id="database">Database</h2>
<p>I want to learn more about using databases on Kubernetes and how to interact with databases using Go. For this I&rsquo;d like to store my RSS feed into a database and keep track of information in the database. I could track whether an article has been posted to Twitter and when.</p>
<h2 id="bot">Bot</h2>
<p>Rather than posting my latest blog post to Twitter by manually running a command, I should have a bot scanning my blog and posting to Twitter when it detects a new article. Or I could trigger the bot whenever I make a push to my blog repo.</p>
<p>In any case, I want to have an application running on a server. I&rsquo;m making plans to start up a proper home lab and this will be a perfect use case to start running on my home Kubernetes cluster.</p>
<h2 id="links">Links:</h2>
<p>202304081304</p>
<p><a href="https://github.com/mischavandenburg/twitter-cli">https://github.com/mischavandenburg/twitter-cli</a></p>
<p><a href="https://twitter.com/mischa_vdburg">https://twitter.com/mischa_vdburg</a></p>
<p>[[go]]</p>
<p>[[go-twitter-cli-project]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Kubernetes Resource Management for Pods and Containers - CPU and Memory</title>
      <link>https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/</link>
      <pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/</guid>
      <description>Pods have containers, and limits can be set on those containers.
Requests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.
Limits containers may never use more than the set limit
enforced by kubelet and container</description>
      <content:encoded><![CDATA[<p>Pods have containers, and limits can be set on those containers.</p>
<h1 id="requests">Requests</h1>
<ul>
<li>used by the kube-scheduler to determine where the Pod will be placed</li>
<li>containers can use more than requested resources if it is available on node</li>
</ul>
<p>If a limit is specified, but no request, Kubernetes will use the limit value as the request value.</p>
<h1 id="limits">Limits</h1>
<ul>
<li>
<p>containers may never use more than the set limit</p>
</li>
<li>
<p>enforced by kubelet and container</p>
</li>
<li>
<p>host kernel will kill processes that attempt to allocate more than limit (OOM error)</p>
</li>
<li>
<p>reactively: killed when exceeded</p>
</li>
<li>
<p>enforcement: system prevents container to ever exceed limit</p>
</li>
<li>
<p>If the node runs out of memory and the container exceeds its memory request, the pod will be evicted</p>
</li>
<li>
<p>container runtimes don&rsquo;t terminate Pods or containers for excessive CPU usage</p>
</li>
</ul>
<h1 id="pods">Pods</h1>
<p>The pod resource request and limit is the sum of the resource requests of the containers in the pod.</p>
<h1 id="cpu-units">CPU Units</h1>
<p>Defined as an absolute amount of resource. 1000m = 1 CPU.</p>
<p>This is always the same unit, regardless whether the host has 4 or 48 CPU&rsquo;s.</p>
<p>500m CPU = 0.5 CPU</p>
<h1 id="memory-units">Memory Units</h1>
<p>Can use P, T, G, M etc.</p>
<p>Note that &ldquo;m&rdquo; is not megabyte. 0.8m = 0.8 bytes.</p>
<p>Use mebibytes Mi or megabytes M.</p>
<h1 id="definiton-example">Definiton Example</h1>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">nginx</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">100Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">250m</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">memory</span><span class="p">:</span><span class="w"> </span><span class="l">200Mi</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">cpu</span><span class="p">:</span><span class="w"> </span><span class="l">500m</span><span class="w">
</span></span></span></code></pre></div><h1 id="scheduling">Scheduling</h1>
<p>The scheduler ensures that the sum of requests of the pods on the node does not exceed the available resources.</p>
<p>Even if a node has low resource usage, it will not accept pods that have requests which exceed the available resources.</p>
<h1 id="nodes">Nodes</h1>
<p>use <code>k describe node</code> to see the resource status of the node.</p>
<h2 id="links">Links:</h2>
<p>202303281903</p>
<p><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Difference Between DevOps, Cloud and Cloud Native</title>
      <link>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</link>
      <pubDate>Sun, 26 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/cloud-cloudnative-devops/</guid>
      <description>I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.
Cloud These are primarily cloud services. The external cloud.
&amp;ldquo;Something as a Service&amp;rdquo;.
Amazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape
Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.
Computing Edge Computing
High Performance Computing</description>
      <content:encoded><![CDATA[<p>I found <a href="https://youtu.be/gyjRriOyw-k">an excellent video</a> by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.</p>
<h1 id="cloud">Cloud</h1>
<p>These are primarily <em>cloud services</em>. The external cloud.</p>
<p>&ldquo;Something as a Service&rdquo;.</p>
<ul>
<li>Amazon</li>
<li>Azure</li>
<li>GCP</li>
</ul>
<h1 id="cloud-native">Cloud Native</h1>
<p>This is Cloud Native: <a href="https://landscape.cncf.io/">The CNCF Landscape</a></p>
<p>Cloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.</p>
<h2 id="computing">Computing</h2>
<ul>
<li>
<p>Edge Computing</p>
</li>
<li>
<p>High Performance Computing</p>
</li>
<li>
<p>Encapsulates all of the technologies that are involved with containerization of work, jobs and nodes</p>
</li>
<li>
<p>Deployment of compute resources as nodes</p>
</li>
<li>
<p>This is why Google&rsquo;s Borg was called Borg</p>
</li>
<li>
<p>Computers are drones of a larger collective</p>
</li>
<li>
<p>Every node puts all the resources into the collective.</p>
</li>
</ul>
<p>The collective is all the nodes combined, and Kubernetes is the Borg that orchestrates everything. It sees available resources and allocates the work that needs to be done.</p>
<p>Borg is the internal system developed at Google to run their infrastructure. You can read about it in the <a href="https://sre.google/books/">Site Reliability Engineering</a> books and I highly recommend them.</p>
<blockquote>
<p>Kubernetes is /proc for the cloud</p>
</blockquote>
<blockquote>
<p>Rob Muhlenstein</p>
</blockquote>
<h2 id="most-important-technologies">Most Important Technologies</h2>
<ul>
<li>
<p>Docker, Dockerfiles</p>
</li>
<li>
<p>Kubernetes</p>
</li>
<li>
<p>Helm</p>
</li>
<li>
<p>Harbor</p>
</li>
<li>
<p>Different registries, harbor, quay</p>
</li>
<li>
<p>It is a lot of Python and POSIX shell</p>
</li>
<li>
<p>Go for infrastructure application development</p>
</li>
<li>
<p>Kubernetes and Helm have won the game</p>
</li>
</ul>
<h2 id="containers-size-matters">Containers: Size Matters</h2>
<ul>
<li>Size matters (again) in the cloud</li>
<li>The smaller your container the better, because it takes less resources and less costs</li>
</ul>
<h1 id="devops">DevOps</h1>
<p>DevOps is not the same as Cloud Native. It is one piece of it, a specific set of practices and actions that can be done within Cloud Native.</p>
<ul>
<li>How you write software and release it</li>
<li>CI/CD</li>
<li>Focused on getting the software out</li>
<li>GitLab has become the one stop shop</li>
<li>Purpose is to write software and get it published fast</li>
<li>GitOps</li>
</ul>
<h1 id="summary">Summary</h1>
<p>In summary, &ldquo;cloud&rdquo; stands for the services offered by cloud providers such as AWS, Azure and GCP. Cloud Native stands for all of the technology that makes these cloud services possible. DevOps is part of Cloud Native, but definitely not the same thing. DevOps is concerned with how software is written and released.</p>
<h1 id="links">Links:</h1>
<p>202303262003</p>
<p><a href="https://youtu.be/gyjRriOyw-k">https://youtu.be/gyjRriOyw-k</a></p>
<p><a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a></p>
<p><a href="https://sre.google/books/">https://sre.google/books/</a></p>
<p>[[rwxrob]]</p>
<p>[[devops]]</p>
<p>[[Kubernetes]]</p>
<p>[[Cloud Native]]</p>
<p>[[cncf]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Running Docker and Kubernetes on Mac M2</title>
      <link>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</link>
      <pubDate>Sat, 18 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/</guid>
      <description>The past few days I&amp;rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.
Unfortunately you can&amp;rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.
Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&amp;rsquo;t tried any of the other alternatives because I found something better.</description>
      <content:encoded><![CDATA[<p>The past few days I&rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.</p>
<p>Unfortunately you can&rsquo;t just run <code>brew install docker</code> and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.</p>
<p>Minikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven&rsquo;t tried any of the other alternatives because I found something better.</p>
<p>Rancher Desktop provides everything that you need. It sets up a local VM where it will run a Kubernetes cluster using k3s. It will configure the containerd container engine for you which you can interact with using <code>nerdctl</code>.</p>
<p>To install:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">brew install rancher
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#after installing rancher, start it up and wait for it to boot the VM.</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">docker</span><span class="o">=</span>nerdctl
</span></span><span class="line"><span class="cl">docker run hello-world
</span></span></code></pre></div><p>And you&rsquo;re good to go. Rancher will add the rancher-desktop to your kube context.</p>
<p>To test your Kubernetes cluster:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k get pods
</span></span><span class="line"><span class="cl">k get nodes
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># test running a pod</span>
</span></span><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># inspect your services and look for 80:31066/TCP under PORT(S)</span>
</span></span><span class="line"><span class="cl">k get svc
</span></span><span class="line"><span class="cl">curl localhost:31066
</span></span></code></pre></div><p>Or visit localhost:31066 in your browser. Replace 31066 with the port you found listed under your services.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Lab VM project - Install ArgoCD to your Kubernetes cluster</title>
      <link>https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/</guid>
      <description>This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.
Install argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.
curl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer</description>
      <content:encoded><![CDATA[<p>This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.</p>
<h2 id="install-argocd-and-argocd-cli">Install argocd and argocd cli</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl create namespace argocd
</span></span><span class="line"><span class="cl">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
</span></span></code></pre></div><p>My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">curl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64
</span></span><span class="line"><span class="cl">sudo install -m <span class="m">555</span> argocd-linux-arm64 /usr/local/bin/argocd
</span></span><span class="line"><span class="cl">rm argocd-linux-arm64
</span></span></code></pre></div><p>Change the service type to LoadBalancer</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl patch svc argocd-server -n argocd -p <span class="s1">&#39;{&#34;spec&#34;: {&#34;type&#34;: &#34;LoadBalancer&#34;}}&#39;</span>
</span></span></code></pre></div><p>Retrieve your passsword</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl -n argocd get secret argocd-initial-admin-secret -o <span class="nv">jsonpath</span><span class="o">=</span><span class="s2">&#34;{.data.password}&#34;</span> <span class="p">|</span> base64 -d<span class="p">;</span> <span class="nb">echo</span>
</span></span></code></pre></div><p>Find out which port argocd-server is running on</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k get svc -A
</span></span></code></pre></div><p>Look for the argocd-server and see where port 80 is mapped to. In my case, it is 80:31372.</p>
<p>Open this port in your network security group for your VM, and you should be able to log in on ArgoCD in the browser by entering the VM ip followed by the port:</p>
<p><code>http://143.44.179.11:31372</code></p>
<h2 id="links">Links</h2>
<p><a href="https://argo-cd.readthedocs.io/en/stable/getting_started/">https://argo-cd.readthedocs.io/en/stable/getting_started/</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Setting up a Kubernetes cluster on an Ubuntu 20.04 VM with containerd and flannel</title>
      <link>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</link>
      <pubDate>Sun, 05 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/</guid>
      <description>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.
Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.
Installation sudo apt-get update sudo apt install apt-transport-https curl Install containerd
sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo &amp;#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.</description>
      <content:encoded><![CDATA[<p>You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See <a href="/zet/free-oracle-vm.md">this article</a> to create your VM.</p>
<p>Here are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.</p>
<h2 id="installation">Installation</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt install apt-transport-https curl
</span></span></code></pre></div><p>Install containerd</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo mkdir -p /etc/apt/keyrings
</span></span><span class="line"><span class="cl">curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="p">|</span> sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [arch=</span><span class="k">$(</span>dpkg --print-architecture<span class="k">)</span><span class="s2"> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu </span><span class="k">$(</span>lsb_release -cs<span class="k">)</span><span class="s2"> stable&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
</span></span><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt-get install containerd.io
</span></span></code></pre></div><p>Remove the default containerd configuration, because it creates errors when running kubeadm init.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo rm -f /etc/containerd/config.toml
</span></span><span class="line"><span class="cl">sudo systemctl status containerd.service
</span></span></code></pre></div><p>Install Kubernetes</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</span></span><span class="line"><span class="cl"><span class="nb">echo</span> <span class="s2">&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> <span class="p">|</span> sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span><span class="line"><span class="cl">sudo apt install kubeadm kubelet kubectl kubernetes-cni
</span></span></code></pre></div><p>Avoid the error &ldquo;/proc/sys/net/bridge/bridge-nf-call-iptables does not exist&rdquo; on kubeinit (reference <a href="https://github.com/kubernetes/kubeadm/issues/1062)">https://github.com/kubernetes/kubeadm/issues/1062)</a>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo modprobe br_netfilter
</span></span><span class="line"><span class="cl">sudo <span class="nb">echo</span> <span class="m">1</span> &gt; /proc/sys/net/ipv4/ip_forward
</span></span></code></pre></div><h2 id="start-the-cluster">Start the cluster</h2>
<p>Initialize the Kubernetes cluster for use with Flannel</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo kubeadm init --pod-network-cidr<span class="o">=</span>10.244.0.0/16
</span></span></code></pre></div><p>Copy to config as kubadm command says</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">mkdir -p <span class="nv">$HOME</span>/.kube
</span></span><span class="line"><span class="cl">sudo cp -i /etc/kubernetes/admin.conf <span class="nv">$HOME</span>/.kube/config
</span></span><span class="line"><span class="cl">sudo chown <span class="k">$(</span>id -u<span class="k">)</span>:<span class="k">$(</span>id -g<span class="k">)</span> <span class="nv">$HOME</span>/.kube/config
</span></span></code></pre></div><p>Usually you wouldn&rsquo;t run pods on your control-plane node. However, since we are running a lab environment on a single VM, it&rsquo;s ok. To be able to schedule pods on the control-plane node, we need to remove the NoSchedule taint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl taint node instance-20230205-0909 node-role.kubernetes.io/control-plane:NoSchedule-
</span></span></code></pre></div><h2 id="add-a-container-networking-interface">Add a Container Networking Interface</h2>
<p>Install Flannel to the cluster (reference <a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a>)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
</span></span></code></pre></div><h2 id="configure-the-server-firewall">Configure the server firewall</h2>
<p>We use Uncomplicated Firewall. Run these commands:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo ufw allow <span class="m">22</span>
</span></span><span class="line"><span class="cl">sudo ufw allow 6443/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 2379:2380/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10250/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10259/tcp
</span></span><span class="line"><span class="cl">sudo ufw allow 10257/tcp
</span></span><span class="line"><span class="cl">sudo ufw <span class="nb">enable</span>
</span></span><span class="line"><span class="cl">sudo ufw status
</span></span></code></pre></div><h2 id="set-up-bashrc">Set up bashrc</h2>
<p>Next, edit your bashrc with <code>vim ~/.bashrc</code> and add these lines:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nb">source</span> &lt;<span class="o">(</span>kubectl completion bash<span class="o">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">alias</span> <span class="nv">k</span><span class="o">=</span>kubectl
</span></span><span class="line"><span class="cl"><span class="nb">complete</span> -o default -F __start_kubectl k
</span></span></code></pre></div><p>Then run <code>source ~/.bashrc</code></p>
<p>This configures autocompletion for kubectl, and sets up &ldquo;k&rdquo; as an alias for kubectl.</p>
<h2 id="lets-run-a-pod">Let&rsquo;s run a pod!</h2>
<p>To see all pods running on your cluster:</p>
<p><code>k get pods -A</code></p>
<p>Now let&rsquo;s run a simple nginx pod and expose it:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">k run nginx --image<span class="o">=</span>nginx
</span></span><span class="line"><span class="cl">k expose pod nginx --port<span class="o">=</span><span class="m">80</span> --type<span class="o">=</span>NodePort
</span></span></code></pre></div><p>To find out which port it&rsquo;s running on, run <code>k get service</code>. In the PORT(S) column, there will be an nginx service exposing port 80 to a random port on the node in the range of 30000-32767.</p>
<p>In my case, it says &ldquo;80:31878/TCP&rdquo;</p>
<p>To see if we can reach the container, run:</p>
<p><code>curl localhost:31878</code></p>
<p>If everything went well, you will get back the HTML of the default index page served by NGINX:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">ubuntu@instance-20230205-0909:~$ curl localhost:31878
</span></span><span class="line"><span class="cl">&lt;!DOCTYPE html&gt;
</span></span><span class="line"><span class="cl">&lt;html&gt;
</span></span><span class="line"><span class="cl">&lt;head&gt;
</span></span><span class="line"><span class="cl">&lt;title&gt;Welcome to nginx!&lt;/title&gt;
</span></span><span class="line"><span class="cl">&lt;style&gt;
</span></span><span class="line"><span class="cl">html <span class="o">{</span> color-scheme: light dark<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">body <span class="o">{</span> width: 35em<span class="p">;</span> margin: <span class="m">0</span> auto<span class="p">;</span>
</span></span><span class="line"><span class="cl">font-family: Tahoma, Verdana, Arial, sans-serif<span class="p">;</span> <span class="o">}</span>
</span></span><span class="line"><span class="cl">&lt;/style&gt;
</span></span><span class="line"><span class="cl">&lt;/head&gt;
</span></span><span class="line"><span class="cl">&lt;body&gt;
</span></span><span class="line"><span class="cl">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
</span></span><span class="line"><span class="cl">&lt;p&gt;If you see this page, the nginx web server is successfully installed and
</span></span><span class="line"><span class="cl">working. Further configuration is required.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;For online documentation and support please refer to
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.org/&#34;</span>&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
</span></span><span class="line"><span class="cl">Commercial support is available at
</span></span><span class="line"><span class="cl">&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&#34;http://nginx.com/&#34;</span>&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&lt;p&gt;&lt;em&gt;Thank you <span class="k">for</span> using nginx.&lt;/em&gt;&lt;/p&gt;
</span></span><span class="line"><span class="cl">&lt;/body&gt;
</span></span><span class="line"><span class="cl">&lt;/html&gt;
</span></span></code></pre></div><p>To reach the pod from the browser, open your port in the security group configured for the subnet of your VM.</p>
<p>Good luck with your new lab environment!</p>
<h2 id="links">Links</h2>
<p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></p>
<p><a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></p>
<p><a href="https://github.com/flannel-io/flannel">https://github.com/flannel-io/flannel</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Get a free 4 CPU 24GB Ram VM on from Oracle</title>
      <link>https://mischavandenburg.com/zet/free-oracle-vm/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mischavandenburg.com/zet/free-oracle-vm/</guid>
      <description>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.
You can host a 4CPU 24GB VM for free!
This is perfect for a lab environment.
I spent my evening creating the VM and setting up a kubernetes cluster from scratch.
Use this video to claim your free vm:
https://www.youtube.com/watch?v=NKc3k7xceT8</description>
      <content:encoded><![CDATA[<p>A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.</p>
<p>You can host a 4CPU 24GB VM for free!</p>
<p>This is perfect for a lab environment.</p>
<p>I spent my evening creating the VM and setting up a kubernetes cluster from scratch.</p>
<p>Use this video to claim your free vm:</p>
<p><a href="https://www.youtube.com/watch?v=NKc3k7xceT8">https://www.youtube.com/watch?v=NKc3k7xceT8</a></p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
