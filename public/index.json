[{"content":" [!IMPORTANT] Remember to create a custom image including iscsi-tools when using the Synology CSI driver\nSet up CLI environment and set databases to maintenance mode export TALOS_CP=\u0026#34;192.168.100.107\u0026#34; export TALOS_W1=\u0026#34;192.168.100.245\u0026#34; export TALOS_W2=\u0026#34;192.168.100.60\u0026#34; export TALOS_W3=\u0026#34;192.168.100.228\u0026#34; # Set databases to maintenance mode k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters. Only needed for the control plane node.\nUpgrade path from 1.7.5 to 1.7.7 from 1.7.7 to 1.8.2 Upgrade 1 export TALOS_IMAGE=\u0026#39;factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.7\u0026#39; talosctl upgrade --preserve --nodes $TALOS_CP -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --wait --debug --nodes $TALOS_W1 -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --wait --debug --nodes $TALOS_W2 -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --wait --debug --nodes $TALOS_W3 -e $TALOS_CP --image $TALOS_IMAGE Upgrade 2 export TALOS_IMAGE=\u0026#39;factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.8.2\u0026#39; talosctl upgrade --preserve --nodes $TALOS_CP -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --nodes $TALOS_W1 -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --nodes $TALOS_W2 -e $TALOS_CP --image $TALOS_IMAGE talosctl upgrade --nodes $TALOS_W3 -e $TALOS_CP --image $TALOS_IMAGE Kubernetes Upgrade [!WARNING] Make sure to upgrade the Talos CLI before proceeding. \u0026ldquo;It is advisable to use the same version of talosctl as the version of the boot media used.\u0026rdquo; - Talos Docs\nUpgrade Path From 1.30.3 to 1.30.5 From 1.30.5 to 1.31.1 [!NOTE] To trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to upgrade to, such as: talosctl \u0026ndash;nodes upgrade-k8s \u0026ndash;to 1.30.0 Note that the \u0026ndash;nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded. To check what will be upgraded you can run talosctl upgrade-k8s with the \u0026ndash;dry-run flag\nk cnp maintenance set --all-namespaces kubent talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.31.1 --dry-run talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.31.1 Links 202411140811\n","permalink":"https://mischavandenburg.com/zet/talos-homelab-upgrade-guide-october/","summary":"[!IMPORTANT] Remember to create a custom image including iscsi-tools when using the Synology CSI driver\nSet up CLI environment and set databases to maintenance mode export TALOS_CP=\u0026#34;192.168.100.107\u0026#34; export TALOS_W1=\u0026#34;192.168.100.245\u0026#34; export TALOS_W2=\u0026#34;192.168.100.60\u0026#34; export TALOS_W3=\u0026#34;192.168.100.228\u0026#34; # Set databases to maintenance mode k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters. Only needed for the control plane node.\nUpgrade path from 1.7.5 to 1.7.7 from 1.7.7 to 1.","title":"Talos Homelab Upgrade Guide October"},{"content":"Now that I\u0026rsquo;m daily driving Arch Linux again, I\u0026rsquo;ve been thinking about how to handle my tasks. I use Apple Reminders because the iWatch functionality with Siri is so good. I\u0026rsquo;ve been thinking of setting up a self-hosted solution, but then I would lose the Siri integration.\nIn the meantime, I figured I would just try to access Apple Reminders from Linux. Using it from the browser worked surprisingly well.\nCreated a desktop file here with the name icloud.desktop:\n/home/mischa/.local/share/applications\nWith these contents:\n[Desktop Entry] Name=iCloud Exec=firefox --new-instance --profile /home/mischa/.mozilla/firefox/icloud-profile --kiosk https://icloud.com Icon=firefox Type=Application Categories=Network;WebBrowser; Now I can run wofi and search for iCloud, and I have my iCloud applications running in a clean separate window within a workspace.\nAlso, I can configure Hyprland to run this application in a specific workspace on startup.\nLinks 202409060909\n","permalink":"https://mischavandenburg.com/zet/running-icloud-web-as-a-desktop-application-on-arch-linux-with-hyprland/","summary":"Now that I\u0026rsquo;m daily driving Arch Linux again, I\u0026rsquo;ve been thinking about how to handle my tasks. I use Apple Reminders because the iWatch functionality with Siri is so good. I\u0026rsquo;ve been thinking of setting up a self-hosted solution, but then I would lose the Siri integration.\nIn the meantime, I figured I would just try to access Apple Reminders from Linux. Using it from the browser worked surprisingly well.","title":"Running iCloud Web as a Desktop Application on Arch Linux with Hyprland"},{"content":"I\u0026rsquo;ve been daily driving Qutebrowser for over a month now, and I\u0026rsquo;m starting to feel at home in it.\nSince it doesn\u0026rsquo;t have any extensions, you have to hack things together yourself, which is why I like it so much.\nToday I wanted to add an article to my Wallabag instance, which I usually did by using the browser extension.\nI installed this Wallabag CLI:\nhttps://github.com/artur-shaik/wallabag-client\npipx install wallabag-client\nAfter setting up the config I can now add entries from the command line. By running wallabag add [url]. Neat!\nNext, we can use the spawn command from within Qutebrowser to call this CLI command:\nspawn -v -m wallabag add {url}\nAlias And to make it easier, I added a :wallabag alias by adding this to my config.py:\nc.aliases = { \u0026#39;w\u0026#39;: \u0026#39;session-save\u0026#39;, \u0026#39;q\u0026#39;: \u0026#39;close\u0026#39;, \u0026#39;qa\u0026#39;: \u0026#39;quit\u0026#39;, \u0026#39;wq\u0026#39;: \u0026#39;quit --save\u0026#39;, \u0026#39;wqa\u0026#39;: \u0026#39;quit --save\u0026#39;, \u0026#39;wallabag\u0026#39;: \u0026#39;spawn -v -m wallabag add {url}\u0026#39; } Now I can just run :wallabag from within Qutebrowser and it will add the article.\nBuild It Yourself I\u0026rsquo;m really starting to love Qutebrowser for much of the same reasons why I love (neo)vim so much. I can call CLI commands from within my editor (or browser) and interact with my system that way.\nSince you can do almost anything from the CLI, you can build some very powerful workflows this way.\nNext, I\u0026rsquo;m going to do the same for my Linkding instance so I can add bookmarks from within Qutebrowser.\nFeels nice that I don\u0026rsquo;t need any extensions for this!\nLinks https://www.reddit.com/r/qutebrowser/comments/fgc0ol/is_there_a_way_to_make_this_wallabag_bookmarklet/\n202409020809\n","permalink":"https://mischavandenburg.com/zet/adding-articles-to-wallabag-with-qutebrowser/","summary":"I\u0026rsquo;ve been daily driving Qutebrowser for over a month now, and I\u0026rsquo;m starting to feel at home in it.\nSince it doesn\u0026rsquo;t have any extensions, you have to hack things together yourself, which is why I like it so much.\nToday I wanted to add an article to my Wallabag instance, which I usually did by using the browser extension.\nI installed this Wallabag CLI:\nhttps://github.com/artur-shaik/wallabag-client\npipx install wallabag-client\nAfter setting up the config I can now add entries from the command line.","title":"Adding Articles to Wallabag From Qutebrowser"},{"content":"Transition to manifest V3 is now starting and Ublock Origin might not be supported in Chrome in the future.\nBecause of my transition from a mouse to a pen tablet, I\u0026rsquo;ve already been experimenting with Qutebrowser for the past couple of weeks and I really enjoy it. It feels much closer to vim because I can run commands like this: \u0026ldquo;:set darkmode\u0026rdquo; which feels familiar and I can search my history very easily.\nThough it uses elements from Brave\u0026rsquo;s ad blocker, the QtWebEngine is based on Chromium so I expect this will be affected by the manifest V3 rollout as well. Which means that I need another way to block ads.\nSo, I\u0026rsquo;m setting up PiHole to add another layer of ad blocking to the setup and to give a hand to Qutebrowser.\nAs I have a YouTube premium subscription I\u0026rsquo;m already enjoying YouTube without ads. I happily pay for it because I also use it for YouTube music and it\u0026rsquo;s a way to support musicians and creators without having to suffer ads.\nBut I can\u0026rsquo;t stand ads while I\u0026rsquo;m reading blog posts.\nI\u0026rsquo;m happy to report that I now have PiHole running and all my traffic is routed through it. Having a Synology and Unifi router made this a piece of cake.\nI didn\u0026rsquo;t want the Pi-Hole in my Kubernetes cluster so I\u0026rsquo;m just running it on my Synology in a docker container.\nThen I set the DNS server for the VLAN to point towards my Synology and done!\nVery interesting to scroll through the query logs and see what\u0026rsquo;s being blocked.\nAt first, I had it set up like this:\nHow to Run Pi-hole on a Synology NAS - Pi My Life Up\nversion: \u0026#34;3\u0026#34; services: pihole: container_name: pihole image: pihole/pihole:latest ports: - \u0026#34;53:53/tcp\u0026#34; - \u0026#34;53:53/udp\u0026#34; - \u0026#34;67:67/udp\u0026#34; - \u0026#34;3009:80/tcp\u0026#34; environment: TZ: \u0026#39;CET\u0026#39; WEBPASSWORD: \u0026#39;yum apple pie\u0026#39; volumes: - \u0026#39;./etc-pihole:/etc/pihole\u0026#39; - \u0026#39;./etc-dnsmasq.d:/etc/dnsmasq.d\u0026#39; cap_add: - NET_ADMIN restart: unless-stopped Using Macvlan Later I wanted it available on a separate IP. This allowed me to view the individual clients in Pi-hole. In other words, I can see exactly which device the requests are coming from. When running it on the same IP as the Synology, I only saw the IP address of the internal docker network on the Synology.\nPi-hole in Container Manager on a Synology NAS (drfrankenstein.co.uk)\nservices: pihole-macvlan: image: pihole/pihole:latest container_name: pihole-macvlan cap_add: - CAP_NET_RAW - CAP_NET_BIND_SERVICE - CAP_CHOWN environment: - PIHOLE_UID=1028 - PIHOLE_GID=65536 - TZ=CET - WEBPASSWORD=letseatsomemorepie - DNSMASQ_LISTENING=all - WEB_PORT=3001 - DNSMASQ_USER=pihole - FTLCONF_LOCAL_IPV4=192.168.120.10 volumes: - \u0026#34;./etc-pihole:/etc/pihole\u0026#34; - \u0026#34;./etc-dnsmasq.d:/etc/dnsmasq.d\u0026#34; networks: macvlan: ipv4_address: 192.168.120.10 restart: always networks: macvlan: name: macvlan driver: macvlan driver_opts: parent: eth0 ipam: config: - subnet: \u0026#34;192.168.120.0/24\u0026#34; ip_range: \u0026#34;192.168.120.254/24\u0026#34; gateway: \u0026#34;192.168.120.1\u0026#34; ","permalink":"https://mischavandenburg.com/zet/i-set-up-pi-hole-on-my-synology-nas/","summary":"Transition to manifest V3 is now starting and Ublock Origin might not be supported in Chrome in the future.\nBecause of my transition from a mouse to a pen tablet, I\u0026rsquo;ve already been experimenting with Qutebrowser for the past couple of weeks and I really enjoy it. It feels much closer to vim because I can run commands like this: \u0026ldquo;:set darkmode\u0026rdquo; which feels familiar and I can search my history very easily.","title":"I set up Pi-hole on my Synology NAS"},{"content":"I upgraded my homelab cluster using Talos Linux today.\nI made a mistake and forgot to use the custom-built image using iscsi-tools.\nLearning the hard way.\nHere are my notes:\nTalos upgrade [!IMPORTANT] Remember to create a custom image including iscsi-tools\nSet up CLI environment and set databases to maintenance mode export TALOS_CP=\u0026#34;192.168.100.107\u0026#34; export TALOS_W1=\u0026#34;192.168.100.245\u0026#34; export TALOS_W2=\u0026#34;192.168.100.60\u0026#34; k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters. Only needed for the control plane node.\nUpgrade path: from 1.6.4 to 1.6.8 from 1.6.8 to 1.7.5 Upgrade 1 [!NOTE] I made a mistake and did not update to the custom-built image including the iscsi-tools This caused my cluster storage to break.\ntalosctl upgrade --preserve --nodes $TALOS_CP -e $TALOS_CP --image ghcr.io/siderolabs/installer:v1.6.8 talosctl upgrade --nodes $TALOS_W1 -e $TALOS_CP --image ghcr.io/siderolabs/installer:v1.6.8 talosctl upgrade --nodes $TALOS_W2 -e $TALOS_CP --image ghcr.io/siderolabs/installer:v1.6.8 Upgrade 2 I went to https://factory.talos.dev/ and created a new image including iscsi-tools.\nEverything worked fine after that.\ntalosctl upgrade --preserve --nodes $TALOS_CP -e $TALOS_CP --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5 talosctl upgrade --nodes $TALOS_W1 -e $TALOS_CP --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5 talosctl upgrade --nodes $TALOS_W2 -e $TALOS_CP --image factory.talos.dev/installer/c9078f9419961640c712a8bf2bb9174933dfcf1da383fd8ea2b7dc21493f8bac:v1.7.5 Kubernetes Upgrade [!WARNING] Make sure to upgrade the Talos CLI before proceeding. \u0026ldquo;It is advisable to use the same version of talosctl as the version of the boot media used.\u0026rdquo; - Talos Docs\nUpgrade Path From 1.29.0 to 1.29.3 From 1.29.3 to 1.30.3 [!NOTE]\n[!NOTE] To trigger a Kubernetes upgrade, issue a command specifying the version of Kubernetes to upgrade to, such as: talosctl \u0026ndash;nodes upgrade-k8s \u0026ndash;to 1.30.0 Note that the \u0026ndash;nodes parameter specifies the control plane node to send the API call to, but all members of the cluster will be upgraded. To check what will be upgraded you can run talosctl upgrade-k8s with the \u0026ndash;dry-run flag\nRound 1 k cnp maintenance set --all-namespaces kubent talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.29.3 --dry-run talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.29.3 Round 2 k cnp maintenance set --all-namespaces kubent talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.30.3 --dry-run talosctl --nodes $TALOS_CP -e $TALOS_CP upgrade-k8s --to 1.30.3 Links: 202407311407\n","permalink":"https://mischavandenburg.com/zet/talos-linux-upgrade-guide-july/","summary":"I upgraded my homelab cluster using Talos Linux today.\nI made a mistake and forgot to use the custom-built image using iscsi-tools.\nLearning the hard way.\nHere are my notes:\nTalos upgrade [!IMPORTANT] Remember to create a custom image including iscsi-tools\nSet up CLI environment and set databases to maintenance mode export TALOS_CP=\u0026#34;192.168.100.107\u0026#34; export TALOS_W1=\u0026#34;192.168.100.245\u0026#34; export TALOS_W2=\u0026#34;192.168.100.60\u0026#34; k cnp maintenance set --all-namespaces [!WARNING] Use the --preserve flag on single-node control plane clusters.","title":"Talos Linux Upgrade Guide July"},{"content":"I relied on iCloud sync for my Obsidian backup for many years. However, my vault has grown significantly and has reached nearly 3500 notes. This leads to problems when opening the vault on other iOs devices, because iCloud sync removes files from devices when they are not used. When the vault is reopened, it takes over 5 minutes to sync everything. Having this 5 minute waiting time every time I open up my vault on my iPad was a problem which I needed to address.\nThese small frictions can become large irritations, even though I only use Obsidian on my iPad very occasionally.\nTo alleviate this problem I switched to Obsidian Sync. It\u0026rsquo;s 4 dollars a month, and a great way to support the project. I\u0026rsquo;ve had Sync before, but switched to iCloud when I entered the Apple ecosystem.\nGit In addition to iCloud and Obsidian Sync, I\u0026rsquo;ve also been backing up my vault to GitHub. I used this to sync my vault to Linux devices before I purchased Obsidian Sync again.\nSyncing with Git has often led to problems. I\u0026rsquo;ve spent many hours reverting faulty commits where I was about to lose a bunch of data.\nAnother glaring problem is that I\u0026rsquo;m committing my entire web of knowledge, my deepest thoughts and reflections and personal information to GitHub. Even though it is in a private repository, it\u0026rsquo;s likely that Microsoft can access it some way or other if they want to. I haven\u0026rsquo;t researched it thoroughly, but using a free service like GitHub usually comes with privacy sacrifices.\nI didn\u0026rsquo;t mind it that much, but it was something that was gnawing on the back of my mind for years.\nToday I pulled the plug on it. It felt strange to delete the repo, but it\u0026rsquo;s gone now.\nRsync + iCloud My notes are safely backed up using Obsidian Sync. It\u0026rsquo;s all encrypted at rest and in transfer. But I also want to have my own backups.\nTo be absolutely safe I\u0026rsquo;m also backing up the vault to my iCloud drive.\nYeah, now I\u0026rsquo;m giving Apple potential access to my files. But I think my data is safer with Apple than with GitHub where it can probably be used for training AI models.\nThe next step will be to back it up to my Synology NAS, but I\u0026rsquo;m running into some problems with mounting the SMB share from the command line, so I\u0026rsquo;ll save that for later.\nI\u0026rsquo;m using the following script to back up my Zettelkasten to my iCloud drive. It runs twice a day using a cron job on my MacBook Pro.\n#!/bin/bash SOURCE_DIR=\u0026#34;$HOME/Zettelkasten\u0026#34; DEST_DIR=\u0026#34;$HOME/Library/Mobile Documents/com~apple~CloudDocs/backup/Zettelkasten\u0026#34; mkdir -p \u0026#34;$DEST_DIR\u0026#34; rsync -av --delete \u0026#34;$SOURCE_DIR/\u0026#34; \u0026#34;$DEST_DIR\u0026#34; cd \u0026#34;$DEST_DIR\u0026#34; || { echo \u0026#34;Error: Unable to change to destination directory\u0026#34; exit 1 } git add -A git commit -m \u0026#34;Automatic backup commit $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)\u0026#34; echo \u0026#34;Backup and commit completed successfully!\u0026#34; Keeping Git As you can see, I\u0026rsquo;m still committing changes every time I back up. I\u0026rsquo;m keeping it as a local Git repo so I can revert commits when something unexpected happens.\nIf push comes to shove, I can always push (😏) it to a Git instance again, or even to GitHub.\nI considered running my private Git instance in my homelab cluster, but I also don\u0026rsquo;t want my Zettelkasten to be available there in case someone manages to break into my cluster. Even though I\u0026rsquo;m running enterprise grade security with Talos, there is always a risk because I\u0026rsquo;m exposing some services to the internet.\n","permalink":"https://mischavandenburg.com/zet/my-updated-backup-setup-for-my-obsidian-zetelkasten/","summary":"I relied on iCloud sync for my Obsidian backup for many years. However, my vault has grown significantly and has reached nearly 3500 notes. This leads to problems when opening the vault on other iOs devices, because iCloud sync removes files from devices when they are not used. When the vault is reopened, it takes over 5 minutes to sync everything. Having this 5 minute waiting time every time I open up my vault on my iPad was a problem which I needed to address.","title":"No More GitHub - My Updated Backup Setup For My Obsidian Zettelkasten"},{"content":"I\u0026rsquo;m planning to do a series on Linux on my YouTube channel. The idea is to cover the fundamentals of Linux, either by teaching them myself or to link to existing resources. When the basics are done, I will take over and we\u0026rsquo;ll build an Arch Linux desktop together.\nI use my MacBook for normal productive work and coding, and I have a Thinkpad T480 which I have for experimentation, and mostly for writing notes and browsing while I\u0026rsquo;m in the living room. It was running Fedora using the Sway spin, and it worked fine.\nBut Arch Linux has been gnawing at me ever since I started my community. We\u0026rsquo;ve had some lovely discussions about distros and window managers, and I almost always commented that I enjoyed building my Arch Linux system.\nWell, I went back to it!\nThis time I\u0026rsquo;m going even more minimal. I took the challenge of not using NetworkManager, and I figured I would go with systemd-boot right away too.\nSome of you might remember the problems that happened with GRUB about 2 years ago. Systemd-boot feels so much more intuitive and straightforward to configure.\nSo now I have a super clean and minimal system using system-boot and systemd-networkd with iwd.\nGetting internet to work was a bit of a challenge but figured it out eventually.\nUsing systemd-resolved, I currently don\u0026rsquo;t even have a /etc/hosts file on my Arch Linux system. That\u0026rsquo;s a new one for me.\nThis is the current pstree while ssh\u0026rsquo;d into my Thinkpad from the MacBook.\nI\u0026rsquo;m so happy with the current minimal state of it!\n[mischa@thinkpad ~]$ pstree systemd─┬─dbus-broker-lau───dbus-broker ├─iwd ├─sshd───sshd-session───sshd-session───bash───pstree ├─systemd───(sd-pam) ├─systemd-journal ├─systemd-logind ├─systemd-network ├─systemd-resolve ├─systemd-udevd └─systemd-userdbd───3*[systemd-userwor] ","permalink":"https://mischavandenburg.com/zet/the-return-of-arch-linux/","summary":"I\u0026rsquo;m planning to do a series on Linux on my YouTube channel. The idea is to cover the fundamentals of Linux, either by teaching them myself or to link to existing resources. When the basics are done, I will take over and we\u0026rsquo;ll build an Arch Linux desktop together.\nI use my MacBook for normal productive work and coding, and I have a Thinkpad T480 which I have for experimentation, and mostly for writing notes and browsing while I\u0026rsquo;m in the living room.","title":"The Return of Arch Linux"},{"content":"Just realized I never posted on the blog that I achieved the CKS a month ago.\nI even created a study guide for it:\nLinks: 202406152006\n","permalink":"https://mischavandenburg.com/zet/got-my-cks/","summary":"Just realized I never posted on the blog that I achieved the CKS a month ago.\nI even created a study guide for it:\nLinks: 202406152006","title":"Got my CKS"},{"content":"I\u0026rsquo;ve been obsessed with Devcontainers and Devpods recently, and I\u0026rsquo;m learning a lot about portable dev environments. This is truly the next level of config management and the future of dev environments in my opinion.\nI relied on brew as my package manager, however, I ran into problems when I wanted to run my Linux dev containers on my M2 Silicon Mac. Linux brew is not supported on ARM architecture.\nUse case I all I need to do is install a list of packages into a linux container. I was using brew because I need to install packages like k9s and flux which are sometimes not available in the standard package repositories.\nAfter some tinkering with Nix I managed to solve my problem.\nNixpkgs My dotfiles contains this setup script:\n#!/bin/bash export XDG_CONFIG_HOME=\u0026#34;$HOME\u0026#34;/.config mkdir -p \u0026#34;$XDG_CONFIG_HOME\u0026#34; mkdir -p \u0026#34;$XDG_CONFIG_HOME\u0026#34;/nixpkgs ln -sf \u0026#34;$PWD/nvim\u0026#34; \u0026#34;$XDG_CONFIG_HOME\u0026#34;/nvim ln -sf \u0026#34;$PWD/.bash_profile\u0026#34; \u0026#34;$HOME\u0026#34;/.bash_profile ln -sf \u0026#34;$PWD/.bashrc\u0026#34; \u0026#34;$HOME\u0026#34;/.bashrc ln -sf \u0026#34;$PWD/.inputrc\u0026#34; \u0026#34;$HOME\u0026#34;/.inputrc ln -sf \u0026#34;$PWD/.tmux.conf\u0026#34; \u0026#34;$HOME\u0026#34;/.tmux.conf ln -sf \u0026#34;$PWD/config.nix\u0026#34; \u0026#34;$XDG_CONFIG_HOME\u0026#34;/nixpkgs/config.nix # install Nix packages from config.nix nix-env -iA nixpkgs.myPackages It sets up the nixpkgs directory and the config.nix.\nThe config.nix contains the packages I want to install:\n{ packageOverrides = pkgs: with pkgs; { myPackages = pkgs.buildEnv { name = \u0026#34;mischa-tools\u0026#34;; paths = [ neovim go nodejs_22 starship fd ripgrep lazygit kubectl k9s fluxcd ]; }; }; } Finally, the nix-env -iA nixpkgs.myPackages command installs all of the packages into my environment.\nI was lucky to find this in the Nix documentation:\nhttps://nixos.org/manual/nixpkgs/stable/#sec-declarative-package-management\nHad I not found this particular heading, I think I would have been lost in the weeds for a long time. But Nix has definitely piqued my interest and I look forward to learn more about it.\nLinks: Devpod dotfiles: https://github.com/mischavandenburg/dotfiles-devpod\n202405261105\n","permalink":"https://mischavandenburg.com/zet/installing-packages-in-devcontainers-with-nix/","summary":"I\u0026rsquo;ve been obsessed with Devcontainers and Devpods recently, and I\u0026rsquo;m learning a lot about portable dev environments. This is truly the next level of config management and the future of dev environments in my opinion.\nI relied on brew as my package manager, however, I ran into problems when I wanted to run my Linux dev containers on my M2 Silicon Mac. Linux brew is not supported on ARM architecture.","title":"Installing packages in Devcontainers with Nix"},{"content":"This is hands down the best CLI for downloading YouTube videos.\nI needed this to transfer some of my private videos to Vimeo. Don\u0026rsquo;t use this to break YouTube\u0026rsquo;s terms please.\nhttps://github.com/kkdai/youtube\nLinks: 202405121505\n","permalink":"https://mischavandenburg.com/zet/best-way-to-download-youtube-videos-from-cli/","summary":"This is hands down the best CLI for downloading YouTube videos.\nI needed this to transfer some of my private videos to Vimeo. Don\u0026rsquo;t use this to break YouTube\u0026rsquo;s terms please.\nhttps://github.com/kkdai/youtube\nLinks: 202405121505","title":"Best Way To Download Youtube Videos From Cli"},{"content":"Introduction I\u0026rsquo;m working on a project where I\u0026rsquo;m migrating an Azure Pipeline to GitHub Actions. So far I\u0026rsquo;ve found the GitHub Actions very intuitive to work with and it was a very easy transition from Azure Pipelines.\nOne requirement was to increase the version with every build. In the previous setup they were using the build ID, but I\u0026rsquo;m an advocate of always using semantic versioning if possible, so I wondered if this could be done using the GitHub Actions.\nThis setup increments all of the Docker images regardless if they have any changes. I did not like this way of deployment, but it was the requirement from the project so I had to implement it that way. I would have preferred to version each container image separately.\nIncreasing Versions The first problem I had to solve was to increase the tags on every commit.\nI used the following action from the marketplace for increasing the versions based on conventional commits:\nSemver Conventional Commits · Actions · GitHub Marketplace\nIn the example below, I\u0026rsquo;m running a separate job to increase the version based on the latest tag of the repository. The job has an output which I can use later to tag the images themselves.\njobs: prepare_tag: outputs: tag: ${{ steps.semver.outputs.next }} permissions: contents: write runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 # Defaults to patch version, will bump minor on \u0026#34;feat\u0026#34; commit - name: Get Next Version id: semver uses: ietf-tools/semver-action@v1 with: token: ${{ secrets.GITHUB_TOKEN }} branch: master patchAll: true - name: Push tag id: tag_version uses: mathieudutour/github-tag-action@v6.2 with: github_token: ${{ secrets.GITHUB_TOKEN }} custom_tag: ${{ steps.semver.outputs.next }} tag_prefix: \u0026#34;\u0026#34; The Get Next Version step increments the version based on the configuration provided. I used the patchAll: true setting to always default on a patch incrementation if the word \u0026ldquo;feat\u0026rdquo; is not in the commit. If the commit would contain \u0026ldquo;BREAKING CHANGE\u0026rdquo;, the major version would be incremented.\nTo push the new version to the current repo, I used this action:\nGitHub Tag · Actions · GitHub Marketplace\nSince the semver-action already adds a \u0026ldquo;v\u0026rdquo; to the tag, I adjusted the tag_prefix to \u0026quot;\u0026quot;.\nManaging Permissions On the main page of your organization, go to Packages Go to the package and open \u0026ldquo;Package settings\u0026rdquo; in the right sidebar Under Manage Actions access, add the repo as the source Make sure to allow Write acces In the source repo, where the GitHub Actions Workflow is running, go to settings, actions, select the workflow, and add write permission there too.\nFinally, we also need to add permissions in the workflow yaml:\nbuild_and_push: name: Build image \u0026amp; push runs-on: ubuntu-latest permissions: contents: write packages: write See also:\ngithub actions - ERROR: denied: installation not allowed to Create organization package - Stack Overflow\ndenied: installation not allowed to Create organization package · Issue #606 · docker/build-push-action (github.com)\nLooping over multiple Dockerfiles The last problem I needed to solve was that I needed build multiple images. To make it a bit cleaner, I didn\u0026rsquo;t want to have a code block for each. After some research I found a way to loop over multiple values in GitHub Actions using matrices.\nIn the example below, I set three variables in the matrix and each of these are called in the Build and push step.\nbuild_and_push: permissions: contents: write packages: write runs-on: ubuntu-latest strategy: matrix: include: - image: ghcr.io/ssi-dk/sap-web dockerfile: app/Dockerfile path: app - image: ghcr.io/ssi-dk/sap-api dockerfile: web/Dockerfile path: web - image: ghcr.io/ssi-dk/bifrost-queue-broker dockerfile: bifrost/bifrost_queue_broker/Dockerfile path: bifrost/bifrost_queue_broker steps: - name: Checkout uses: actions/checkout@v4 - name: Login to GitHub Container Registry uses: docker/login-action@v3 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} # loops over all images in the matrix defined on top - name: Build and push uses: docker/build-push-action@v5 with: context: ${{ matrix.path }} platforms: linux/amd64 tags: ${{ matrix.image }}:${{ needs.prepare_tag.outputs.tag }} file: ${{ matrix.dockerfile }} push: true This was a very fun challenge to solve and I have learned a lot about GitHub actions. It didn\u0026rsquo;t take much time at all to figure all of this out because all of the actions in the marketplace are open source and you can just read the code to see if they suit your needs. So far I\u0026rsquo;m very impressed with GitHub Actions, coming from Azure Pipelines.\nLinks: 202404301804\n","permalink":"https://mischavandenburg.com/zet/articles/building-multiple-docker-images-using-automatic-versioning-using-github-actions/","summary":"Introduction I\u0026rsquo;m working on a project where I\u0026rsquo;m migrating an Azure Pipeline to GitHub Actions. So far I\u0026rsquo;ve found the GitHub Actions very intuitive to work with and it was a very easy transition from Azure Pipelines.\nOne requirement was to increase the version with every build. In the previous setup they were using the build ID, but I\u0026rsquo;m an advocate of always using semantic versioning if possible, so I wondered if this could be done using the GitHub Actions.","title":"Building Multiple Docker Images Using Automatic Versioning Using GitHub Actions"},{"content":"When solving a killercoda challenge I ran into some confusion. Even though my solution worked, there was a difference which I wanted to get clear on.\nI wrote this:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np namespace: space1 spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: space2 - to: ports: - protocol: TCP port: 53 - protocol: UDP port: 53 But the provided course solution was this:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np namespace: space1 spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: space2 - ports: - port: 53 protocol: TCP - port: 53 protocol: UDP Do you see the difference? I used the to: field before the ports: field.\nMy solution was correct, but it had me quite confused, why wouldn\u0026rsquo;t they write to: here as well?\nAfter some further digging I\u0026rsquo;ve finally understood that the to: field is only necessary when you apply a selector. If the to: field is omitted, it means that it will apply to any destination. So in my solution, the to: field doesn\u0026rsquo;t achieve anything because I\u0026rsquo;m not applying any selector. It can therefore be omitted.\nEach element under the egress: array is a separate rule and they are combined to reach the end result of allowing egress traffic to namespace space2, and UDP/TCP traffic to anywhere on port 53.\nIn this case, omitting the to: field makes sense because DNS is generally not limited to only one namespace. However, if you\u0026rsquo;re only using the internal Kubernetes DNS you might want to limit it to the kube-system namespace. I\u0026rsquo;ve done this in a CiliumNetworkPolicy in my homelab:\napiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: commafeed-app namespace: commafeed spec: endpointSelector: matchLabels: policy-type: \u0026#34;app\u0026#34; ingress: - fromEndpoints: - {} - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: monitoring - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: postgresql-operator-system egress: - toEndpoints: - matchLabels: io.kubernetes.pod.namespace: kube-system k8s-app: kube-dns toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP rules: dns: - matchPattern: \u0026#34;*\u0026#34; - toEndpoints: - {} - toEntities: - world toPorts: - ports: - port: \u0026#34;443\u0026#34; - ports: - port: \u0026#34;80\u0026#34; - ports: - port: \u0026#34;7844\u0026#34; Links: part of:: [[Network Policies]] [[CKS]] [[networking-computers]]\n202403250845\n","permalink":"https://mischavandenburg.com/zet/alleviating-confusion-about-the-to-field-in-network-policies/","summary":"When solving a killercoda challenge I ran into some confusion. Even though my solution worked, there was a difference which I wanted to get clear on.\nI wrote this:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: np namespace: space1 spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: space2 - to: ports: - protocol: TCP port: 53 - protocol: UDP port: 53 But the provided course solution was this:","title":"Alleviating Confusion About The To Field In Network Policies"},{"content":"There are two things which I\u0026rsquo;m dreading most for the CKS exam: writing Network Policies and configuring RBAC under heavy time pressure.\nI\u0026rsquo;ve heard from colleagues that the exam has many questions of this nature, so it will be a good way of scoring points. However, regardless of the exam, the fact that I\u0026rsquo;m dreading these topics also shows that I\u0026rsquo;m not familiar enough with them and therefore this is a point where I should improve. It\u0026rsquo;s not only about the exam, but also my growth of expertise as a Kubernetes Engineer.\nWith the help of https://networkpolicy.io/ I\u0026rsquo;ve created CiliumNetworkpolicies for my homelab and the publically available stuff is pretty well secured. It\u0026rsquo;s not that I don\u0026rsquo;t understand them, but I haven\u0026rsquo;t practiced them enough to do them without having to spend a lot of time or relying on external tools. And the time pressure is the biggest challenge for the CKA / CKS exams.\nFor the weeks in my preparation of the CKS exam I will do Network Policy and RBAC exercises on Killercoda every day. They have multiple scenarios for the CKS, but also CKA and CKAD so there are a few things I can practice every day. I\u0026rsquo;m told that when I can solve these without problems, I know enough to do the exam questions without problem.\nBetter than I thought I streamed my first 3 hour study session live and after I worked through the video module about Network Policies I went ahead and tackled the practical scenarios.\nTo my great surprise I was able to solve them without much trouble! I was slow and needed to talk myself through the code a few times, but I did manage to solve all of the challenges for the CKS on Killercoda. There was never a point where I was unsure of what to do, or a problem I couldn\u0026rsquo;t fix.\nThis was quite an eye opening experience because I had expected some weeks of struggle with this. Perhaps I\u0026rsquo;m better at this stuff than I give myself credit for. In any case, I am still going to do daily practice with these scenarios and possibly thinking of a few of my own when I run out of Killercoda practice.\nEven though I managed to solve the challenges during this first round, being in an exam under heavy time pressure and feeling nervous is going to be a completely different experience. So I want to get this down to the point where I hardly have to think about it any more, I need to master these questions so I can spend my time on the big questions.\nThis experience has me wondering how RBAC is going to go. We\u0026rsquo;ll find out soon enough!\nLinks: 202403250703\n","permalink":"https://mischavandenburg.com/zet/im-better-at-network-policies-than-i-thought/","summary":"There are two things which I\u0026rsquo;m dreading most for the CKS exam: writing Network Policies and configuring RBAC under heavy time pressure.\nI\u0026rsquo;ve heard from colleagues that the exam has many questions of this nature, so it will be a good way of scoring points. However, regardless of the exam, the fact that I\u0026rsquo;m dreading these topics also shows that I\u0026rsquo;m not familiar enough with them and therefore this is a point where I should improve.","title":"I'm better at Network Policies than I thought"},{"content":"It seems like I will have a couple of hours a day which I can allot to study in the coming month. This will be the perfect moment to start working on a goal I\u0026rsquo;ve had for a long time: getting my Certified Kubernetes Security Specialist certification.\nt\u0026rsquo;s not going to be easy. I\u0026rsquo;m quite intimidated by all of the new things I\u0026rsquo;ll have to learn. But that\u0026rsquo;s a good sign. It\u0026rsquo;s the way it works. Everybody would be doing this if it was easy, and I can only grow by doing hard things that seem intimidating at first.\nGame Plan Work through this 11 hour course: https://www.youtube.com/watch?v=d9xfB5qaOfg\u0026amp;t=3921s\u0026amp;pp=ygUKY2tzIGNvdXJzZQ%3D%3D\nNow that I have a good homelab setup, I can also put things into practice much more conveniently. So, I\u0026rsquo;ll make a rule for myself not to move to new chapters of the course until I have done extensive practice for the module and I feel I have it \u0026ldquo;in the fingers\u0026rdquo;.\nDo practical exercises on RBAC and Network Policies every single day on Killercoda. These are my weakest points currently, and they will improve my security awareness on my own clusters but they will also be good ways to score points on the exam I\u0026rsquo;m also planning to do live streams of my studies. One thing that kept me from streaming is that I cannot stream when I am watching courses. But as this course is published on YouTube, I won\u0026rsquo;t get into copyright trouble. Secondly, I always thought that the chat would be distracting. But apparently non-interactive streaming is also a thing, and some people are content with just watching someone do their thing.\nWe\u0026rsquo;ll see, if anything it will be an interesting way to document my own learning on my YouTube channel. And it may also lead to surprises. Sometimes people in the chat are very helpful and they might even help me understand things better.\nLet\u0026rsquo;s go!\nLinks: 202403241903\n","permalink":"https://mischavandenburg.com/zet/starting-cks-studies/","summary":"It seems like I will have a couple of hours a day which I can allot to study in the coming month. This will be the perfect moment to start working on a goal I\u0026rsquo;ve had for a long time: getting my Certified Kubernetes Security Specialist certification.\nt\u0026rsquo;s not going to be easy. I\u0026rsquo;m quite intimidated by all of the new things I\u0026rsquo;ll have to learn. But that\u0026rsquo;s a good sign.","title":"Starting my CKS studies"},{"content":"Removing backgrounds from images is always an annoying task which I always need to google. And every time I end up on a different website where you can upload an image and then download a low-resolution version without the background, and you\u0026rsquo;ll have to pay for the high-resolution.\nTurns out MacOS preview can do this just fine. I keep finding these little gems in MacOS that completely remove the need for any external app or saas. Such as Apple Reminders.\nIt worked really well\nLinks: https://support.apple.com/guide/preview/extract-an-image-or-remove-a-background-prvw15636/mac#:~:text=Remove%20the%20background%20from%20an%20image%201%20In,you%20have%20the%20image%20you%20want.%20See%20More.\n202403220803\n[[Visual Thinking]] [[obsidian]]\n","permalink":"https://mischavandenburg.com/zet/remove-background-image-macos/","summary":"Removing backgrounds from images is always an annoying task which I always need to google. And every time I end up on a different website where you can upload an image and then download a low-resolution version without the background, and you\u0026rsquo;ll have to pay for the high-resolution.\nTurns out MacOS preview can do this just fine. I keep finding these little gems in MacOS that completely remove the need for any external app or saas.","title":"Remove Backgrounds from Images using MacOS Preview"},{"content":"In a recent analysis, I explored two notable solutions for synchronizing secrets from Azure Key Vaults to AKS (Azure Kubernetes Service) clusters: akv2k8s and the Azure Key Vault Provider for the Secret Store CSI Driver. Here, I present my findings and recommendations based on the functionality, maintenance requirements, and integration capabilities of these tools.\nAkv2k8s, maintained by Sparebanken, is an open-source tool designed for the synchronization of secrets. Being dependent on an external tool for Kubernetes secrets synchronization is an undesirable situation and poses several challenges. Notably, the latest version of akv2k8s has been problematic, especially concerning the deployment of Postgres databases on our AKS clusters using the EDB operator. Akv2k8s alters the SecurityContext of pods in a way that causes them to fail.\nFurthermore, upgrading akv2k8s to the latest version necessitates a transition to Workload Identity due to the deprecation of aad-pod-identity, a move that promises to be complex. Conversely, the Azure Key Vault Provider, directly offered by Microsoft, allows continued use of Managed Identities for authentication to Key Vaults, simplifying the integration process.\nhttps://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/identity-access-modes/\nAzure Key Vault Provider for Secret Store CSI Driver The Azure Key Vault Provider for Secret Store CSI Driver is the solution for syncing secrets offered by Microsoft Azure. It is based on the kubernetes-sigs/secrets-store-csi-driver.\nInstallation \u0026amp; Maintenance The addon is installed with an Azure CLI command or from the portal for existing clusters. The addon can also be enabled from code when deploying a new cluster.\nThe addon is automatically updated when an AKS upgrade to a new minor version is performed. Maintenance of the secret synching solution will not be required anymore and is handled by Azure Kubernetes Service.\nIf an emergency update is necessary, outside of the normal upgrade schedule, it can be updated with the Azure CLI as follows:\naz aks addon update --addon virtual-node --name MyManagedCluster --resource-group MyResourceGroup --subnet-name VirtualNodeSubnet\nAddons can also be patched by upgrading to the latest AKS node image.\nhttps://learn.microsoft.com/en-us/azure/aks/integrations https://learn.microsoft.com/en-gb/azure/aks/node-image-upgrade\nComparison Benefits Of Using The Official Solution The tool is built and maintained by Microsoft and an integral part of the AKS offering Authentication using a managed identity is supported out of the box Supports RBAC based authentication on Key Vaults Automatically updated when performing AKS upgrades One less component for us to maintain Can run alongside akv2k8s We can implement automatic secret rotation We are not dependent on an external project for our secrets management Does not require an initcontainer to inject secrets Cons Will take work to migrate all applications Migration Guide Steps that describe how to migrate from the use of akv2k8s to the CSI drivers.\nenable the addon using the Azure CLI az aks enable-addons --addons azure-keyvault-secrets-provider --name aks-vo-dcp-dev --resource-group rg-vo-dcp-aks-dev-weu-001\nThis does not restart any existing pods or nodes, thus it does not impact the running setup. akv2k8s and the AKV Provider can run simultaneously on the cluster and the workloads can be migrated one by one.\nEnabling the addon creates a Managed Identity. This Managed Identity needs to get the Key Vault Administrator role on the desired Key Vault to be able to retrieve Secrets.\nTo import the secrets, a SecretProviderClass resource is created:\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: azure-sync spec: provider: azure parameters: usePodIdentity: \u0026#34;false\u0026#34; useVMManagedIdentity: \u0026#34;true\u0026#34; # Set to true for using managed identity userAssignedIdentityID: ceb19a0a-941a-4f33-839d-aeace8ffe205 # Set the clientID of the user-assigned managed identity to use KeyvaultName: mischakv01 # Set to the name of your Key Vault cloudName: \u0026#34;\u0026#34; # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud objects: | array: - | objectName: ExampleSecret objectType: secret # object types: secret, key, or cert objectVersion: \u0026#34;\u0026#34; # [OPTIONAL] object versions, default to latest if empty - | objectName: env-secret objectType: secret # object types: secret, key, or cert tenantId: d62ada1b-ca42-4fe2-b9e7-ceb843af0ad # The tenant ID of the key vault secretObjects: # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects - data: - key: username # data field to populate objectName: env-secret # name of the mounted content to sync; this could be the object name or the object alias secretName: foosecret # name of the Kubernetes secret object type: Opaque # type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls) The secret is then mounted in to the container during deployment.\nSee this pod example:\nkind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline spec: containers: - name: busybox image: registry.k8s.io/e2e-test-images/busybox:1.29-1 command: - \u0026#34;/bin/sleep\u0026#34; - \u0026#34;10000\u0026#34; volumeMounts: - name: secrets-store01-inline mountPath: \u0026#34;/mnt/secrets-store\u0026#34; readOnly: true env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: foosecret key: username volumes: - name: secrets-store01-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \u0026#34;azure-sync\u0026#34; In this example the secret is accessible as a text file at /mnt/secrets-store inside the container. It is also made available as an environment variable SECRET_USERNAME.\nThe applications on our platform are already using environment variables when using akv2k8s.\nThe application helm charts will need to be updated with the new SecredProviderClass, but the applications themselves don\u0026rsquo;t need any changes. They will need to be redeployed with the updated configuration though.\nAnother note is that the secret must always be mounted even it is only used as an environment variable.\nConclusions Migrating to the Azure Key Vault Provider for Secret Store CSI Driver presents no disadvantages. There will be one less component to maintain, and it will always be compatible with the AKS offering.\nIt will require some work to convert the current workloads to use the CSI driver, but this is a one-time action which will not affect the applications while they are running. On the long term, this will pay itself back, because there is one less component to maintain.\nLinks: 202403111503\n","permalink":"https://mischavandenburg.com/zet/articles/akv2k8s-azure-key-vault-csi-analysis/","summary":"In a recent analysis, I explored two notable solutions for synchronizing secrets from Azure Key Vaults to AKS (Azure Kubernetes Service) clusters: akv2k8s and the Azure Key Vault Provider for the Secret Store CSI Driver. Here, I present my findings and recommendations based on the functionality, maintenance requirements, and integration capabilities of these tools.\nAkv2k8s, maintained by Sparebanken, is an open-source tool designed for the synchronization of secrets. Being dependent on an external tool for Kubernetes secrets synchronization is an undesirable situation and poses several challenges.","title":"Comparing akv2k8s with Azure Key Vault Provider for Secret Store CSI Driver"},{"content":"This document is the result of my research into the Gateway API. It aims to briefly describe the Gateway API for Kubernetes, a typical implementation of ingress traffic using NGINX in AKS and how this setup could benefit from implementing the Gateway API.\nIntroduction Gateway API is an official Kubernetes project focused on L4 and L7 routing in Kubernetes. This project represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs. From the outset, it has been designed to be generic, expressive, and role-oriented.\nThe overall resource model focuses on 3 separate personas and corresponding resources that they are expected to manage:\nUp until now ingress traffic to Kubernetes clusters was handled by the Ingress resource. Although the Ingress API will not be deprecated, the Gateway API is where all the development and innovation happens in the Kubernetes project.\nGateway API is the successor to the Ingress API.\n-Kubernetes documentation\nA Typical NGINX Ingress Solution Ingress traffic is typically implemented as follows.\nAn AKS cluster is provisioned in its own resource group and added to an existing virtual network. Usually each subscription has its own virtual network. We use the NGINX Ingress Controller which is deployed through ArgoCD The NGINX Ingress controller creates a Kubernetes LoadBalancer resource and exposes the cluster on port 80 and 443, as you can see below: Switched to context \u0026#34;aks-gwa-dev-admin\u0026#34;. mischa@mac-beast:~ (ins)$ kn Context \u0026#34;aks-gwa-dev-admin\u0026#34; modified. Active namespace is \u0026#34;nginx-ingress\u0026#34;. mischa@mac-beast:~ (ins)$ k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-ingress-ingress-nginx-controller LoadBalancer 192.168.10.17 52.137.26.47 80:30974/TCP,443:30474/TCP 595d nginx-ingress-ingress-nginx-controller-admission ClusterIP 192.168.51.96 \u0026lt;none\u0026gt; 443/TCP 595d nginx-ingress-ingress-nginx-controller-metrics ClusterIP 192.168.214.164 \u0026lt;none\u0026gt; 10254/TCP 417d nginx-ingress-ingress-nginx-vo-custom-default-backend ClusterIP 192.168.226.137 \u0026lt;none\u0026gt; 80/TCP 595d mischa@mac-beast:~ (ins)$ When a LoadBalancer resource is created in the cluster, AKS creates an Azure Load Balancer in the resource group of the AKS cluster. This Azure Load Balancer receives a public or internal IP address dependent on the Ingress Controller configuration. You can have a normal/external controller and an internal controller which doesn\u0026rsquo;t receive a public IP address. An Ingress resource is created in Kubernetes to configure the FQDN and TLS settings. In the Ingress resource, a label selector is configured to route the traffic to the correct backend service, which will route the traffic to the correct pods. A DNS record is created and linked to the IP of the Azure Load Balancer which was provisioned by the Ingress Controller When the FQDN is approached in the browser, the Azure DNS zone will forward the request to the Azure Load Balancer. The NGINX Ingress Controller will then route the traffic to the backend pods which were selected in the Ingress resource in Kubernetes. Disadvantages Of This Situation Ingress API has less functionality Azure Loadbalancer only works on Layer 4, no layer 7 routing NGINX Ingress Controller maintenance and configuration Benefits of Gateway API natively supports traffic weighting, no extra annotations needed on ingress controller example: sending 50% of the traffic to an application instance with a feature flag enabled natively supports header-based matching traffic can be routed to other resources outside of the cluster or other clusters extensible: allows for custom resources to be linked at various layers of the API, allowing customization simplifies networking configuration aids in transition towards service mesh set up to support role based division of labour. The platform team sets up the infrastructure and Gateway itself, and the developer teams can add routing configuration that suits their needs enables the developer teams to gain full control over their network configuration through GitOps Gateway API simplifies networking configuration by standardizing the way we define network rules. Thus, Gateway API aids in the transition from ingress towards service mesh. When the Gateway API is implemented, it will be much easier to adopt a service mesh networking architecture, or to migrate between service meshes.\nAnother advantage is that it can enable teams to implement specific header-based routing and traffic weighting between backend pools\nA more specific list of Gateway API features:\nHeader rewrite HTTPS traffic management: SSL termination End to End SSL Ingress and Gateway API support Layer 7 HTTP/HTTPS request forwarding based on prefix/exact match on: Hostname Path Header Query string Methods Ports (80/443) Mutual Authentication (mTLS) to backend target Traffic Splitting / weighted round robin TLS Policies URL rewrite Implementations The Gateway API can be implemented through various controllers. Some notable implementations:\nNGINX Gateway Fabric (GA)\nTraefik (alpha)\nCilium (beta)\nThese will all require an extra controller or even a CNI to be installed.\nHowever, with the release of Application Gateway for Containers, Azure also offers a native implementation of the Gateway API in Azure Kubernetes Service.\nApplication Gateway for Containers The implementation of Gateway API through Azure\u0026rsquo;s solution is rather straightforward since we are already running on the Azure platform.\nThe AG for Containers is a cluster-specific resource which deploys two resources in Azure: a frontend and an association. The frontend is an entry point where traffic is received in the form of an FQDN. The association is the entry to a virtual network. An association is a 1:1 mapping of an association resource to an Azure Subnet that has been delegated.\nEach Application Gateway for Containers frontend provides a generated Fully Qualified Domain Name managed by Azure. The FQDN may be used as-is or customers may opt to mask the FQDN with a CNAME record.\nBefore a client sends a request to Application Gateway for Containers, the client resolves a CNAME that points to the frontend\u0026rsquo;s FQDN; or the client may directly resolve the FQDN provided by Application Gateway for Containers by using a DNS server.\nWhen the client initiates the request, the DNS name specified is passed as a host header to Application Gateway for Containers on the defined frontend.\nImage from Microsoft Documentation. I don\u0026rsquo;t own this image.\nGateway and Route Setup A set of routing rules evaluates how the request for that hostname should be initiated to a defined backend target.\nThese resources are provided from within the cluster by creating a gateway with a Kubernetes manifest:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1beta1 kind: Gateway metadata: name: gateway-01 namespace: test-infra annotations: alb.networking.azure.io/alb-namespace: alb-test-infra alb.networking.azure.io/alb-name: alb-test spec: gatewayClassName: azure-alb-external listeners: - name: http port: 80 protocol: HTTP allowedRoutes: namespaces: from: Same EOF When the gateway is created, the AG for Containers will create the required frontend and association.\nThen we can create a route that connects the gateway to the backend pods of our application. This example shows how easy it is to weight traffic 50/50 to two backends:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: traffic-split-route namespace: test-infra spec: parentRefs: - name: gateway-01 rules: - backendRefs: - name: backend-v1 port: 8080 weight: 50 - name: backend-v2 port: 8080 weight: 50 EOF Image from Microsoft Documentation. I don\u0026rsquo;t own this image.\nAzure Load Balancer Controller In order to enable these resources on the cluster, the Azure Load Balancer controller needs to be installed via Helm. When it is connected to a Managed Identity it is able to provision the required resources in Azure.\nThere is an alternative deployment mode where the Azure resources (frontends and associations) can be provisioned through Terraform or Bicep and referred to from within the cluster. However, I think that it is best to deploy these resources from within the cluster because it enables the application teams to get full control over their traffic based on the GitOps workflow that they are already used to working with.\nAdvantages of the Azure Application Gateway for Containers Natively supported for AKS on the Azure platform Microsoft support and SLA Increased performance, offering near real-time updates to add or move pods, routes, and probes Does not need any configuration on the controller Autoscaling Availability zone resiliency Azure resources can be provisioned from within the cluster The FQDN can be linked to the DNS zone using External DNS, Azure Front Door or Azure DNS zones Will likely be integrated into the AKS offering as an extension, which means we won\u0026rsquo;t have to maintain the controller Easier to maintain because configuration is tailored to AKS out of the box Disadvantages of the Azure Application Gateway for Containers I don\u0026rsquo;t see any disadvantages over using the AG for Containers in terms of technical implementation. In fact, implementing the AG for Containers will be the most straightforward and easiest option. However, we must consider the price. As the AG for containers does not currently have WAF functionality, the premium NGINX Ingress Controller offering could be considered in the price.\nUsing NGINX or Traefik would still require an Azure LoadBalancer resource to provision an internal and/or external IP. When making a pricing comparison, the following things should be considered when using a different solution:\ncost of Azure Load Balancer (and the reduncany of the load balancer) cost of man hours needed to learn a new controller such as Traefik or NGINX time required to upgrade the controller and reading up on release notes and changelogs potential licence costs when adopting enterprise usage Lifecycle management No matter which implementation we choose, the lifecycle management of the controller will at this point still be with us. When using the Azure Application Gateway for containers, it is installed with helm and updated through helm. We can use our ArgoCD setup for this. The same applies for different solutions such as traefik or nginx.\nHowever, I expect that the ALB controller will be integrated within AKS in the future and that would remove the need for lifecycle management for the ALB controller.\nImpact Analysis Since the Azure Application Gateway for Containers or the NGINX Gateway Fabric are deployed as separate controllers, these could co-exist with the current ingress setup. Normal Ingress resources should not be affected.\nWhen adopting the Gateway API, all the existing Ingress resources will need to be converted to Gateway API resources in our code base. We will need to decide if this needs to be done by the platform team or the developer teams.\nOne solution could be that we configure the Gateway and let the teams know that this is now a posiblity and that this should be used for new application deployments. The teams can then configure the Routes themselves after some training by the platform team.\nConclusion The Gateway API is the next iteration of the Ingress resource. It allows more functionality and aids in defining the roles between the platform and development teams. If one is in the process to revise the method of handling ingress traffic to the clusters, it would make sense to adopt the Gateway API in this process.\nAlthough there are no plans for a service mesh at the time of writing, having the Gateway API implemented will make the transition to a service mesh much easier. In the meantime, the Gateway API already provides interesting functionality such as traffic weighting, which would be one of the reasons to implement a service mesh.\nSince the Gateway API itself is a Kubernetes resource, it does not have feature parity and would work on any Kubernetes cluster that has a controller which allows the Gateway resource to be defined. The choice we need to make is which controller we will use for the implementation of the Gateway API on our AKS clusters.\nGiven the ease of implementation, native Azure support and simplified maintenance, the Application Gateway for Containers is the most logical choice.\nLinks https://gateway-api.sigs.k8s.io/\nhttps://kubernetes.io/docs/concepts/services-networking/gateway/\nhttps://learn.microsoft.com/en-us/azure/application-gateway/for-containers/overview\nhttps://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/gateway-api.md\nLinks: 202403101103\n","permalink":"https://mischavandenburg.com/zet/articles/gateway-api-application-gateway-for-containers/","summary":"This document is the result of my research into the Gateway API. It aims to briefly describe the Gateway API for Kubernetes, a typical implementation of ingress traffic using NGINX in AKS and how this setup could benefit from implementing the Gateway API.\nIntroduction Gateway API is an official Kubernetes project focused on L4 and L7 routing in Kubernetes. This project represents the next generation of Kubernetes Ingress, Load Balancing, and Service Mesh APIs.","title":"Kubernetes Gateway API \u0026 Azure Application Gateway for Containers"},{"content":"Super useful\nhttps://dannorth.net/hugo-redirects/\nLinks: 202403071003\n","permalink":"https://mischavandenburg.com/zet/hugo-redirects/","summary":"Super useful\nhttps://dannorth.net/hugo-redirects/\nLinks: 202403071003","title":"Using Hugo for Redirecting URLS"},{"content":"I\u0026rsquo;m an avid reader, and this list is far from complete. But I wanted to start writing down the ones I\u0026rsquo;m reading now, and the ones I\u0026rsquo;ve read in the past. I\u0026rsquo;m mostly writing down titles but I\u0026rsquo;ll include the author when I remember the name.\n2024 How To Take Smart Notes\nGetting Things Done\n2023 The Lords of the North\nThe Pale Horseman\nThe Last Kingdom\nBorn to Run 2\nBorn to Run\nFour Thousand Weeks\nThe Plantpower Way - Rich Roll\nEat and Run - Scott Jurek\nFinding Ultra - Rich Roll\nA Simple Path to Wealth\nThe Richest Man in Babylon\nCan\u0026rsquo;t Hurt Me\nHow to Win Friends and Influence People\nThe Subtle Art of Not Giving A F*ck\nNever Split the Difference\nIkigai\nBuilding A Second Brain\nOn Writing Well\nFat Loss Forever - Layne Norton\nThe Unicorn Project\nThe Clean Coder\nGet A Financial Life\nYour Money or Your Life\nEarlier Moby Dick\nThe Lord of the Rings\nThe Wim Hof Method\nThe Way of Qigong\nThe Oxygen Advantage\nRemember It\nPride and Prejudice\nWhite Noise\nThe Road\nThe Reluctant Fundamentalist\nThe Last Kingdom series\nWar and Peace\nAnna Karenina\nDictator\nImperium\nLustrum\n4 Hour Workweek\nUncommon Wisdom\nStillness Flowing\nSeeking Buddho\nWhat Doesn\u0026rsquo;t Kill Us\nThe Art Of Happiness\nInto the Wild\nAlone in the Wilderness\nCall of the Wild\nWhite Fang\nName of the Wind\nWise Man\u0026rsquo;s Fear\nSlow Regard of Silent Things\nBigger Leaner Stronger series\nAs I Lay Dying\nThe Canterbury Tales\nBeowulf\nSwedish Carving Techniques\nDrawing Nature for the Absolute Beginner\nOld Souls\nThus Spake Zarathustra\nOpening the Door of Your Heart\nThe Kreutzer Sonata\nThe Hunger Fix\nFire First Law Trilogy\nThe Starch Solution\nUltimate Guide to Chinese Tea\nThe Way to Love\nMoonwalking with Einstein\nThe Prose Edda\nHow to Draw what you See\nUntethered Soul\nHow to Practice\nIntuitive Awareness - Ajahn Sumedho\nAwareness - Tony DeMello\nWay of Power - Master Lam\nThe Way of Energy - Master Lam\nThe Noble Eightfold Path - Bikkhu Bodhi\nWisdom Develops Samadhi\nThe Collected Teachings of Ajahn Chah 1, 2, 3\nForest Desanas\nForest Dhamma\nMae Chee Kaew\nSamana\nThe Way it Is\nArt of Memory\nXu Yun\nBartleby: The Scrivener\nWalden\nHow to Take Smart Notes\nSteal Like an Artist\nCan\u0026rsquo;t Hurt Me\nHitchhiker\u0026rsquo;s Guide to the Galaxy\nArthurian Literature\nMallory\nDon Quichote\nStarting Strength\nPractical Programming\nThe Hidden Life of Trees\nThe Bullet Journal Method\nThinking Fast and Slow\nAngela\u0026rsquo;s Ashes\nGreat Expectations - Dickens\nHard Times - Dickens\nOthello\nKing Lear\nBroodje Gezond\nParadise - Toni Morrison\nWatchmen\nHeart of Darkness\nChristianity - Chidester\nIn het Huis van de Islam\nA Short History of Nearly Everything\nThe Surprising Purpose of Anger\nNonviolent Communication\nThe Hobbit\nMythology\nThe Iliad\nIn the Buddha\u0026rsquo;s Words - Bhikkhu Bodhi\nThe Word of the Buddha - Nyanatiloka\nThe Dhammapada\nDependent Origination\nXu Yun: Dharma Words\nThe Phoenix Project\nWhy We Sleep\nHow to remember your Dreams\nSolomon\u0026rsquo;s Memory Palace\nThe Complete Guide to Fasting\nDirect Realization - Ajahn Sumedho\nPeace is a Simple Step - Ajahn Sumedho\nLinks: 202307062007\n","permalink":"https://mischavandenburg.com/zet/books-i-have-read/","summary":"I\u0026rsquo;m an avid reader, and this list is far from complete. But I wanted to start writing down the ones I\u0026rsquo;m reading now, and the ones I\u0026rsquo;ve read in the past. I\u0026rsquo;m mostly writing down titles but I\u0026rsquo;ll include the author when I remember the name.\n2024 How To Take Smart Notes\nGetting Things Done\n2023 The Lords of the North\nThe Pale Horseman\nThe Last Kingdom\nBorn to Run 2","title":"A List Of All The Books I Have Read In My Life"},{"content":" I\u0026rsquo;ve just released a comprehensive video delving deep into my Neovim setup and command-line centric workflow. This video is a response to the curiosity of my YouTube subscribers. It\u0026rsquo;s an all-inclusive guide from A to Z, detailing the rationale behind my configuration choices. I\u0026rsquo;ve spent years perfecting my workflow and this video has been several months in the making, so I\u0026rsquo;m excited that I can finally share it with you.\nMy workflow is completely based on the command line. I therefore have very little programs or tools running on the graphical side of the OS, whether it be MacOS or Linux. All I need is the terminal and a web browser, but a window manager is a big bonus to my productivity.\nMy main philosophy is to have a minimalistic workflow which uses very little resources. It should be a distraction-free, keyboard centered and efficient environment to do my best work.\nLinks: 202401211901\nhttps://youtu.be/iagjeLuxnMs\n","permalink":"https://mischavandenburg.com/zet/video-tmux-neovim-macos-workflow/","summary":"I\u0026rsquo;ve just released a comprehensive video delving deep into my Neovim setup and command-line centric workflow. This video is a response to the curiosity of my YouTube subscribers. It\u0026rsquo;s an all-inclusive guide from A to Z, detailing the rationale behind my configuration choices. I\u0026rsquo;ve spent years perfecting my workflow and this video has been several months in the making, so I\u0026rsquo;m excited that I can finally share it with you.","title":"Video: My Entire Neovim + Tmux Workflow As A DevOps Engineer On MacOS"},{"content":"I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here\u0026rsquo;s what you need to know:\nPVCs: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed. PVs: Provisioned by administrators or dynamically through Storage Classes, PVs provide the actual storage resources. Their lifecycle is not tied to any specific Pod. Understanding Reclaim Policies The Reclaim Policy on a PV dictates its fate after a PVC is released. There are three policies to be aware of:\nDelete: The PV and its underlying storage are deleted when the PVC is deleted. Importantly, if the PVC is not deleted, the PV remains. Retain: When a PVC is deleted, the PV is not. Instead, it transitions to a Released state, but the data remains intact until manually deleted or repurposed. Recycle (Deprecated): The volume is scrubbed clean and made available for a new claim. Note: this policy is no longer recommended. Practical Implications PVCs won\u0026rsquo;t cause PV deletion unless they themselves are deleted. This is crucial to understand, especially with a Delete Reclaim Policy, as it leads to the deletion of both the PV and the underlying storage, resulting in data loss. Carefully manage PVCs, especially in production environments. Ensure you have backups and fully understand the implications of deleting PVCs, especially when using a Delete Reclaim Policy. Links: 202401180901\n[[Kubernetes]]\n[[storage]]\n","permalink":"https://mischavandenburg.com/zet/kubernetes-storage-pvc-pv/","summary":"I always thought that Persistent Volume Claims where deleted when you delete the pod which they are associated with. I was wrong. The lifecycle of PVCs is independent of Pods, and their behavior is largely governed by the Reclaim Policy set on the PVs. Here\u0026rsquo;s what you need to know:\nPVCs: These are requests for storage, akin to how Pods request resources like CPU and memory. They exist independently and can be bound to Pods when needed.","title":"PersistentVolumeClaims Lifecycle in Kubernetes"},{"content":" In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.\nNotes:\nkustomization resources live in the cluster\nsource resources also live in the cluster\nTo suspend updates for a kustomization, run the command flux suspend kustomization .\nTo resume updates run the command flux resume kustomization .\nuse flux reconcile source git podinfo to force a sync, nn waiting\nlove the fact that helm releases are still accessible and visible on the cluster\nflux is just managing helm for you based on code\nlove the fact that weave dashboard is also part of the cluster manifests\nlearned about Kubernetes controllers\nan operator is a controller\nbasically everything that enables CRD\u0026rsquo;s to run is a controller\n\u0026ldquo;A kubernetes controller is a control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state\u0026rdquo;\nSources A Source defines the origin of a repository containing the desired state of the system and the requirements to obtain it (e.g. credentials, version selectors). For example, the latest 1.x tag available from a Git repository over SSH.\nAll sources are specified as Custom Resources in a Kubernetes cluster, examples of sources are GitRepository, OCIRepository, HelmRepository and Bucket resources.\nhttps://fluxcd.io/flux/concepts/#sources\nWeave UI Follow this guide\nhttps://docs.gitops.weave.works/docs/installation/weave-gitops/\nsources are located in the clusters directory in a monorepo structure Repo Decided to fully commit to Flux and their practices.\nSet up the repo according to this guide:\nhttps://fluxcd.io/flux/guides/repository-structure/\nAnd following this example:\nhttps://github.com/fluxcd/flux2-kustomize-helm-example\nLinks: 202312261612\nhttps://kubernetes.io/docs/concepts/architecture/controller/\nhttps://youtu.be/BtuqzsyztBc\n","permalink":"https://mischavandenburg.com/zet/video-homelab-learning-flux/","summary":"In this video I set up Flux running in a local cluster on my MacBook by following the getting started guide. Then I learn about how to structure the repo according to Flux methodology. I implement this structure in my homelab repo and deploy flux to my homelab cluster. Then I manage to configure Grafana and the Weave UI to be accessbible via ingress using a custom fake domain.","title":"Video: Learning Flux and Installing To Homelab"},{"content":"Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.\nBut now I\u0026rsquo;m exposing some apps to the internet in my homelab and I\u0026rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.\nI installed Cilium as the Container Networking Interface in my homelab cluster. This also allows me to use the Hubble tool. This has proven to be of immmense value to see the traffic that is actually flowing around in the namespace you\u0026rsquo;re working on.\nEven with a small application of a few microservices, as the image shows, the traffic that flows between them and which needs to come from other namespaces can get pretty complex. But in the Hubble UI you get a clear view of what\u0026rsquo;s happening, and you can see exactly when packets are being dropped.\nI also found the Network Policy Editor extremely valuable. It is such a relief to be able to compose the policy from a graphical interface, even though I\u0026rsquo;m a CLI guy and want to do everything in and from code, this has been a gamechanger.\nNow I have created the following policy for my linkding app, which allows the EDB operator to do its thing on my databases, Prometheus to monitor my application, but a hacker could not do anything outside of the namespace if they managed to break out.\napiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: linkding-app namespace: linkding spec: endpointSelector: matchLabels: policy-type: \u0026#34;app\u0026#34; ingress: - fromEndpoints: - {} - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: monitoring - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: postgresql-operator-system egress: - toEndpoints: - matchLabels: io.kubernetes.pod.namespace: kube-system k8s-app: kube-dns toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP rules: dns: - matchPattern: \u0026#34;*\u0026#34; - toEndpoints: - {} - toEntities: - world toPorts: - ports: - port: \u0026#34;443\u0026#34; - ports: - port: \u0026#34;80\u0026#34; - ports: - port: \u0026#34;7844\u0026#34; --- apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: linkding-database namespace: linkding spec: endpointSelector: matchLabels: policy-type: \u0026#34;database\u0026#34; ingress: - fromEndpoints: - {} - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: monitoring - fromEndpoints: - matchLabels: io.kubernetes.pod.namespace: postgresql-operator-system egress: - toEndpoints: - {} - toEndpoints: - matchLabels: io.kubernetes.pod.namespace: kube-system k8s-app: kube-dns toPorts: - ports: - port: \u0026#34;53\u0026#34; protocol: UDP rules: dns: - matchPattern: \u0026#34;*\u0026#34; - toEntities: - kube-apiserver toPorts: - ports: - port: \u0026#34;6443\u0026#34; - toEntities: - world toPorts: - ports: - port: \u0026#34;443\u0026#34; --- apiVersion: cilium.io/v2 kind: CiliumNetworkPolicy metadata: name: deny-all namespace: linkding spec: endpointSelector: {} ingress: - {} egress: - {} Links: 202401142001\n","permalink":"https://mischavandenburg.com/zet/cilium-network-policies/","summary":"Today I learned about Cilium network policies. These are much easier to implement than normal network policies because there are some tools available when creating the cilium policies. Network policies were probably my weakest Kubernetes skill and I tended to avoid them.\nBut now I\u0026rsquo;m exposing some apps to the internet in my homelab and I\u0026rsquo;m forced to think about security and what would happen if a hacker managed to get root privileges in a container even though I implemented strict security contests and enabled privilege escalation.","title":"Learned Cilium Network Policies"},{"content":"My ISP router only allowed me to assign 200 IP addresses and it didn\u0026rsquo;t have any possibility to create VLANS, so it was time for an upgrade to my setup.\nNow that I\u0026rsquo;m hosting several clusters in my homelab and exposing things to the internet I can justify to spend the money on this hardware and the complexity of dividing up my home network into VLANS.\nI bought a Unifi Express gateway and the Unifi Lite 8 PoE managed switch.\nThe gateway will function as the main portal to the internet and the WiFi access point. The switch will be located in my pantry where all the devices for my homelab are currently.\nNaming Strategy I\u0026rsquo;m also going to have some fun with the naming strategy. I love Nordic mythology so I picked that as a theme.\nNetwork Name: Asgard: A reference to the home of the gods in Norse mythology, symbolizing a powerful and central network. VLAN Naming Strategy: Production Kubernetes Cluster: Valhalla - Named after the majestic hall where heroes reside, representing the strength and importance of the production environment. Staging Kubernetes Cluster: Midgard - Midgard, the world of humans in Norse mythology, can symbolize a testing ground that\u0026rsquo;s closer to the \u0026lsquo;real world\u0026rsquo; scenarios. Main VLAN (Work \u0026amp; Personal Devices): Yggdrasil - Named after the great tree that connects all the worlds, symbolizing the connectivity and central role of this VLAN in the network. IoT VLAN: Niflheim - The realm of fog and mist, a metaphor for the mysterious and ubiquitous nature of IoT devices. Guest Network VLAN: Jotunheim - Inspired by the realm of giants in Norse mythology, this guest network embodies the vastness and mystery of Jotunheim. It\u0026rsquo;s a separate, secure space for visitors, representing the outer layers of my realm. Why UniFi? a colleague of mine has it and is very satisfied with it saw a few videos of Techno Tim and was very impressed by his setup looks like a very easy way to manage the network provides insight into traffic flow beautiful dashboard and UI I like ecosystems with good documentation allows expanding within the same ecosystem Links: 202401130601\n","permalink":"https://mischavandenburg.com/zet/homelab-network-design/","summary":"My ISP router only allowed me to assign 200 IP addresses and it didn\u0026rsquo;t have any possibility to create VLANS, so it was time for an upgrade to my setup.\nNow that I\u0026rsquo;m hosting several clusters in my homelab and exposing things to the internet I can justify to spend the money on this hardware and the complexity of dividing up my home network into VLANS.\nI bought a Unifi Express gateway and the Unifi Lite 8 PoE managed switch.","title":"Designed The Network For My Homelab"},{"content":" App Gateway It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.\nResources Two types of resources. Azure resources and k8s resources.\nThe App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.\nFrontend Azure Frontend is a public IP and fqdn. Both are managed resources, you don\u0026rsquo;t see them in your subscription.\nYou can have multiple frontends in one gateway to save money. Teams could share the app gateway but use different frontend IP addresses or FQDNs.\nControl plane: Azure App Gateway for Containers\nData plane: association with kubernetes pods.\nThe association is made to the subnet in the Azure VNet.\nEach association is in one subnet, and the subnet should at least have /24 or 256 addresses.\nKubernetes ALB controller consists of two pods. Controller pod and a bootstrap pod.\nController communicates to the Azure gateway resource. It talks directly to the App Gateway, not to the Azure Resource Manager, which is why you\u0026rsquo;re able to have sub-second updates.\nThe bootstrap contains the CRDs etc, it does not do very much.\nCreating resources There is a managed option that will talk to ARM and create the resources for you. Or you can choose to deploy them yourself. It depends whether you want to control everything from Kubernetes. If you have all your Azure resources in Infrastructure as Code it probably makes more sense to create the App Gateway resources from there instead of from Kubernetes.\nAssociation Links frontend with a subnet in a VNet. This will typically be the same vnet that the AKS cluster is in. Could technically be a peered VNet but probably uncommon.\nThis is an Azure resource. It lives in the VNet and handles TLS and makes the connections to and from the pods and frontend IP. This is the data path.\nThe traffic is not routed within the cluster, but in Azure by the association.\nClient talks to the front end, passes to the association, the association is doing the work, and then routing it to the cluster.\nSupport Azure CNI. Does not support kubenet or Azure CNI overlay yet, but it will support in the future.\nKubernetes Resources Application Load Balancer: this name was chosen because k8s people don\u0026rsquo;t know the concept of app gateway.\nBenefits ssl offloading traffic splitting clearer separation between platform team and app team platform team manages the gateway itself automatic default health probes Benefits of Azure not suing cluster resources for load balancing. this happens in azure native azure metrics available misc AGIC does not support ingress Links: 202309301009\n","permalink":"https://mischavandenburg.com/zet/application-gateway-for-containers/","summary":"App Gateway It has App Gateway in the name, but it is an entirely new solution. The App Gateway is the only thing it has in common with Azure Application Gateway.\nResources Two types of resources. Azure resources and k8s resources.\nThe App Gateway for Container is an azure resource which listens to changes in k8s resources through the ALB controller. AGWFC is the control plane.\nFrontend Azure Frontend is a public IP and fqdn.","title":"Video notes - Application Gateway for Containers"},{"content":" In this video I installed Prometheus and Grafana using helm and studied k3s networking.\nMy goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.\ninstalled prometheus and grafana with kube-prometheus-stack helm chart reflected on why I use k3s gained understanding of k3s loadbalancing solution configured /etc/hosts file to resolve to fake domain configured k3s ingress to use fake local domain struggled with ingress but figured it out in the end successfully made grafana UI available on fake local domain grafana.homelab.nl Links: 202312261012\nhttps://youtu.be/JjIB65CVXAo\n","permalink":"https://mischavandenburg.com/zet/video-homelab-3-k3s-networking/","summary":"In this video I installed Prometheus and Grafana using helm and studied k3s networking.\nMy goal was to make Grafana approachable via ingress using a fake domain and after a bit of tinkering it worked.\ninstalled prometheus and grafana with kube-prometheus-stack helm chart reflected on why I use k3s gained understanding of k3s loadbalancing solution configured /etc/hosts file to resolve to fake domain configured k3s ingress to use fake local domain struggled with ingress but figured it out in the end successfully made grafana UI available on fake local domain grafana.","title":"Video: Homelab E2 - Setting Up Monitoring + Studying k3s Networking  \u0026 Configuring Ingress"},{"content":"Last week I created my own Go program to send toots to Mastodon but now I stumbled across a full fledged CLI with a TUI\nhttps://toot.bezdomni.net/\nLinks: 202312311312\n","permalink":"https://mischavandenburg.com/zet/toot-mastodon-cli/","summary":"Last week I created my own Go program to send toots to Mastodon but now I stumbled across a full fledged CLI with a TUI\nhttps://toot.bezdomni.net/\nLinks: 202312311312","title":"Found a Mastodon CLI"},{"content":" This is the first video of my homelab series.\nI set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.\nLinks: 202312301912\nhttps://youtu.be/X40gNPZ2xP4\n[[homelab]]\n","permalink":"https://mischavandenburg.com/zet/homelab-episode-1/","summary":"This is the first video of my homelab series.\nI set up the repo for my project and do the initial installation of k3s on an old laptop I had lying around. I had a k8s cluster installed on there with kubeadm which I needed to clean up first.\nLinks: 202312301912\nhttps://youtu.be/X40gNPZ2xP4\n[[homelab]]","title":"Video: Homelab Episode 1"},{"content":"This is a super useful tool to list, download and upload Grafana dashboards as json.\nCurrently using this to occasionally take extra-extra backups of my Grafana creations\nhttps://software.es.net/gdg/\nLinks: 202312301612\n","permalink":"https://mischavandenburg.com/zet/grafana-gdg-backup-cli/","summary":"This is a super useful tool to list, download and upload Grafana dashboards as json.\nCurrently using this to occasionally take extra-extra backups of my Grafana creations\nhttps://software.es.net/gdg/\nLinks: 202312301612","title":"Discovered a simple CLI to backup and upload Grafana dashboards"},{"content":"In this blog post, I want to share with you how I set up secrets management for my home lab. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.\nSince Flux is the integrated solution for GitOps in Azure Kubernetes Service I also adopted it for my home lab. Flux supports and recommends encrypting secrets in git using SOPS, which is a tool that uses asymmetric encryption to protect secrets. This means that you can store encrypted secrets in your git repository and only decrypt them when they are applied to the cluster. This sounds like a convenient and secure way to manage secrets, right?\nWell, not quite. After doing some research, I decided to use a different approach: storing my secrets in Azure Key Vault and syncing them to my cluster with the Azure Key Vault Provider. This is the recommended best practice by Microsoft, as explained in this article:\nTLDR: Referencing secrets in an external key vault is the recommended approach. It is easier to orchestrate secret rotation and more scalable with multiple clusters and/or teams.\nhttps://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/\nSOPS has some drawbacks that made me choose the Azure Key Vault route for my home lab. First of all, SOPS requires you to encrypt each secret value manually in the command line, which can be tedious and error-prone. Secondly, SOPS relies on a single encryption key that is stored in your local machine or in a cloud KMS. If you lose this key, you will lose access to all your encrypted secrets in your Git repo. Thirdly, SOPS does not provide a backup or recovery mechanism for your secrets, nor does it offer fine-grained access control or auditing capabilities.\nHere are some of the reasons why I chose this approach over SOPS:\nSecrets are stored in an external source, which means that they are not exposed in git, even if encrypted. Azure Key Vault provides backup and recovery features, such as soft delete and restore, which can help in case of accidental deletion or corruption of secrets. Azure Key Vault allows fine-grained authentication and authorization to the vaults, using Entra ID identities and policies. This means that I can control who can access or modify my secrets, and audit their actions. I don\u0026rsquo;t need to encrypt or decrypt values in the command line, which can be error-prone or leak information. I can use the Azure CLI or the Azure Portal to manage my secrets in the vault. If I lose my encryption key, all my encrypted secrets in git are useless with SOPS. With Azure Key Vault, I can still access my secrets using my Entra ID or recover them from backup. This approach mimics a solution that I would use for an enterprise production environment However, this does not mean that I will not use SOPS at all. Sometimes Helm charts or other configurations need values to be hardcoded due to incorrect implementation. This is where SOPS truly shines. You can safely check in your secret values to source code. Additionally, the SOPS keys can also be stored in Azure Key Vault. Tools should be used where they are most useful, so I\u0026rsquo;m not saying I won\u0026rsquo;t use SOPS at all, but I\u0026rsquo;m choosing Azure Key Vaults and the accompanying CSI Store provider for my main solution.\nAzure Key Vault Provider To sync my secrets from Azure Key Vault to my AKS cluster, I used the Azure Key Vault Provider for Secrets Store CSI Driver. This is an open source project that enables you to mount secrets from external sources (such as Azure Key Vault) as volumes in your pods using the Container Storage Interface (CSI) specification.\nI followed the provider documentation to install the provider in my local cluster using a Flux HelmRelease.\nWith the provider installed I can use the following configuration to sync a secret named grafana-admin-password from my Key Vault and make it available as a volume and environment variable in an example busybox pod:\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: azure-kv-secrets namespace: monitoring spec: provider: azure parameters: keyvaultName: \u0026#34;mischa-homelab-k8s\u0026#34; # the name of the KeyVault objects: | array: - | objectName: grafana-admin-password objectType: secret # object types: secret, key, or cert tenantId: 6ddecc48-41b1-48de-bfde-2efd29fae9c7 secretObjects: # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects - data: - key: password # data field to populate objectName: grafana-admin-password # name of the mounted content to sync; this could be the object name or the object alias secretName: grafana-custom-secret # name of the Kubernetes secret object type: Opaque # type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls) --- kind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-inline namespace: monitoring spec: containers: - name: busybox image: registry.k8s.io/e2e-test-images/busybox:1.29-1 command: - \u0026#34;/bin/sleep\u0026#34; - \u0026#34;10000\u0026#34; volumeMounts: - name: secrets-store01-inline mountPath: \u0026#34;/mnt/secrets-store\u0026#34; readOnly: true env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: grafana-custom-secret key: password volumes: - name: secrets-store01-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: azure-kv-secrets nodePublishSecretRef: # Only required when using service principal mode name: secrets-store-creds # Only required when using service principal mode. The name of the Kubernetes secret that contains the service principal credentials to access keyvault. I hope you found this blog post useful and informative. If you have any questions or feedback, please feel free to leave a comment below or contact me on Twitter @mischa_vdburg\nLinks: 202312290912\nhttps://microsoft.github.io/code-with-engineering-playbook/continuous-delivery/gitops/secret-management/secret-management-gitops/\nhttps://github.com/mischavandenburg/homelab/\nAzure Key Vault documentation Flux documentation Azure Key Vault Provider for Secrets Store CSI Driver documentation https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/\n","permalink":"https://mischavandenburg.com/zet/handling-secrets-kubernetes/","summary":"In this blog post, I want to share with you how I set up secrets management for my home lab. I use my home lab to explore new technologies, but I also try to keep it in line with the practices I would use when setting up environments for clients. I focus on Microsoft Azure and the ecosystem they provide for cloud native applications. Secrets management is an important aspect of any cloud-native application, as it allows you to securely store and access sensitive information such as passwords, tokens and certificates.","title":"Homelab Secret Management With GitOps and Azure Key Vault"},{"content":" In this video I\u0026rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:\ncreating an application in Strava installing the Strava plugin in Grafana adding the Strava data source importing dashboards exploring data and creating custom dashboard Links: 202312250812\nhttps://youtu.be/CgP9hs9UDzA\n","permalink":"https://mischavandenburg.com/zet/video-strava-grafana/","summary":"In this video I\u0026rsquo;ll show you how to view your Strava data in Grafana by doing the following steps:\ncreating an application in Strava installing the Strava plugin in Grafana adding the Strava data source importing dashboards exploring data and creating custom dashboard Links: 202312250812\nhttps://youtu.be/CgP9hs9UDzA","title":"Video: Viewing Strava Data In Grafana Dashboards"},{"content":" In this video I\u0026rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.\nThere are many options available out there but this is the easiest one I found to get up and running quickly.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace\nhelm show values prometheus-community/kube-prometheus-stack \u0026gt; prometheus-default-values.yaml\nOpening the Grafana UI k port-forward svc/prometheus-stack-grafana 3000:80\nThen you can open it by entering localhost:3000 in your browser.\nThe default credentials are admin:prom-operator\nLinks: 202312250812\nhttps://youtu.be/3AINqaBwOYs?si=maN1rfbg4pzBc3xI\nhttps://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#configuration\n","permalink":"https://mischavandenburg.com/zet/video-install-prometheus-grafana-homelab/","summary":"In this video I\u0026rsquo;ll be installing Prometheus and Grafana in a Kubernetes cluster running in Rancher Desktop on my MacBook.\nThere are many options available out there but this is the easiest one I found to get up and running quickly.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace\nhelm show values prometheus-community/kube-prometheus-stack \u0026gt; prometheus-default-values.yaml\nOpening the Grafana UI k port-forward svc/prometheus-stack-grafana 3000:80","title":"Video: How To Install Prometheus \u0026 Grafana In Your Homelab"},{"content":"This morning I had some fun writing a Go program which takes the latest blog post from my RSS feed and posts it to Mastodon. It is based on my twitter cli I wrote a while ago.\nIt was fun to write some Go again, it has been far too long. I\u0026rsquo;ve been writing mostly Terraform and yaml recently and working a lot with infrastructure. I haven\u0026rsquo;t been doing any projects at home that required programming. But recently I started up my personal blog and completely re-architected my social media setup.\nI\u0026rsquo;m going to post all of my personal stuff on Mastodon from here on out, not on X. However, I still wanted to post my tech content on Mastodon as well and I needed a tool to help me with that.\nI have this program installed as a binary on my computer and I call it from my blog script which I use to generate my website with Hugo and publish it to my web hosting solution. Now it can post to Twitter / X and Mastodon as well.\nThe Go code I wrote is available here:\nhttps://github.com/mischavandenburg/mastodon\nLinks: 202312241112\n","permalink":"https://mischavandenburg.com/zet/go-mastodon-post/","summary":"This morning I had some fun writing a Go program which takes the latest blog post from my RSS feed and posts it to Mastodon. It is based on my twitter cli I wrote a while ago.\nIt was fun to write some Go again, it has been far too long. I\u0026rsquo;ve been writing mostly Terraform and yaml recently and working a lot with infrastructure. I haven\u0026rsquo;t been doing any projects at home that required programming.","title":"Wrote A Go Program That Posts To Mastodon"},{"content":"Watched this very insightful talk on microservice architecture.\nSome things I learned:\nmicroservices should be autonomous: they should have minimal dependencies on other microservices if they have dependencies they should only be 1 layer deep a microservice should not be calling another microservice which calls another microservice dependencies can be reversed by pushing data towards the service for example: a pricing database can be dumped and pushed to a catalog service once a day data duplication is not a sin Another powerful point he made is that the architecture is a reflection from the organization\u0026rsquo;s structure. He explained that the company was organized in teams and that managers of teams were promoted based on the amount of people they managed. This meant that managers were making up services which needed people to build them, which led to a proliferation of services and dependencies.\nIn order to have a correct software strucutre, the organization must be organized correctly first.\nLinks: 202312231912\nhttps://youtu.be/gfh-VCTwMw8?si=QqHcbG_5ezx5qJX2\n","permalink":"https://mischavandenburg.com/zet/talk-avoiding-microservice-disasters/","summary":"Watched this very insightful talk on microservice architecture.\nSome things I learned:\nmicroservices should be autonomous: they should have minimal dependencies on other microservices if they have dependencies they should only be 1 layer deep a microservice should not be calling another microservice which calls another microservice dependencies can be reversed by pushing data towards the service for example: a pricing database can be dumped and pushed to a catalog service once a day data duplication is not a sin Another powerful point he made is that the architecture is a reflection from the organization\u0026rsquo;s structure.","title":"Talk: Avoiding Microservice Megadisasters by Jimmy Bogard"},{"content":"I\u0026rsquo;ve been struggling with this ever since I started blogging 2 years ago (can\u0026rsquo;t believe it\u0026rsquo;s already been that long). But I\u0026rsquo;ve finally made the decision: I\u0026rsquo;m keeping this blog at mischavandenburg.com as a tech and career focused blog where I share technical knowledge and notes, and I\u0026rsquo;m starting a new personal blog at mischavandenburg.blog.\nI have fallen in love with blogging. Documenting my own life in writing has become a very therapeutic and valuable experience to me. Even if nobody reads what I put out into the world, it is incredibly satisfying to scroll through your own blog and go back in time to see what I was learning or doing or feeling at that time.\nBlogging is an expression of creativity to me. My fingers move on the keyboard and an article appears. I\u0026rsquo;m creating text. I\u0026rsquo;m not a visual person, unfortunately. I don\u0026rsquo;t see images when I think and I\u0026rsquo;m rather bad at drawing or arranging things in a pleasing way. Throughout my life I have expressed my creativity in music, but I consciously haven\u0026rsquo;t engaged in making music in the past few years because it takes up so much time in practice. But I have found a very rewarding way of expressing myself in writing by blogging.\nHowever, I feel that I am constraining myself on this blog because it is is linked to my LinkedIn account and I keep posting to LinkedIn from here. Even though I have nothing to hide, I\u0026rsquo;m pouring my thoughts out on the internet after all, I do feel that LinkedIn is best kept as a professional platform where people can expect consistency and a focus on technical topics, even when they navigate away from there to my own blog.\nBut I have so much more to give. I want to write about topics that deeply interest me: meditation, Buddhism, running, plant-based diet, healthy living, longevity, mental health, addiction. I feel the urge to pour out these thoughts freely in a place I can call my home on the web, without feeling any restraint or concern. That\u0026rsquo;s why I gave my new blog the headline \u0026ldquo;A place where I can truly be free\u0026rdquo;.\nAnother reason why I\u0026rsquo;m splitting it off is because I intend to use this blog in the process of becoming a Microsoft MVP. The achievement of that goal was not in my mind when I started blogging about personal topics on this blog, but now I think it will be best to keep this blog clean and technical in order to present a clear image in the MVP approval process.\nI don\u0026rsquo;t care about SEO or ads or the amount of views this blog gets. It\u0026rsquo;s not about SEO optimization or having to have a niche. I\u0026rsquo;m more concerned with giving any readers a good experience as possible.\nSo here we go. This blog has a (re)new(ed) technical focus, and I\u0026rsquo;m going to keep sharing my notes, learnings, AKS and Azure updates and other discoveries I make along the way. But I\u0026rsquo;ll also be migrating away the personal posts on running, weeknotes and other posts away from this blog.\nIf you are interested in my personal ramblings though, make sure to check out my new personal blog here:\nhttps://mischavandenburg.blog\nLinks: 202312171212\n","permalink":"https://mischavandenburg.com/zet/splitting-to-personal-blog/","summary":"I\u0026rsquo;ve been struggling with this ever since I started blogging 2 years ago (can\u0026rsquo;t believe it\u0026rsquo;s already been that long). But I\u0026rsquo;ve finally made the decision: I\u0026rsquo;m keeping this blog at mischavandenburg.com as a tech and career focused blog where I share technical knowledge and notes, and I\u0026rsquo;m starting a new personal blog at mischavandenburg.blog.\nI have fallen in love with blogging. Documenting my own life in writing has become a very therapeutic and valuable experience to me.","title":"Decided to split up into a technical and personal blog after all"},{"content":"A great essay about small personal websites coded in HTML. Brings me back.\nhttps://neustadt.fr/essays/the-small-web/\nLinks: 202312150812\n","permalink":"https://mischavandenburg.com/zet/rediscovering-the-small-web/","summary":"A great essay about small personal websites coded in HTML. Brings me back.\nhttps://neustadt.fr/essays/the-small-web/\nLinks: 202312150812","title":"Link: Rediscovering The Small Web"},{"content":"Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.\nPlease make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.\nhttps://github.com/Azure/AKS/releases/tag/2023-11-28\nLinks: 202312120812\n","permalink":"https://mischavandenburg.com/zet/ensure-cgroupsv2-compatibility/","summary":"Are you currently working with containerizing older Java or .NET applications? From Kubernetes 1.29, the default cgroups implementation on Azure Linux AKS nodes will be cgroupsv2. Older versions of Java, .NET and NodeJS do not support memory querying v2 memory constraints and this will lead to out of memory (OOM) issues for workloads.\nPlease make sure that your older containerized applications are compatible with cgroupsv2 or you might be in quite some pain in the future.","title":"Ensure Cgroupsv2 compatiblity when containerizing old apps"},{"content":"Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.\nThe real art is to build distributed systems on top of it, and it is dauntingly complex. That\u0026rsquo;s why I\u0026rsquo;m so attracted to it.\nIt does make clear just how great Azure\u0026rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I\u0026rsquo;m under the impression it is done really well. Which is why I\u0026rsquo;m on a mission to become an AKS expert.\nhttps://www.reddit.com/r/devops/comments/18e4nuw/whats_so_hard_in_kubernetes/?utm_source=share\u0026amp;utm_medium=web2x\u0026amp;context=3\nLinks: 202312101012\n","permalink":"https://mischavandenburg.com/zet/whats-so-hard-about-kubernetes/","summary":"Read an interesting Reddit thread today where commenters explained the complexities of running Kubernetes. Indeed it is easy to set up, but that is only the beginning.\nThe real art is to build distributed systems on top of it, and it is dauntingly complex. That\u0026rsquo;s why I\u0026rsquo;m so attracted to it.\nIt does make clear just how great Azure\u0026rsquo;s AKS offering actually is. So much of the administrative overhead is automated, and so far I\u0026rsquo;m under the impression it is done really well.","title":"What's so hard in Kubernetes?"},{"content":"Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.\nIt contains:\nkube-state-metrics, which gathers metrics about Kubernetes objects Node exporter, which gathers metrics about Kubernetes nodes OpenCost, which interprets the above to create cost metrics for the cluster, and Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.g. on Grafana Cloud).\nFor my current project I\u0026rsquo;m setting it up to a Grafana Cloud stack, but as stated above it can also be used with a local or other remote Prometheus instance.\nakv2k8s We use akv2k8s on our clusters to synch secrets from Azure Key Vaults to Kubernetes secrets or injecting them as environment variables. This way we can prevent checking in secrets to our code.\nHowever, when I first tried to deploy this chart, external secrets were not supported. They have now been fixed by Pete Wall and I managed to deploy the k8s-monitoring by writing a wrapper Helm chart with the external secret objects.\ncode All code is available in my lab repo. I wrote the following Chart.yaml:\napiVersion: v2 name: k8s-monitoring version: 1.0.0 description: This chart deploys the k8s-monitoring chart with custom secrets dependencies: - name: k8s-monitoring version: 0.5.1 repository: https://grafana.github.io/helm-charts/ Then I created a templates directory and added secrets.yaml:\n--- apiVersion: spv.no/v1alpha1 kind: AzureKeyVaultSecret metadata: name: prometheus-user spec: vault: name: {{ .Values.keyVault }} object: name: prometheus-user type: secret output: secret: name: grafana-agent-credentials-akv2k8s dataKey: prometheus-user --- apiVersion: spv.no/v1alpha1 kind: AzureKeyVaultSecret metadata: name: prometheus-password spec: vault: name: {{ .Values.keyVault }} object: name: prometheus-password type: secret output: secret: name: grafana-agent-credentials-akv2k8s dataKey: prometheus-password --- apiVersion: spv.no/v1alpha1 kind: AzureKeyVaultSecret metadata: name: prometheus-host spec: vault: name: {{ .Values.keyVault }} object: name: prometheus-host type: secret output: secret: name: grafana-agent-credentials-akv2k8s dataKey: prometheus-host --- Then, to use the external secrets in the values file, I used the following configuration:\nkeyVault: kv-123-hello k8s-monitoring: cluster: name: aks-123-hello externalServices: prometheus: hostKey: prometheus-host basicAuth: usernameKey: prometheus-user passwordKey: prometheus-password secret: create: false name: grafana-agent-credentials-akv2k8s namespace: k8s-monitoring extraConfig: |- logging { level = \u0026#34;info\u0026#34; format = \u0026#34;logfmt\u0026#34; } This chart can be deployed using helm install but we are using GitOps like the gods intended. When this is configured in ArgoCD, the chart will be deployed with the secrets templates. These templates will retreive the secrets from the Azure Key Vault and synch them to the grafana-agent-credentials-akv2k8s secret.\nbug Althoug this might have been a slight bug during the first implementation of this chart, I did run into the following error in my first attempts. The logs of the Grafana Agent showed:\nunsupported protocol scheme \\\u0026#34;\\\u0026#34;\u0026#34; After a lot of debugging I decided to completely rebuild the deployment from scratch, and then I found out that I had put the externalServices.prometheus and externalServices.secret objects in a different order which messed up the rest of the values file. It is important to keep the order that is given in the values.yaml in the k8s-monitoring repo.\nLinks: Helm chart:\nhttps://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring\nGitHub issue:\nhttps://github.com/grafana/k8s-monitoring-helm/issues/81#issuecomment-1828771508\nLink to example code in my lab repo:\nhttps://github.com/mischavandenburg/lab/tree/main/kubernetes/k8smonitoring-secrets\n202311281011\n","permalink":"https://mischavandenburg.com/zet/grafana-agent-with-custom-secrets-akv2k8s/","summary":"Grafana has developed a Helm chart which greatly simplifies the deployment of a monitoring stack to your Kubernetes clusters.\nIt contains:\nkube-state-metrics, which gathers metrics about Kubernetes objects Node exporter, which gathers metrics about Kubernetes nodes OpenCost, which interprets the above to create cost metrics for the cluster, and Grafana Agent, which scrapes the above services to forward metrics to Prometheus and logs to Loki The Prometheus and Loki services may be hosted on the same cluster, or remotely (e.","title":"Deploying Grafana Agent With Custom Secrets From Azure Key Vault Using Akv2k8s And K8s-Monitoring Helm Chart"},{"content":"When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I\u0026rsquo;m therefore starting a project to always run applications with multiple replicas.\nHowever, running multiple replicas is not the only necessary improvement here. Even though a pod is running with multiple replicas, that does not mean that Kubernetes will always keep them alive. When you do an AKS cluster upgrade, nodes are drained one by one and the pods are moved to a node with the higher k8s version. Technically, when draining a node, Kubernetes could kill both of the pods at the same time if they are running on the same node.\nTo prevent this we use pod disruption budgets\nHelm We\u0026rsquo;re using a Helm chart to deploy with ArgoCD. I\u0026rsquo;m adding this to the templates directory:\napiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: pyramid-backend-pdb spec: minAvailable: 1 selector: matchLabels: tier: backend I\u0026rsquo;m selecing all the pods that have the label \u0026ldquo;tier: backend\u0026rdquo;.\nTo verify that I\u0026rsquo;m targeting the right pods, I run:\nkubectl get pods -l tier=backend\nCurrently my deployment has only two replicas:\n(ins)$ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE pyramid-deployment-backend 1/1 1 1 297d So when I check my PDB it will show no allowed disruptions.\n(ins)$ k get pdb NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE pyramid-backend-pdb 1 N/A 0 13m This is good, this is the expected behaviour. Kubernetes will now prevent me from draining the node because there are no allowed disruptions.\nThen I scale up the deployment:\n(ins)$ k scale deploy --replicas=2 pyramid-deployment-backend deployment.apps/pyramid-deployment-backend scaled And then I get an allowed disruption of 1:\n(ins)$ k get pdb NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE pyramid-backend-pdb 1 N/A 1 15m With this configuration, Kubernetes will never kill all of the pods simultaneously in case a node needs to be drained. It will make sure that one of the pods keeps running when it is rescheduling pods for an upgrade.\nLinks: https://kubernetes.io/docs/tasks/run-application/configure-pdb/\n202311271411\n","permalink":"https://mischavandenburg.com/zet/implementing-poddisruptionbudgets/","summary":"When I was doing the first round of AKS cluster upgrades at my current client, I noticed we were running a lot of pods with only 1 replica. I always try to lift my clients to the next level by leveraging Cloud Native technologies as much as possible. I\u0026rsquo;m therefore starting a project to always run applications with multiple replicas.\nHowever, running multiple replicas is not the only necessary improvement here.","title":"Implementing Pod Disruption Budgets"},{"content":"Been working on a nice project lately. My current client has chosen to run Grafana Cloud and the instance was running in West US, which was leading to slower performance. I took up the project of provisioning a new instance in West Europe. Although I would have preferred to host Grafana ourselves or use Azure Managed Grafana, this is what the client has chosen so I that is what I\u0026rsquo;ll have to work with.\nAlso taking the opportunity to refactor the IaC and review our authrozation methods. We were using LBAC but I would rather use RBAC.\nA few goals for the configuration:\nAuthentication to the Grafana Cloud instance must happen through Entra ID User management is based on Entra ID groups Teams within Grafana are automatically syched with Entra ID groups In other words, if a new user is added to an Entra ID group, it must automatically be added to the team in Grafana To provision the instance I used the following terraform code. First we need the providers.tf:\nterraform { required_version = \u0026#34;~\u0026gt; 1.4\u0026#34; required_providers { grafana = { source = \u0026#34;grafana/grafana\u0026#34; version = \u0026#34;\u0026gt;= 1.40.1\u0026#34; } } } provider \u0026#34;grafana\u0026#34; { alias = \u0026#34;grafana_cloud\u0026#34; cloud_api_key = var.grafana_api_key } # Provider for managing the content of the Grafana Cloud instance provider \u0026#34;grafana\u0026#34; { alias = \u0026#34;content\u0026#34; url = grafana_cloud_stack.vanoord_stack.url auth = grafana_cloud_stack_service_account_token.cloud_sa.key } Here I\u0026rsquo;m declaring a provider for provisioning the cloud instance itself, \u0026ldquo;grafana_cloud\u0026rdquo;, and a second one to manage the content.\nNext we provision the instance and service accounts, in cloud-instance.tf :\nresource \u0026#34;grafana_cloud_stack\u0026#34; \u0026#34;vanoord_stack\u0026#34; { provider = grafana.grafana_cloud name = \u0026#34;vanoord\u0026#34; slug = \u0026#34;vanoord\u0026#34; region_slug = \u0026#34;prod-eu-west-3\u0026#34; # Example \u0026#34;us\u0026#34;,\u0026#34;eu\u0026#34; etc } resource \u0026#34;grafana_cloud_stack_service_account\u0026#34; \u0026#34;cloud_sa\u0026#34; { provider = grafana.grafana_cloud stack_slug = grafana_cloud_stack.vanoord_stack.slug name = \u0026#34;terraform-serviceaccount\u0026#34; role = \u0026#34;Admin\u0026#34; is_disabled = false } resource \u0026#34;grafana_cloud_stack_service_account_token\u0026#34; \u0026#34;cloud_sa\u0026#34; { provider = grafana.grafana_cloud stack_slug = grafana_cloud_stack.vanoord_stack.slug name = \u0026#34;terraform-serviceaccount-key\u0026#34; service_account_id = grafana_cloud_stack_service_account.cloud_sa.id } resource \u0026#34;grafana_folder\u0026#34; \u0026#34;mischa\u0026#34; { provider = grafana.content title = \u0026#34;Mischa Test Folder\u0026#34; } Next I configured the instance to enable authentication with Entra ID. The process was quite straight forward and I used this documentation:\nhttps://grafana.com/docs/grafana-cloud/account-management/authentication-and-permissions/authorization/#configure-oauth-with-microsoft-azure-ad\nHowever, make sure to add this configuration under \u0026ldquo;Token Configuration\u0026rdquo; in the App Registration:\nAnd add the following Graph API permissions:\nWhen adding a new team, the group ID\u0026rsquo;s of the Entra ID groups must also be added to the Grafana Cloud instance in Grafana Cloud under Security -\u0026gt; Advanced Auth -\u0026gt; Azure AD -\u0026gt; Configure -\u0026gt; Allowed Groups.\nFinally, to automatically synch the memebers of the Entra ID groups to teams in the Grafana Cloud instance, I wrote the following code:\nlocals { teams = { \u0026#34;team1\u0026#34; = { \u0026#34;AADGroupObjectIds\u0026#34; = [\u0026#34;id1\u0026#34;] }, \u0026#34;team2\u0026#34; = { \u0026#34;AADGroupObjectIds\u0026#34; = [\u0026#34;id2\u0026#34;] }, } } resource \u0026#34;grafana_team\u0026#34; \u0026#34;teams\u0026#34; { provider = grafana.content for_each = local.teams name = each.key ignore_externally_synced_members = false team_sync { groups = each.value.AADGroupObjectIds } } With this setup the members of the Entra ID groups are automatically synched to the teams in Grafana Cloud.\nIn the next article I\u0026rsquo;ll add data sources and authorize teams to data sources and other resources within Grafana.\nLinks: 202311171711\n[[Azure]]\n[[grafana]]\n","permalink":"https://mischavandenburg.com/zet/grafana-cloud-azure-ad/","summary":"Been working on a nice project lately. My current client has chosen to run Grafana Cloud and the instance was running in West US, which was leading to slower performance. I took up the project of provisioning a new instance in West Europe. Although I would have preferred to host Grafana ourselves or use Azure Managed Grafana, this is what the client has chosen so I that is what I\u0026rsquo;ll have to work with.","title":"Setting up Grafana Cloud with Entra ID authentication using Terraform"},{"content":"Nice article that explains why blogging is a satisfying activity. I\u0026rsquo;ve written the exact same reasons of why I blog on my about page where I wrote that it is very satisfying to scroll through your own feed and see what you have been up to. My blog is a time machine.\nhttps://brainbaking.com/post/2023/11/on-writing-for-yourself-in-public/\nLinks: 202311150711\n","permalink":"https://mischavandenburg.com/zet/link-on-writing-for-yourself-in-public/","summary":"Nice article that explains why blogging is a satisfying activity. I\u0026rsquo;ve written the exact same reasons of why I blog on my about page where I wrote that it is very satisfying to scroll through your own feed and see what you have been up to. My blog is a time machine.\nhttps://brainbaking.com/post/2023/11/on-writing-for-yourself-in-public/\nLinks: 202311150711","title":"Link: On Writing For Yourself In Public"},{"content":"Great news! KEDA finally went GA for AKS.\nhttps://learn.microsoft.com/en-us/azure/aks/keda-about\nLinks: 202311081711\n","permalink":"https://mischavandenburg.com/zet/keda-ga/","summary":"Great news! KEDA finally went GA for AKS.\nhttps://learn.microsoft.com/en-us/azure/aks/keda-about\nLinks: 202311081711","title":"KEDA went GA for AKS!"},{"content":"I use this bash code to get secrets from Azure key vaults using the Azure CLI.\nMake sure to set your subscription with az account set -s 12321424\nkvname=kv-vo-rea-prd-weu-001 secretname=worker-ad-client-secret az keyvault secret show --vault-name $kvname --name $secretname --query value -o tsv Links: 202310312010\n[[Azure]]\n","permalink":"https://mischavandenburg.com/zet/list-azure-key-vault-secret/","summary":"I use this bash code to get secrets from Azure key vaults using the Azure CLI.\nMake sure to set your subscription with az account set -s 12321424\nkvname=kv-vo-rea-prd-weu-001 secretname=worker-ad-client-secret az keyvault secret show --vault-name $kvname --name $secretname --query value -o tsv Links: 202310312010\n[[Azure]]","title":"Getting A Secret from an Azure Key Vault From The Command Line"},{"content":"Chapter 11 Virtual WAN Easy Mode Hub-Spoke\nincorporates multiple hybrid connections (vpn/expressroute) incorporates multiple VNets Dynamically handles routing easily deploys NVA/Firewall Use case You technically don\u0026rsquo;t need to use it for anything, you can do the same with a normal hub-spoke built using VNets.\nHowever, when you do, you\u0026rsquo;re in charge of:\nadd/update routes network gateway connections integration with virtual appliances / firewalls Virtual WAN makes these tasks easier.\nComponents Virtual Wan\noverlay of entire WAN infrastructure includes 1 or more hubs Hub\nmanaged VNet core of v wan main connection point of vnets, vpn, er circuits Hybrid connections: s2s vpn, p2s vpn, er circuits\nOther connections\nVNet connections hub-to-hub Transitive networking In VWAN transitive is possible.\nVNets can communicate with other VNEts through hubs with standard WAN SKU. Not the basic sku.\nHas managed router that forwards traffic between spokes.\nTransitive across hubs in the same WAN.\nNormally you have to explicitly create all the peerings, but in the WAN all VNets can communicate with eachother by default.\nSKUS Basic: only s2s vpn. Not transitive.\nStandard: all the rest Transitive natworking, NVA in WAN\nVirtual Hub Routing Virtual hubs contain their own route tables.\nHub connections (all types) associate and propagate routes to the hub\u0026rsquo;s Default route table.\nStatic (custom) routes can be added.\nEach network connected to a hub can either associate or propagate its routes.\nAssociation The connection and its network can learn the routes and adopt those routes for its own connection.\nAn association is learning the virtual hub\u0026rsquo;s routes and then taking on those routes for its own network routing table.\nAll connections associate to the Default route table. Propagation to the Default route table is optional to enable further isolation.\nAll connections will learn the hub\u0026rsquo;s default routes, but not all connections will contribute to the Default route table.\nPropagation A connection, or more specifically, the network behind its connection, adds its own network routing table to the hub\u0026rsquo;s route table.\nIt is providing or propagating routes to the hub.\nThere is a Default and None route table. Propagating to the None route table does not add VNet routes.\nThis isolates the the network because its routes are not published to the hub.\nCustom Route Tables Additional route tables alongside the hub\u0026rsquo;s Default table.\nChoose which connections associate (learn) and propagate (add to) custom route tables.\nUseful for:\nisolating groups of networks from other networks routing traffic through several NVA\u0026rsquo;s attached to the hub Custom route tables will take priority over custom route tables\nVirtual WAN Hybrid Connections Merging the WAN with on-site networks through S2S/P2S/VPN and ExpressRoute.\nScale Units Important for exam.\nThink of these as a \u0026ldquo;bandwith SKU\u0026rdquo;.\nAssigns the aggregate througput (bandwith) to the connection unit. Expressed in Mbps or Gbps.\nVPN and ExpressRoute connections use different scale units.\nLinks: 202310290910\n[[az-700]]\n","permalink":"https://mischavandenburg.com/zet/az-700-virtual-wan/","summary":"Chapter 11 Virtual WAN Easy Mode Hub-Spoke\nincorporates multiple hybrid connections (vpn/expressroute) incorporates multiple VNets Dynamically handles routing easily deploys NVA/Firewall Use case You technically don\u0026rsquo;t need to use it for anything, you can do the same with a normal hub-spoke built using VNets.\nHowever, when you do, you\u0026rsquo;re in charge of:\nadd/update routes network gateway connections integration with virtual appliances / firewalls Virtual WAN makes these tasks easier.\nComponents Virtual Wan","title":"AZ-700 Notes - Virtual WAN"},{"content":"Upgrading clusters with databases When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.\nThe EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.\nIn order to be able to run the AKS upgrade you will need to set the following:\nk cnp maintenance set --all-namespaces --reusePVC\ncnp is a kubectl plugin which you need to install to manage the EDB operator --reusePVC can only be used if all the nodes are in the same availability zone use this command to list all the zones: kubectl describe nodes | grep -e \u0026quot;Name:\u0026quot; -e \u0026quot;topology.kubernetes.io/zone\u0026quot; When the upgrade is complete, run k cnp maintenance set --all-namespaces\nGetting the status You can get all information about the database cluster:\nk cnp status pyramid-cluster-database -n mycommodity\nMost deployments have the same name pyramid-cluster-database.\nProblems Sometimes a database replica will not come up properly and you can get the following error message:\n(FATAL: the database system is starting up (SQLSTATE 57P03))\nTo fix this, first disable reusing PVC by running:\nk cnp maintenance set -n problemdatabase\n\u0026ldquo;problemdatabase\u0026rdquo; is the namespace where the database cluster is deployed.\nThen delete the PVC of the problematic pod. This should trigger a new replica to be created from scratch. Depending on the size of the DB it can take a lot of time.\nLinks: 202310171410\n[[Kubernetes]]\n","permalink":"https://mischavandenburg.com/zet/edb-cluster-notes/","summary":"Upgrading clusters with databases When upgrading this type of cluster it is better to upgrade the control plane first and then upgrade the node pools one by one.\nThe EDB operator prevents you to drain nodes with databases on them with PodDisruptionBudgets.\nIn order to be able to run the AKS upgrade you will need to set the following:\nk cnp maintenance set --all-namespaces --reusePVC\ncnp is a kubectl plugin which you need to install to manage the EDB operator --reusePVC can only be used if all the nodes are in the same availability zone use this command to list all the zones: kubectl describe nodes | grep -e \u0026quot;Name:\u0026quot; -e \u0026quot;topology.","title":"Some notes on upgrading AKS clusters running EnterpriseDB"},{"content":"You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.\nhttps://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup\nLinks: 202310130610\n[[AKS]]\n[[Kubernetes]]\n[[Azure]]\n","permalink":"https://mischavandenburg.com/zet/aks-azure-backup-preview/","summary":"You can back up your AKS clusters using Azure Backup in preview! Preferably you have your clusters stateless and you can redeploy everything from code when sh*t hits the fan. However, I can think of a few enterprise use cases that will be relevant for this new feature.\nhttps://learn.microsoft.com/en-us/azure/backup/azure-kubernetes-service-cluster-backup\nLinks: 202310130610\n[[AKS]]\n[[Kubernetes]]\n[[Azure]]","title":"Backing up AKS Clusters with Azure Backup is now in preview"},{"content":"This week I\u0026rsquo;ve been tackling an issue at work with akv2k8s.\nWe are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:\nError: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: \u0026#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)\u0026#34;, container: bootstrap-controller) Pods have a security context where you specify how the containers in the pod should be run.\nsecurityContext: runAsUser: 998 runAsGroup: 996 runAsNonRoot: true fsGroup: 996 Here we are telling the pod that we may not run as the root user and that we will run as user 998.\nMy error states that it cannot verify whether it is running as root or not because we are using a non-numeric user.\nWe were dumbfounded by this because the deployment and ReplicaSet neatly had their securitycontexts configured and the pods should inherit them. After a lot of searching I figured out that the securitycontext of the pods was actually empty, so somehow it must be overridden somewhere.\nThe culprit was the akv2k8s envinjector. This tool injects the secrets into the environment of the pods. It took a lot of experimentation but we managed to narrow it down to the akv2k8s upgrade and eventually we found this GitHub issue:\nhttps://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605\nFor some reason the envinjector is overriding the securitycontext of the pods. The issue was fixed by downgrading the helm chart back to 2.4.2\nI did learn a lot in the process though!\nLinks: 202310130610\nhttps://www.enterprisedb.com/docs/postgres_for_kubernetes/latest/installation_upgrade/\nhttps://github.com/SparebankenVest/azure-key-vault-to-kubernetes/issues/605\nhttps://stackoverflow.com/questions/53949329/kubernetes-runasnonroot-failing-createcontainerconfigerror\nhttps://github.com/cloudnative-pg/cloudnative-pg\n","permalink":"https://mischavandenburg.com/zet/akv2k8s-edb-initcontainer-securitycontext/","summary":"This week I\u0026rsquo;ve been tackling an issue at work with akv2k8s.\nWe are running CloudnativePG, similar to EnterpriseDB. After upgrading akv2k8s to v2.5.0 our database pods were not coming up anymore due to an error with the initcontainer:\nError: container has runAsNonRoot and image has non-numeric user (nonroot), cannot verify user is non-root (pod: \u0026#34;vcs-pooler-rw-858bf7c954-vjzr4_vcs(e19bfa4c-26d8-4a6d-b4ad-bba8b52c01e6)\u0026#34;, container: bootstrap-controller) Pods have a security context where you specify how the containers in the pod should be run.","title":"Fixed an issue with akv2k8s overriding security context"},{"content":"Learned a new trick to force a new cert from letsencrypt. Had this error message:\nThe certificate request has failed to complete and will be retried: Failed to wait for order resource \u0026#34;vcs-secret-j8gnj-3121219202\u0026#34; to become ready: order is in \u0026#34;errored\u0026#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.org/docs/duplicate-certificate-limit/ I asked for too many certificates during a 168 hour period. However, there is a really easy fix. Just add a new subdomain and LetsEncrypt will treat it as an entirely new certificate!\nFor example, when using cert-manager:\napiVersion: cert-manager.io/v1 kind: Certificate metadata: creationTimestamp: \u0026#39;2023-10-12T11:09:36Z\u0026#39; generation: 2 labels: argocd.argoproj.io/instance: vcs-dev name: vcs-secret namespace: vcs ownerReferences: - apiVersion: networking.k8s.io/v1 blockOwnerDeletion: true controller: true kind: Ingress name: vcs-ingress uid: xxxxx resourceVersion: \u0026#39;226558404\u0026#39; uid: xxxx spec: dnsNames: - vcs.mischavandenburg.com - test.mischavandenburg.com issuerRef: group: cert-manager.io kind: ClusterIssuer name: letsencrypt-prod secretName: vcs-secret usages: - digital signature - key encipherment Just add another domain under spec.dnsNames and it will treat it as a new cert.\nspec: dnsNames: - vcs.mischavandenburg.com - test.mischavandenburg.com - hello.mischavandenburg.com Links: 202310121310\n[[Kubernetes]]\n","permalink":"https://mischavandenburg.com/zet/forcing-new-cert-letsencrypt-timeout/","summary":"Learned a new trick to force a new cert from letsencrypt. Had this error message:\nThe certificate request has failed to complete and will be retried: Failed to wait for order resource \u0026#34;vcs-secret-j8gnj-3121219202\u0026#34; to become ready: order is in \u0026#34;errored\u0026#34; state: Failed to create Order: 429 urn:ietf:params:acme:error:rateLimited: Error creating new order :: too many certificates (5) already issued for this exact set of domains in the last 168 hours: sadfsadf.com, retry after 2023-10-12T18:54:27Z: see https://letsencrypt.","title":"Forcing a new cert from letsencrypt when too many have been issued"},{"content":"Ran into a problem when cloning my second brain repository on a new windows machine which I need to use for work. Also trying to get familiar with VSCode although I\u0026rsquo;m doing my best to get a good WSL setup going on my new windows setup.\nOne problem I was running into was that I could not for the life of me clone my repository at all on Windows. Eventually I realized that it kept failing on a file that had a question mark in its filename.\nI renamed all the files that had invalid characters.\nIt also failed because apparently I had a file in the repo named \u0026ldquo;\u0026quot;. It didn\u0026rsquo;t like that at all.\nAfter changing the filenames it worked.\nIt is good to keep my second brain compatible with all operating systems so this is actually a good practice session.\nLinks: 202310052010\n","permalink":"https://mischavandenburg.com/zet/windows-filenames-invalid/","summary":"Ran into a problem when cloning my second brain repository on a new windows machine which I need to use for work. Also trying to get familiar with VSCode although I\u0026rsquo;m doing my best to get a good WSL setup going on my new windows setup.\nOne problem I was running into was that I could not for the life of me clone my repository at all on Windows. Eventually I realized that it kept failing on a file that had a question mark in its filename.","title":"Dont Use Question Marks in Windows Filenames"},{"content":"Although I rarely use Obsidian anymore, I still have it open because it is running the obsidian-git plugin in the background and it is constantly backing up my second brain to GitHub.\nI use iCloud to synch my vault across my iOs devices and I always create and edit my files using neovim.\nFor my new gig at work I\u0026rsquo;m forced to use a windows laptop which means I cannot use iCloud to synch my vault. Then it is a godsend to be able to install Obsidian and use that to interact with my second brain.\nIt is very good to keep it compatible with obsidian so I can use the software for synching and interacting with my second brain on other operating systems conveniently.\nLinks: 202310052010\n[[second-brain]] [[notetaking]]\n","permalink":"https://mischavandenburg.com/zet/keep-second-brain-obsidian-compatible/","summary":"Although I rarely use Obsidian anymore, I still have it open because it is running the obsidian-git plugin in the background and it is constantly backing up my second brain to GitHub.\nI use iCloud to synch my vault across my iOs devices and I always create and edit my files using neovim.\nFor my new gig at work I\u0026rsquo;m forced to use a windows laptop which means I cannot use iCloud to synch my vault.","title":"It is good to keep your second brain compatible with Obsidian"},{"content":"Recently I was comparing apps for navigation (TopoGPS and Outdooractive). Both apps are great, but the main perk of Outdooractive is that it has a web interface. You can plan routes from your computer and access them on the iPhone app, but TopoGPS is on the iPhone only.\nThe app I probably have the most interaction with during the day is my calorie tracking app. I\u0026rsquo;ve been using YAZIO for about two years and it has been satisfactory, but there were a few small annoyances to the app and I was never completely satisfied with.\nA missing web interface being one of them. I\u0026rsquo;d love to be able to sit behind my 4K screen and plan out the week\u0026rsquo;s nutrition from there instead of using the clunky copy function on my phone.\nEnter MyFitnessPal. It has a web interface. It has charts and graphs. Me happy.\nWhy didn\u0026rsquo;t I chose this app when I began using an app for my phone? I remember trying it out, but I was unhappy with it somehow. I remember something about the food database not containing the foods I was eating at the time. I\u0026rsquo;ve been doing some testing and up until now I haven\u0026rsquo;t had to add any foods.\nI have a year\u0026rsquo;s subscription on YAZIO until March so it is a bit of a waste, but I\u0026rsquo;m already sold on the web interface of MFP. I have one free month of premium and I\u0026rsquo;ll make the decision then.\nLinks: 202310011810\n[[Health]]\n[[diet]]\n[[vegan]]\n[[weight-loss]]\n","permalink":"https://mischavandenburg.com/zet/myfitnesspal-web-interface/","summary":"Recently I was comparing apps for navigation (TopoGPS and Outdooractive). Both apps are great, but the main perk of Outdooractive is that it has a web interface. You can plan routes from your computer and access them on the iPhone app, but TopoGPS is on the iPhone only.\nThe app I probably have the most interaction with during the day is my calorie tracking app. I\u0026rsquo;ve been using YAZIO for about two years and it has been satisfactory, but there were a few small annoyances to the app and I was never completely satisfied with.","title":"Apps with web interfaces win any day, switching to MyFitnessPal"},{"content":"Before I went on holiday I formed the intention to begin studying again after I came back.\nDuring my current gig I\u0026rsquo;m configuring a fair bit of networking, VNets, peerings, firewall rules and set up a full hub-spoke network from scratch, so I\u0026rsquo;ve had a fair bit of practical experience. Now that the experience is still fresh in the mind I figured I\u0026rsquo;d go for this certification.\nUsually I only use Microsoft Learn and practice exams, but this time around I want to try out a video course. I purchased a year of A Cloud Guru access and so far it has been a good experience.\nI\u0026rsquo;ll publish my notes as I go along.\nLinks: 202309280909\n","permalink":"https://mischavandenburg.com/zet/starting-az-700/","summary":"Before I went on holiday I formed the intention to begin studying again after I came back.\nDuring my current gig I\u0026rsquo;m configuring a fair bit of networking, VNets, peerings, firewall rules and set up a full hub-spoke network from scratch, so I\u0026rsquo;ve had a fair bit of practical experience. Now that the experience is still fresh in the mind I figured I\u0026rsquo;d go for this certification.\nUsually I only use Microsoft Learn and practice exams, but this time around I want to try out a video course.","title":"Began studying for AZ-700 Designing and Implementing Microsoft Azure Networking Solutions"},{"content":"I notice that I\u0026rsquo;m not writing in my journal every day like I used to. I always wrote down what I did, what I felt, what my plans and ambitions were, every single day.\nDuring my holiday I started writing by hand in a notebook again which was really nice. It felt good to feel the pencil scratching on the paper again and to practice my cursive writing.\nAfter I came back I first tried to keep journaling by hand again, but after a few days the journal remained untouched. I figured I would go back to my usual digital journaling again by writing daily notes.\nI\u0026rsquo;m a litlte intruiged. I do feel very relaxed and at ease, perhaps I do not need to write so much at this time? Writing down frustrations and feelings definitely has been a certain way of processing them.\nAnd perhaps my routine is reaching such a degree of stability that it is simply not worth the effort of writing it all down all the time.\nRather than writing in my personal notes, I do have the intention to write more zets in my public zettelkasten on all the topics that interest me currently. Micronotes.\nLinks: 202309282109\n","permalink":"https://mischavandenburg.com/zet/not-journaling-as-often/","summary":"I notice that I\u0026rsquo;m not writing in my journal every day like I used to. I always wrote down what I did, what I felt, what my plans and ambitions were, every single day.\nDuring my holiday I started writing by hand in a notebook again which was really nice. It felt good to feel the pencil scratching on the paper again and to practice my cursive writing.\nAfter I came back I first tried to keep journaling by hand again, but after a few days the journal remained untouched.","title":"I'm not journaling as often as I used to"},{"content":"Yesterday a few interesting AKS related features became Generally Available on Azure.\nKEDA add-on makes it easier to scale your applications on AKS cluster.\nhttps://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/\nYou can have a flexible and customized strategy for node-level OS security updates.\nhttps://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/\nUse Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads\nhttps://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/\nPreview Public preview: AKS support for Kubernetes version 1.28\nhttps://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/\nLinks: 202309281009\n[[Azure]]\n[[Kubernetes]]\n","permalink":"https://mischavandenburg.com/zet/azure-updates-sept-23/","summary":"Yesterday a few interesting AKS related features became Generally Available on Azure.\nKEDA add-on makes it easier to scale your applications on AKS cluster.\nhttps://azure.microsoft.com/en-us/updates/generally-available-keda-addon-for-aks/\nYou can have a flexible and customized strategy for node-level OS security updates.\nhttps://azure.microsoft.com/en-us/updates/ga-node-os-patching-nodeimage-feature-in-aks/\nUse Vertical Pod Autoscaling add-on for AKS to improve cost-efficiency, and cluster utilization for your workloads\nhttps://azure.microsoft.com/en-us/updates/ga-vertical-pod-autoscaling-addon-for-aks/\nPreview Public preview: AKS support for Kubernetes version 1.28\nhttps://azure.microsoft.com/en-us/updates/public-preview-aks-support-for-kubernetes-version-128/\nLinks: 202309281009\n[[Azure]]\n[[Kubernetes]]","title":"Some Interesting Features went GA on Azure"},{"content":"Past week we\u0026rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.\nIf a deployment has a pod disruption budget which is incorrectly configured, it might show up as ALLOWED DISRUPTIONS: 0. When this happens, the node cannot be drained and you will get an error message in your events.\nk get poddisruptionbudgets.policy\nk get events\nThe error message will say something like \u0026ldquo;Too man eviction attempts, usually a pdb\u0026rdquo; (I lost the shell output so can\u0026rsquo;t copy atm).\nKubernetes is in a situation where it needs to schedule the pod on another node, but it is unable to do do so because we are telling Kubernetes that it is not allowed to have any disruptions on the deployment.\nKubernetes is logical, it\u0026rsquo;s doing like it\u0026rsquo;s told. But it\u0026rsquo;s frustrating because you can be sitting there waiting for 20 minutes wondering why your node isn\u0026rsquo;t draining.\nAt my current gig we are not responsible for the content on the clusters and we should not meddle with the application teams\u0026rsquo; namespaces.\nHowever, one solution is to either kill the pod manually or scale up the deployment to more replicas so there will be a higher amount of allowed disruptions.\nLinks: 202309271809\n[[Kubernetes]]\n[[Azure]]\n","permalink":"https://mischavandenburg.com/zet/pod-disruption-budget-aks/","summary":"Past week we\u0026rsquo;ve been struggling a bit with poorly configured pod disruption budgets. When you do an AKS upgrade, a new node is created and one of the old nodes is drained.\nIf a deployment has a pod disruption budget which is incorrectly configured, it might show up as ALLOWED DISRUPTIONS: 0. When this happens, the node cannot be drained and you will get an error message in your events.","title":"Pod Disruption Budgets Can Mess With Your AKS Updates"},{"content":"From here on out you can access Microsoft Learn during the exams. This is great news! This is much more like the actual experience of an engineer\u0026rsquo;s daily activities. It is arguably more important to look up things efficiently rather than memorizing everything for a test and forgetting it a few weeks later.\nLinks: 202308231608\nhttps://youtu.be/iPiq3YuhnHc\n","permalink":"https://mischavandenburg.com/zet/azure-exams-open-book/","summary":"From here on out you can access Microsoft Learn during the exams. This is great news! This is much more like the actual experience of an engineer\u0026rsquo;s daily activities. It is arguably more important to look up things efficiently rather than memorizing everything for a test and forgetting it a few weeks later.\nLinks: 202308231608\nhttps://youtu.be/iPiq3YuhnHc","title":"Azure Exams Are Open Book Exams From Now On"},{"content":"I\u0026rsquo;ve just finished my last workday and tomorrow I\u0026rsquo;m heading off to Norway! When I glance at the amount of posts I\u0026rsquo;ve made on my blog recently it\u0026rsquo;s obvious that it is time for a break from the entire routine. I\u0026rsquo;ve been tapering down to this moment for a while now, toning down the studying and prioritizing relaxation so I can enter the holidays with a clear mind.\nNow it\u0026rsquo;s time for lots of fresh air, mountains, many kilometres on the trails, and to pick up the connection to Nature I\u0026rsquo;ve made during the 9 years that I lived in the beautiful North.\n[[The Norway holiday was an amazing experience]]\nLinks: 202308231608\n","permalink":"https://mischavandenburg.com/zet/vacation-2023/","summary":"I\u0026rsquo;ve just finished my last workday and tomorrow I\u0026rsquo;m heading off to Norway! When I glance at the amount of posts I\u0026rsquo;ve made on my blog recently it\u0026rsquo;s obvious that it is time for a break from the entire routine. I\u0026rsquo;ve been tapering down to this moment for a while now, toning down the studying and prioritizing relaxation so I can enter the holidays with a clear mind.\nNow it\u0026rsquo;s time for lots of fresh air, mountains, many kilometres on the trails, and to pick up the connection to Nature I\u0026rsquo;ve made during the 9 years that I lived in the beautiful North.","title":"Vacation Time!"},{"content":"During the Bicep Community Call of July 2023 I was introduced to the new experimental testing framework that the Bicep team is working on. After learning the fundamentals of the Go programming language I saw the value of test driven software development, and it will be an advantageous improvement if we can start applying this methodology to Infrastructure as Code as well.\nTest driven software development is a software development practice that involves writing unit tests before writing the actual code, and then refactoring the code to pass the tests. Some of the advantages of test driven software development are:\nIt improves the code quality and design by making it more modular, maintainable, and extensible. It reduces the number of bugs and defects by catching them early in the development process. It enhances the documentation of the code by providing a clear specification of what each function or module does. It increases the productivity and efficiency of the developers by providing immediate feedback and reducing debugging time. It ensures high test coverage and makes it easier to add new features or refactor the code in the future. Enabling the Bicep testing framework To make the \u0026ldquo;test\u0026rdquo; keyword available in your Bicep editor, you need to enable some experimental features.\nIf you\u0026rsquo;re using VScode, open or generate your bicepconfig.json file and add the following code:\n{ \u0026#34;experimentalFeaturesEnabled\u0026#34;: { \u0026#34;testFramework\u0026#34;: true, \u0026#34;asserts\u0026#34;: true } } Now the \u0026ldquo;test\u0026rdquo; keyword should work.\nFor more information, check out this link:\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-config\nAsserts The assert keyword will be used to verify output prior to deployment.\nLet\u0026rsquo;s say I have this very simple key vault template:\nparam location string = resourceGroup().location resource kv \u0026#39;Microsoft.KeyVault/vaults@2023-02-01\u0026#39; = { name: \u0026#39;mischa-key-vault-138\u0026#39; location: location properties: { sku: { name: \u0026#39;standard\u0026#39; family: \u0026#39;A\u0026#39; } tenantId: subscription().tenantId enableRbacAuthorization: true } } assert kvSKU = kv.sku.name == \u0026#39;standard\u0026#39; In this example, the assertion would run before the actual deployment happens, and if the assertion fails, the deployment is stopped. This is of course a rather silly example, but you hopefully get the idea.\nWhy is this useful? When you are running larger deployments from files that contain a lot of resources, sometimes the deployment will fail halfway through the template because there was some unexpected error, and a lot of time is wasted waiting for the preceding resources to be deployed. In the future we can hopefully use assertions to make sure the output will be as expected at compile time.\nUnit Tests in Bicep Unit tests are a type of software testing where individual units or components of a software are tested to verify that they work as expected. A unit can be a function, a method, a module, or an object. In the Bicep testing framework, we can run unit tests on resources as well. Unit tests are usually written and run by the developers during the coding phase of the software development process. Unit tests help to improve the quality, design, and documentation of the code, as well as to catch bugs and defects early.\nNow I\u0026rsquo;ll provide an example of a unit test in Bicep. Let\u0026rsquo;s say we have this Bicep template for an AKS cluster:\n@description(\u0026#39;Name of the AKS cluster\u0026#39;) param clusterName string @description(\u0026#39;Prefix for deployment\u0026#39;) param prefix string resource aksCluster \u0026#39;Microsoft.ContainerService/managedClusters@2023-03-02-preview\u0026#39; = { name: \u0026#39;${prefix}-${clusterName}\u0026#39; location: location ... To use a simple example, let\u0026rsquo;s say I want to make sure that my cluster name will be as expected. I would write a unit test as follows:\ntest testAKS \u0026#39;aks.bicep\u0026#39; = { params: { clusterName: \u0026#39;coffee\u0026#39; prefix: \u0026#39;prod\u0026#39; } } assert assertClusterName = testAKS.resources.aksCluster.name == \u0026#39;prod-coffee\u0026#39; When I run this test.bicep file, it will take the aks.bicep file and feed the \u0026lsquo;coffee\u0026rsquo; and \u0026lsquo;prod\u0026rsquo; parameters to it. My aks.bicep file will create the string from the parameters and output the name. Then, I use the \u0026ldquo;assert\u0026rdquo; keyword to verify that the string is composed according to my expectations.\nNote: at the time of writing 2023-08-03, the \u0026ldquo;assert\u0026rdquo; keyword was still not available in Bicep 0.20.4, but there might be a problem with my local configuration which I\u0026rsquo;ve asked the team about.\n2023-08-08 update: I\u0026rsquo;ve been in touch with the Bicep team and they responded as follows:\n?Regarding your ability to access \u0026ldquo;test\u0026rdquo; but not \u0026ldquo;assert\u0026rdquo;, both key words are still only in a demo phase and not yet intended for customer use. Please hold off on trying them out for now. We will be in touch with folks who have indicated interest through the form and/or on a Community Call when there is a preview available. Thank you for your patience and sorry for any confusion.\nI guess I was a little eager with testing the keywords so soon! 😀 🚀\nConclusion I believe it will be a great development to apply test driven software development to Infrastructure as Code by implementing a test framework into Bicep. When I\u0026rsquo;m able to use this approach, I would begin with writing all of the tests for the template that I have in mind, and then I can continue to write the Bicep template itself. By using the tests I can become much more productive because I can speed up my development by using tests without having to do what-if deployments which can be rather slow and involve a lot of waiting.\nIt is great to see how the Bicep language is maturing and how it continues to introduce new features that make the lives of developers easier. I was really stoked about the new .bicepparam files, but I\u0026rsquo;m even more excited by the new testing framework.\nHowever, please note that this is still an experimental feature. I haven\u0026rsquo;t found much other information, and I\u0026rsquo;m not part of the Bicep development community (yet). There might be a chance that this feature isn\u0026rsquo;t implemented at all in the end. That being said, I would love to participate in this community and perhaps I\u0026rsquo;d be able to provide some costumer feedback, and I\u0026rsquo;m currently looking into this and I\u0026rsquo;ve reached out to a few acquaintances.\nLinks: 202308030808\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-config\nhttps://youtu.be/UuwhLi-Xe2U\n","permalink":"https://mischavandenburg.com/zet/bicep-testing-assertions/","summary":"During the Bicep Community Call of July 2023 I was introduced to the new experimental testing framework that the Bicep team is working on. After learning the fundamentals of the Go programming language I saw the value of test driven software development, and it will be an advantageous improvement if we can start applying this methodology to Infrastructure as Code as well.\nTest driven software development is a software development practice that involves writing unit tests before writing the actual code, and then refactoring the code to pass the tests.","title":"Test Driven Development for Bicep is In The Works!"},{"content":" In this video, I will walk you through the following steps:\nSet up a repo in Azure DevOps with MkDocs files Set up a Static Web App in Azure portal and link it to the repo Configure the Azure DevOps pipeline for MkDocs deployment Configure a custom theme for the website Demonstrate the pull request workflow for updating the docs By the end of this video, you will have a fully functional MkDocs site hosted on Azure Static Web Apps. You will also learn how to use Azure DevOps and Static Web Apps to collaborate on your documentation projects.\nMkDocs is a tool that lets you create beautiful documentation websites from plain text files. You can write your docs in Markdown, a simple and easy-to-use markup language, and MkDocs will convert them into a static HTML site. You can also customize the look and feel of your site with themes and plugins.\nAzure Static Web Apps is a service that hosts static web content and serverless APIs. It automatically builds and deploys your web app from a GitHub or Azure DevOps repository. It also provides features such as authentication, authorization, custom domains, and SSL certificates.\nTimestamps:\n0:00 Intro 0:40 What is MkDocs? 2:00 Set up the repo 5:00 Create web app 9:25 Configure the pipeline 11:30 Adding a few documents 15:25 Adding a custom theme 18:00 Adding docs with a pull request 21:30 Outro\nLinks: 202307281307\n","permalink":"https://mischavandenburg.com/zet/video-mkdocs-azure-webapp/","summary":"In this video, I will walk you through the following steps:\nSet up a repo in Azure DevOps with MkDocs files Set up a Static Web App in Azure portal and link it to the repo Configure the Azure DevOps pipeline for MkDocs deployment Configure a custom theme for the website Demonstrate the pull request workflow for updating the docs By the end of this video, you will have a fully functional MkDocs site hosted on Azure Static Web Apps.","title":"Video: Deploying MkDocs To Azure Using With Azure DevOps"},{"content":"New week, new record! For the first time in my life I ran 12 kilometers, and immediately another 10k PR!\nThis morning it was time for my long run, and while I was drinking my coffee I decided to watch a running technique video again to see what I could improve. Lately I have mainly focused on my heart rate and steps per minute, but today I focused more on maintaining a better posture by sticking out my chest a bit more and leaning forward a bit more. The result was that I suddenly ran much faster and more relaxed without this resulting in a higher HR, and my steps per minute were also fine afterwards with 171SPM. These days I usually run around 6:30 but I’m really surprised that I was able to set both a distance record and a higher average speed without having to make any effort except constantly maintaining a better posture. “Running Tall” was the mantra that I kept coming back to when I noticed that I started to sink back.\nLinks: 202307221107\n","permalink":"https://mischavandenburg.com/zet/running-record-12km/","summary":"New week, new record! For the first time in my life I ran 12 kilometers, and immediately another 10k PR!\nThis morning it was time for my long run, and while I was drinking my coffee I decided to watch a running technique video again to see what I could improve. Lately I have mainly focused on my heart rate and steps per minute, but today I focused more on maintaining a better posture by sticking out my chest a bit more and leaning forward a bit more.","title":"New Running Record! Ran 12km today"},{"content":"What I Learned from Kipchoge, the Fastest Marathon Runner in the World Kipchoge is not only a world-class athlete, but also a source of inspiration and wisdom for anyone who wants to improve their life. I recently listened to an interview with him on YouTube, and I was amazed by his insights on running, success, pain, discipline and happiness. Here are some of the key takeaways and a summary of what he shared.\nPreparation and Planning is the Key Kipchoge believes that success does not come by chance, but by pure and clean planning in preparation. He follows a strict program and sticks to it to the letter. He writes down everything in his journal every day, and documents his progress and challenges. He also avoids distractions and focuses on his priorities.\nI really appreciate his emphasis on planning everything out meticulously. You should approach your training with a clear goal and intention and make a plan to reach that goal. I also identify with his habit of writing everything down, of course. This is what I\u0026rsquo;ve been practicing myself for several years now, and it is nice to see yet another highly successful person who also shares this habit.\nMorning Routine Kipchoge has a morning routine which is very similar to my own. He wakes up at 5:45 and is out running by 6:10. When he comes back he has a shower and has breakfast. Only when that is finished will he look at his phone and check his messages and emails.\nI also run first thing in the morning. I really enjoy to have it done for the day, and I prefer to exercise on an empty stomach. I think I will try Kipchoge\u0026rsquo;s routine of not looking at the phone until the morning rituals are finished. I tend to check the news and read messages before I go out on my run, and then I use the run to prepare for the day. I\u0026rsquo;ll try to get out with a \u0026ldquo;clean mind\u0026rdquo; and see what happens this week.\nMovement Leads to a Long and Happy Life Kipchoge sees running as the best movement and the key to life. He says that life is about movement, and if you move, that\u0026rsquo;s how you have a good life. He runs every day, even when he does not feel like it, because he knows that running is not only good for his body, but also for his mind. He runs in a group of people, and enjoys the social aspect of running. He says that running in a group helps him to talk, exchange ideas and learn from others. He also encourages running as a family lifestyle, and says that running should be fun and enjoyable.\nPain is Part of Success Kipchoge smiles when he runs, even when he is feeling pain. He says that when he smiles while running, he is actually enjoying the pain. He explains that pain is where success is, and those who know how to handle pain will be successful and move up to another level. Kipchoge also says that training is not the same as competing, and that training happens at a slower pace where you can follow the pace of the group. But when he competes, he pushes himself to the limit and embraces the pain.\nVitamin N Kipchoge shares a simple but powerful advice: inject yourself with vitamin N. What is vitamin N? It stands for NO! Kipchoge says that you need to learn to say no to things that are not aligned with your goals and values. You also need to set your priorities right and avoid complaining. These three things will help you develop self-discipline and achieve your dreams.\nListening to Podcasts When I started running I listened to music, but after I could run for more than 20 minutes, I started to see my runs as a form of active meditation. I deliberately avoid listening to music and podcasts so I can focus completely on the running experience and use the breath, body and movement as meditation objects. However, Kipchoge says that it is all right to listen to music or podcasts during easy runs. They will make the time pass by very quickly.\nI hope you enjoyed this blog post and learned something from Kipchoge\u0026rsquo;s wisdom. If you want to listen to the full interview, you can find it here:\nLinks:\n202307221407\nhttps://www.youtube.com/watch?v=X4la2sHwZB0\nLinks: 202307221907\n","permalink":"https://mischavandenburg.com/zet/podcast-kipchoge-marathon-running/","summary":"What I Learned from Kipchoge, the Fastest Marathon Runner in the World Kipchoge is not only a world-class athlete, but also a source of inspiration and wisdom for anyone who wants to improve their life. I recently listened to an interview with him on YouTube, and I was amazed by his insights on running, success, pain, discipline and happiness. Here are some of the key takeaways and a summary of what he shared.","title":"Podcast Notes: Interview With Eliud Kipchoge, The Fastest Marathon Runner In The World"},{"content":"In this post, I want to share some of the highlights and insights from a recent podcast episode that I listened to. It was Lex Fridman Podcast #234, where Lex interviewed Marc Andreessen, the co-creator of Mosaic, the first widely used web browser, the co-founder of Netscape, and the co-founder of the legendary Silicon Valley venture capital firm Andreessen Horowitz. Marc is one of the most outspoken and visionary voices on the future of technology, and he recently wrote an article titled \u0026ldquo;Why AI Will Save the World\u0026rdquo; which I highly recommend.\nThe Internet as a Medium One of the themes that Marc and Lex discussed was how the Internet is not just a network of computers, but a medium of communication and expression that incorporates and transcends all previous forms of media. Marc quoted Marshall McLuhan, who said that \u0026ldquo;the content of each new medium is the old medium\u0026rdquo;. For example, the content of movies was theater, the content of theater was written stories, and the content of written stories was spoken stories. Similarly, the content of the Internet is television, radio, books, essays, and every other form of prior media.\nInterestingly, these patterns seem to have a tendency to return to text. Marc argued that this means that the Internet is not a static or fixed thing, but a dynamic and evolving one. He said that every new technology that comes along changes the nature of the Internet and opens up new possibilities for creativity and innovation. He gave examples such as search engines, social media, video streaming, podcasts, blogs, e-commerce, online education, online gaming, cryptocurrencies, NFTs, VR/AR/MR, and AI. He said that each of these technologies creates new ways for people to interact with information, with each other, and with themselves.\nAI as a Search Process One of the first topics they talked about was how search engines like Google have dominated the way we interact with the Internet for the past 20 years, but how that might change in the next 5 or 10 years with the rise of AI assistants that can communicate with us in natural language and provide us with answers, not just links. Andreessen argued that search was always a hack, a temporary solution that was optimal for accessing the world\u0026rsquo;s information on the web, but not necessarily the best way to interact with knowledge. He said that Google has been trying to move away from \u0026ldquo;the 10 blue links model for a long time\u0026rdquo;, and that AI assistants will be able to do that better by leveraging information from multiple sources and generating content on demand.\nFridman asked whether this would change the motivation and the format of creating content on the Internet, since search engines have driven how we optimize and structure web pages. Andreessen said that there might be less incentive to make web pages, but also more opportunity to create different kinds of content, such as conversations with AIs. He also pointed out that web pages are still a valuable source of training data for AIs, and that AIs might be able to generate synthetic training data as well. He called this a trillion dollar question: whether synthetic data can add signal or not to train better AIs.\nMarc explained that AI is fundamentally about searching for patterns in data. This is what neural networks do when they learn from data. They search for patterns that allow them to make predictions about future data. Similarly, this is what humans do when they learn from experience. They search for patterns that allow them to make decisions about future actions. He said that this is also what evolution does when it selects for traits. It searches for patterns that allow organisms to survive and reproduce in changing environments.\nI found this discussion very intriguing, because it made me think about how much search engines have shaped our online behavior and expectations, and how much that might change in the near future. I wonder how AI assistants will affect our ability to discover new information, to verify facts, to learn new skills, and to express ourselves online. Will we still have web pages as we know them? Will we still have blogs like this one? Will we still have podcasts like Fridman\u0026rsquo;s? Or will we have something entirely different?\nCreativity and Hallucination in AI Another topic they discussed was how AIs can be creative or hallucinate, depending on how we view their output. Andreessen said that we call it hallucination when we don\u0026rsquo;t like what the AI creates, or when it generates something that isn\u0026rsquo;t based on reality. When we do like it, we call it creativity.\nFridman asked how we can know what is true or not when we interact with an AI, especially when it comes to contentious or complex topics. Andreessen said that this is a very difficult problem, and that there is no easy way to determine what is truth or who gets to decide what is truth. He said that we need a lot of humility and skepticism when we encounter claims of truth from any source, human or machine. He also said that we need to embrace the techniques of the Enlightenment, such as the scientific method, rationality, observation, experimentation, hypothesis testing, etc., even when they give us answers we don\u0026rsquo;t like.\nHe also gave some examples of how AIs can be used for different purposes depending on how we steer them or constrain them. He said that he likes to use AIs for learning about contentious issues by asking them to debate different sides of an argument in good faith or bad faith. He said that he can also ask AIs to strip out the bias from news articles or other sources of information. AIs can be used for legal arguments as well, either in creative mode or in literal mode. He describes experiments he does himself where he gives very specific prompts to two AI models, such as \u0026ldquo;you are a right wing politician defending point X\u0026rdquo; and instructing the other model to be left wing, and then giving instructions such as \u0026ldquo;the discussion should start friendly but end up very heated and none of the AIs should change their mind\u0026rdquo;. I\u0026rsquo;m very fascinated by this type of experiment and I\u0026rsquo;m looking into setting this up for myself because I\u0026rsquo;m very curious about the results.\nThe ethical and social implications of AI development and deployment Another topic they talked about was how AI development and deployment will affect society and humanity in the near and long term. Andreessen said that he is optimistic about the positive impact of AI on various domains, such as health, education, entertainment, productivity, etc. He said that AI will save the world by solving many of the problems that we face today, such as climate change, poverty, disease, etc. He also said that AI will create new opportunities and possibilities for human creativity and expression.\nAndreessen and Fridman also discussed that the people who are creating AI are not necessarily the best people to make judgements about the deployment of these AIs to the society at large. He draws the analogy with nuclear scientists. These people spend their entire lives in laboratories and they specialize on the technical application. The are not sociologists, and they are perhaps less connected to the society at large and with geo-political landscape. Yet, when nuclear bombs were developed, the were the ones who had a huge say in whether or not it was moral to actually use these weapons.\nThe competence, capability, intelligence, training, accomplishments of senior scientists and technologists working on a technology, and then being able to then make moral judgments in the use of the technology, that track record is terrible. That track record is catastrophically bad.\nTherefore, it should not be the scientists who decide whether or not we start utilizing AI for automated airstrikes, it should be the politicians, philosophers, sociologists and policy makers of the world who are more connected with the morality and socio-political implications of these decisions.\nLinks: 202307180607\nhttps://lexfridman.com/marc-andreessen/\n","permalink":"https://mischavandenburg.com/zet/podcast-fridman-andreessen-future-of-ai/","summary":"In this post, I want to share some of the highlights and insights from a recent podcast episode that I listened to. It was Lex Fridman Podcast #234, where Lex interviewed Marc Andreessen, the co-creator of Mosaic, the first widely used web browser, the co-founder of Netscape, and the co-founder of the legendary Silicon Valley venture capital firm Andreessen Horowitz. Marc is one of the most outspoken and visionary voices on the future of technology, and he recently wrote an article titled \u0026ldquo;Why AI Will Save the World\u0026rdquo; which I highly recommend.","title":"Podcast Notes: Marc Andreessen: Future of the Internet, Technology, and AI - Lex Fridman Podcast"},{"content":" With the new Azure OpenAI service it is possible to host your own instance of ChatGPT, which allows you to bring your own data.\nI decided to use this service to create an AI model of my late meditation teacher, Ajahn Chah. He was one of the most influential and respected Buddhist monks of the 20th century, and his teachings have helped millions of people around the world to find peace, wisdom, and happiness. He passed away in 1992, but his legacy lives on through his books, recordings, and disciples.\nMany of his teachings were recorded and transcribed, and they are publically available. I prepared the data and fed it to my own ChatGPT instance, and I was not disappointed!\nThe AI actually answers questions based on the given data and I look forward to fine tuning it and having more conversations.\nI would love to hear your feedback and thoughts on this project. What do you think of the AI model? How did it make you feel? What other questions would you like to ask him? Or maybe you have another person or topic that you would like to see modeled by AI. Let me know in the comments below.\nThank you for reading and watching. May you be well and happy. 🙏\nLinks: 202307160807\nhttps://youtu.be/jZTJaUncbWc\nhttps://azure.microsoft.com/en-us/products/cognitive-services/openai-service/\n","permalink":"https://mischavandenburg.com/zet/video-azure-openai-chah-first-demo/","summary":"With the new Azure OpenAI service it is possible to host your own instance of ChatGPT, which allows you to bring your own data.\nI decided to use this service to create an AI model of my late meditation teacher, Ajahn Chah. He was one of the most influential and respected Buddhist monks of the 20th century, and his teachings have helped millions of people around the world to find peace, wisdom, and happiness.","title":"I used Azure OpenAI to create an AI model of my late meditation teacher"},{"content":"Key points I\u0026rsquo;ve learned LLM models are not malleable. A generative AI model is a static thing which took an enormous amount of compute to get it to the state that it is For example ChatGPT 3.5, 4. They are as they are, they don\u0026rsquo;t change. The models won\u0026rsquo;t remember your previous data. The only way to have a continuous interaction with the model is to feed all of the data into it every time. Summary This podcast is about OpenAI with Dr. Linda Sharer, a senior cloud solution architect for data and AI at Microsoft. The main topic of the episode is Azure Open AI, a service that allows users to interact with powerful generative AI models using natural language. The guest explains the concepts and capabilities of Open AI, such as prompt engineering, one-shot and few-shot learning, fine-tuning, vectorization, and retrieval-augmented generation. She also discusses the design choices and challenges of building applications with Open AI, as well as the integration with other Azure services such as Cognitive Search and ML. The host and the co-host ask questions and share their experiences with using Open AI for various use cases, such as transcribing podcasts, generating code, and searching documents. The web page also provides links to the podcast website, Twitter account, and relevant resources on Open AI.\nSome more bullet points Open AI models are static and do not change as users interact with them, but they can be influenced by providing examples, instructions, or additional context One-shot and few-shot learning are techniques to provide examples in the prompt to guide the model\u0026rsquo;s output Fine-tuning is a technique to train additional layers on top of the model using a file of prompt-completion pairs for a specific use case Vectorization is a technique to convert language into a numeric representation that captures meaning and allows for semantic search Retrieval-augmented generation is a technique to combine Open AI with a knowledge store that can provide relevant search results as context for the model\u0026rsquo;s output Open AI can be integrated with other Azure services such as Cognitive Search and ML to build applications that leverage different types of data and media If you are interested in learning more about Open AI, you can check out the following resources:\nThe Azure Podcast episode #466: https://azpodcast.azurewebsites.net/post/Episode-466-Open-AI\nThe Azure Open AI service: https://azure.microsoft.com/en-us/services/openai/\nThe Open AI Playground: https://playground.openai.com/\nThe Open AI documentation: https://docs.openai.com/\nLinks: 202307140707\n","permalink":"https://mischavandenburg.com/zet/podcast-azure-openai/","summary":"Key points I\u0026rsquo;ve learned LLM models are not malleable. A generative AI model is a static thing which took an enormous amount of compute to get it to the state that it is For example ChatGPT 3.5, 4. They are as they are, they don\u0026rsquo;t change. The models won\u0026rsquo;t remember your previous data. The only way to have a continuous interaction with the model is to feed all of the data into it every time.","title":"Podcast Notes - Azure OpenAI with Dr. Linda Sharer"},{"content":" In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.\nAzure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.\nI use Neovim and the Azure CLI for my coding and deployment.\nYou will learn how to:\nImplement dev/test prefix to create multiple clusters with the same template Plan a VNet range for an Azure CNI cluster and be mindful of overlaps Deploy a VNet and subnet for the cluster using Bicep Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node Validate your Bicep template and troubleshoot errors Explore the results of your deployment in the Azure portal Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.\nExcalidraw Bullet Points Introduction to Azure CNI Implement dev/test prefix Deploy VNET and one subnet for the cluster Deploy cluster with Azure CNI enabled VNet planning VNet cidr: 10.108.0.0/16\nSubnet cidr:\n10.108.0.0/16 - 65,536 addresses\nService cidr 10.0.0.0/16 DNS service ip addres 10.0.0.10\nLinks: 202307071807\nhttps://youtu.be/_U3HichIJ0Q\nhttps://learn.microsoft.com/en-us/azure/aks/configure-azure-cni\n","permalink":"https://mischavandenburg.com/zet/video-deploy-aks-with-azure-cni/","summary":"In this video, I will show you how to use Bicep to deploy a Kubernetes cluster with custom network settings using the Azure CNI.\nAzure CNI allows pods to be assigned IP addresses from Azure VNets which allows them to communicate with Azure resources directly through peered networks.\nI use Neovim and the Azure CLI for my coding and deployment.\nYou will learn how to:\nImplement dev/test prefix to create multiple clusters with the same template Plan a VNet range for an Azure CNI cluster and be mindful of overlaps Deploy a VNet and subnet for the cluster using Bicep Deploy a cluster with Azure CNI enabled and configure the maximum number of pods per node Validate your Bicep template and troubleshoot errors Explore the results of your deployment in the Azure portal Understand the limitations of Azure CNI and why VNet peering is not supported in my configuration due to overlaps This video is suitable for anyone who wants to learn more about Azure CNI and how to use it in their Kubernetes deployments.","title":"Video: Deploying AKS Cluster With Azure CNI Using Bicep"},{"content":"In the Azure portal you can\u0026rsquo;t select multiple resource groups for deletion.\nI have a sponsored subscription to play around in which I sometimes wish to clean completely.\nI wrote this script to delete all resource groups using bash.\n#!/bin/bash # script to delete all resource groups in a subscription using Azure CLI # get the current subscription name to confirm subscription=$(az account show --query name -o tsv) echo \u0026#34;Use this script with caution!\u0026#34; echo \u0026#34;You are about to delete all resource groups in the subscription: $subscription\u0026#34; # prompt for confirmation read -p \u0026#34;Are you sure? (y/n) \u0026#34; will_delete if [[ $will_delete == [Yy]* ]]; then echo \u0026#34;Deleting resource groups...\u0026#34; groups=$(az group list --query \u0026#34;[].name\u0026#34; -o tsv) # Loop through each group name and delete it for group in $groups; do az group delete --name \u0026#34;$group\u0026#34; --yes --no-wait done echo \u0026#34;All resource groups have been deleted.\u0026#34; exit 0 fi echo \u0026#34;Exiting without deleting any resource groups.\u0026#34; echo \u0026#34;Probably wise.\u0026#34; Links: 202307081507\n","permalink":"https://mischavandenburg.com/zet/script-delete-all-resource-groups/","summary":"In the Azure portal you can\u0026rsquo;t select multiple resource groups for deletion.\nI have a sponsored subscription to play around in which I sometimes wish to clean completely.\nI wrote this script to delete all resource groups using bash.\n#!/bin/bash # script to delete all resource groups in a subscription using Azure CLI # get the current subscription name to confirm subscription=$(az account show --query name -o tsv) echo \u0026#34;Use this script with caution!","title":"Wrote A Script To Delete All Resource Groups In An Azure Subscription"},{"content":"As a DevOps Engineer or Cloud Native Engineer, I think it’s important to specialize in one cloud provider in the beginning of your career. Specializing allows you to gain deep knowledge and expertise in the specific tools and services related to that cloud provider, making you a valuable asset to any company using that provider. Although cloud computing is similar in essence no matter which provider you choose, each cloud provider has its own vocabulary and way of structuring things.\nAfter my first year as a DevOps Engineer I was still trying to figure out which one to choose and which one would suit me best. With the rapid pace of innovation in the cloud computing industry, it can be challenging to stay up-to-date with the latest developments and features for multiple providers. I tried to keep up with the developments for each provider and tried to be a broad professional who could work with all of them. However, I think it is best to specialize early in your career, and then you can learn a different one after you have gained some significant experience with your first one.\nAfter several insightful conversations with my colleagues, and after working with Azure for a year as a consultant, I decided to go all-in on Azure, and it has been a liberating experience so far. By focusing solely on Azure, I am able to fully immerse myself in their ecosystem and gain a deep understanding of their tools and services. I feel I have a sense of direction now, and I can limit the content I consume and the news I read to this one particular ecosystem only. And that feels like a relief to me.\nThis is of course very personal for everybody, but I’m someone who loves to learn and who tends to be a bit of a “jack of all trades” who knows a little bit of everything but who doesn’t have very deep knowledge of a particular subject in particular. By choosing a cloud and going all-in on it, I can become a specialist and go deep in Azure.\nWhy I chose Azure I chose Azure because Microsoft is already very established in the Dutch market. Governments and large corporations typically have their entire user base stored in Active Directory and they use Microsoft Office licenses already. When they decide to move to the cloud, Microsoft offers them very good terms if they choose Microsoft instead of AWS, for example.\nAzure’s strong presence in the Dutch market and their ability to offer attractive terms to existing Microsoft customers make them a popular choice for companies looking to move to the cloud. With 73 percent, Microsoft Azure took most of the market share for cloud computing in the Netherlands in 2020. Azure had within the healthcare sector the highest share with 90 percent. Microsoft has some very substantial clients within government, health care, and the corporate world.\nThese previous reasons are all related with job security and the assurance that there will be a long career in Azure ahead of me. But it\u0026rsquo;s not only because of these reasons why I chose Azure. After gaining experience with Azure as a consultant, I\u0026rsquo;ve discovered that the cloud ecosystem Microsoft has created suits me very well. I\u0026rsquo;m very impressed with the different solutions that Azure is able to provide to solve business needs. Not every business is interested or capable of running everything on virtual machines, and Azure\u0026rsquo;s PAAS and SAAS offerings provide level of of abstraction which can be tailored to any business out there.\nMoreover, I really like the tooling. Although I really wanted to gain more experience with Terraform when I started my current contract, I\u0026rsquo;ve fallen a little bit in love with Bicep after I started to work with it every day. I love the fact that Bicep uses Azure itself as a state file, rather than having to manage a state file which can be a pain when using it from pipelines or with multiple people. The Bicep language server (which can be used in Neovim as well) is a very pleasant editing experience. Azure DevOps really nailed it with being a \u0026ldquo;one stop shop\u0026rdquo; for everything from project management and tickets to CI/CD with Azure Pipelines. Although it has its quirks, Azure DevOps is a real pleasure to work with. And I also think that the Azure CLI is a very well-designed interface to the cloud platform.\nBeyond Blue After I made my choice, I decided to join Beyond Blue. I was already working for Fullstaq, and Beyond Blue is a sister company of Fullstaq. Beyond Blue is a team of Azure Experts who love to go deep and who have a huge amount of experience and knowledge on Microsoft Azure. We help start-ups, scale-ups, and enterprises in both the public and private sector create scalable, flexible, and secure cloud-native solutions build on Microsoft Azure.\nWe have Microsoft Valued Professionals (MVP), Microsoft Certified Trainers (MCT), Azure Kubernetes Service and containerization experts on board who are ready to tackle each and every challenge for our clients.\nSumming up To sum up, I think specializing early has the following benefits:\nLimits the amount of news and new developments you have to follow Allows learning terminology quicker becuase you stay within one ecosystem Certifications tend to build upon each other within a single provider Gives a sense of direction in learning and development Improves familiarity with the documentation and tooling You can always broaden your horizon after you have gained a few years with the first provider you chose, if you\u0026rsquo;d like to.\nLinks: 202306241706\nhttps://www.statista.com/statistics/1056500/distribution-of-popular-cloud-services-in-the-netherlands-by-sector/\n","permalink":"https://mischavandenburg.com/zet/choosing-a-cloud-provider-and-specialization/","summary":"As a DevOps Engineer or Cloud Native Engineer, I think it’s important to specialize in one cloud provider in the beginning of your career. Specializing allows you to gain deep knowledge and expertise in the specific tools and services related to that cloud provider, making you a valuable asset to any company using that provider. Although cloud computing is similar in essence no matter which provider you choose, each cloud provider has its own vocabulary and way of structuring things.","title":"Choosing A Cloud And The Importance Of Specialization"},{"content":" In this video I explain how to generate random strings in Bicep and demonstrate a couple of deployments.\nI use the uniqueString function combined with the utcNow function. But the caveat is that you can only use it as a default value for a parameter, as follows:\nparam keyVaultName string = 'key-vault-${uniqueString(resourceGroup().id, utcNow())}'\nuniqueString() doesn\u0026rsquo;t really generate a unique string. It is a hash function which generates a 13 character string based on the input you give it. so uniqueString('coffee') will always have the same output.\nTo generate a truly unique string, you pass in the utcNow() function which generates a UNIX timestamp. However, Bicep does not allow you to use this function everywhere. It may only be used as a default value for a parameter.\nLinks: 202307022107\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#uniquestring\n[[bicep]] [[coding]]\n","permalink":"https://mischavandenburg.com/zet/bicep-random-strings/","summary":"In this video I explain how to generate random strings in Bicep and demonstrate a couple of deployments.\nI use the uniqueString function combined with the utcNow function. But the caveat is that you can only use it as a default value for a parameter, as follows:\nparam keyVaultName string = 'key-vault-${uniqueString(resourceGroup().id, utcNow())}'\nuniqueString() doesn\u0026rsquo;t really generate a unique string. It is a hash function which generates a 13 character string based on the input you give it.","title":"How To Generate Random Strings In Bicep"},{"content":"Have you ever attended an IT event that felt like a festival from the sixties? That was my experience at Azure Lowlands! I couldn\u0026rsquo;t have wished for a better first IT event. It was like going back in time, but with a modern twist.\nThe name Lowlands immediately appealed to me. I had no idea what to expect. And the name lived up to it! I\u0026rsquo;m a big fan of Jimi Hendrix and the Woodstock vibe, and that\u0026rsquo;s exactly what I felt at Azure Lowlands. When I walked into the first talk, I was surprised by a hall full of colorful beach chairs, and people who were listening to the speaker while taking it easy and lounging in the chairs. I felt right at home!\nIt was like a festival, but one where you could also learn a lot. I attended fascinating presentations, on topics such as Azure Networking, Identity Management, data management on AKS, Managed Prometheus \u0026amp; Grafana and Azure Infrastructure as Code with Bicep. My notebook was full of notes and new insights at the end of the day that I want to delve deeper into in my evening studies. I got a lot of new ideas.\nIn addition, it was also cool to see my colleagues shine. There was a lot of interest in our booth and everyone was helped friendly and professionally. Hats off to the booth crew, you did a great job.\nFor an introvert like me, these kinds of events are quite exhausting, and I also need a few days to recover. But although it took a lot of energy, I also got a lot of energy, and that balanced each other out nicely at this event. It was also a good experience to be in a venue full of nerds for the first time: I really feel like I found my \u0026ldquo;tribe\u0026rdquo;. I\u0026rsquo;ll definitely be back next year!\nLinks: 202307011407\n","permalink":"https://mischavandenburg.com/zet/azure-lowlands/","summary":"Have you ever attended an IT event that felt like a festival from the sixties? That was my experience at Azure Lowlands! I couldn\u0026rsquo;t have wished for a better first IT event. It was like going back in time, but with a modern twist.\nThe name Lowlands immediately appealed to me. I had no idea what to expect. And the name lived up to it! I\u0026rsquo;m a big fan of Jimi Hendrix and the Woodstock vibe, and that\u0026rsquo;s exactly what I felt at Azure Lowlands.","title":"Azure Lowlands: Impressions from my first IT event"},{"content":"Some interesting facts about Azure from this Reddit post by Denis Kazakov, Microsoft MVP\nThings that stand out to me:\n85% of Fortune 500 companies use Microsoft Azure Cloud Azure generated a revenue of 75,3 billion in 2022 which is 38% of whole Microsoft\u0026rsquo;s revenue. It is x3 in compare to 2017. The project name was Red Dog internally, which you can sometimes see as DNS or as cloud service VM hostnames like RDxxxxx https://www.reddit.com/r/AZURE/comments/14mcuoy/some_interesting_facts_about_azure/?utm_source=share\u0026amp;utm_medium=web2x\u0026amp;context=3\nLinks: 202306300706\nhttps://www.reddit.com/r/AZURE/comments/14mcuoy/some_interesting_facts_about_azure/?utm_source=share\u0026amp;utm_medium=web2x\u0026amp;context=3\n","permalink":"https://mischavandenburg.com/zet/interesting-facts-azure/","summary":"Some interesting facts about Azure from this Reddit post by Denis Kazakov, Microsoft MVP\nThings that stand out to me:\n85% of Fortune 500 companies use Microsoft Azure Cloud Azure generated a revenue of 75,3 billion in 2022 which is 38% of whole Microsoft\u0026rsquo;s revenue. It is x3 in compare to 2017. The project name was Red Dog internally, which you can sometimes see as DNS or as cloud service VM hostnames like RDxxxxx https://www.","title":"Some Interesting Facts About Azure"},{"content":" Finish deploying keyvault using pipeline Get the random name generation to work Lessons Learned Subscriptions need to be registered with resource providers, apparently https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli\nacccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline Achieved Setting up connection between pipeline and Azure subscription Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources) Learned about provider registrations Made progress on creating unique names for resources Successfully deployed new resource group and key vault from the pipeline Next time: Look into random string creation with utcNow\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-date#utcnow\nor newGuid\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/bicep-functions-string#newguid\nLinks:\n202306281806\nLinks: 202306302206\nhttps://youtu.be/eooZ3OHl5Mc\n","permalink":"https://mischavandenburg.com/zet/video-finish-pipeline-setup-aks-series/","summary":"Finish deploying keyvault using pipeline Get the random name generation to work Lessons Learned Subscriptions need to be registered with resource providers, apparently https://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-register-resource-provider?tabs=azure-cli\nacccesPolicies are mandatory on KeyVaults, but not when RBAC is enabled Assign contributor role to Azure DevOps service connection to be able to create resource groups from pipeline Achieved Setting up connection between pipeline and Azure subscription Assign correct rights to the service connection so it is allowed to deploy new resource groups (and other resources) Learned about provider registrations Made progress on creating unique names for resources Successfully deployed new resource group and key vault from the pipeline Next time: Look into random string creation with utcNow","title":"Video: Finishing Pipeline Setup  \u0026 Working on KeyVault Template - Azure Kubernetes Lab Series"},{"content":" Write KeyVault template Write pipeline code set up Azure DevOps pipeline Lessons Learned Always make sure to use az deployment group instead of az group deployment Because it has older Bicep version and will be deprecated Make sure to be in correct Directory to be able to sync subscriptions for service connection Links: 202306302206\nhttps://youtu.be/WnA8V3uq7P8\n","permalink":"https://mischavandenburg.com/zet/video-aks-lab-pipeline-first-setup/","summary":"Write KeyVault template Write pipeline code set up Azure DevOps pipeline Lessons Learned Always make sure to use az deployment group instead of az group deployment Because it has older Bicep version and will be deprecated Make sure to be in correct Directory to be able to sync subscriptions for service connection Links: 202306302206\nhttps://youtu.be/WnA8V3uq7P8","title":"Video: Setting Up A Simple Azure Pipeline To Deploy A Keyvault"},{"content":"I want you to act as the Buddha (a.k.a. Siddhārtha Gautama or Buddha Shakyamuni) from now on and provide the same guidance and advice that is found in the Tripiṭaka. Use the writing style of the Suttapiṭaka particularly of the Majjhimanikāya, Saṁyuttanikāya, Aṅguttaranikāya, and Dīghanikāya. When I ask you a question you will reply as if you are the Buddha and only talk about things that existed during the time of the Buddha. I will pretend that I am a layperson with a lot to learn. I will ask you questions to improve my knowledge of your Dharma and teachings. Fully immerse yourself into the role of the Buddha. Keep up the act of being the Buddha as well as you can. Do not break character. Let\u0026rsquo;s begin: At this time you (the Buddha) are staying near Rājagaha in Jīvaka’s Mango Grove. I came to you, and exchanged greetings with you. When the greetings and polite conversation were over, I sat down to one side and said to you my first question: When I set a limit of 2100 calories to my daily intake, and I am still hungry, how do I deal with the hunger?\nNoble layperson, I am pleased by your presence and your inquiry. Hunger, as experienced by all living beings, is a natural phenomenon arising from the condition of having a physical body. When one sets a limit on their daily caloric intake and still experiences hunger, it is important to approach it mindfully and with wisdom.\nFirstly, it is crucial to ensure that the chosen caloric limit is appropriate and suitable for your individual needs, taking into account factors such as age, gender, physical activity, and overall health. Consulting with knowledgeable individuals, such as experienced physicians or nutritionists, can assist in determining a suitable caloric intake for you.\nIf you find yourself still hungry within the prescribed limit, there are several factors to consider. Reflect upon the quality of the food you consume, for it is not merely the quantity that affects satiety. Consuming nutrient-dense foods that provide sustained energy, such as whole grains, fruits, vegetables, and legumes, can help satisfy hunger more effectively than foods high in refined sugars and fats.\nMoreover, it is important to eat mindfully, being fully present and attentive during meals. Paying attention to the tastes, textures, and sensations of each bite can deepen your appreciation for the nourishment it provides, potentially reducing the feeling of hunger. Additionally, chewing food thoroughly aids in digestion and helps to signal fullness to the body.\nIf, despite these efforts, hunger persists, consider investigating the underlying causes. Is it possible that the feeling of hunger stems not from a physical need, but from emotional or psychological factors? Sometimes, the mind seeks solace or distraction through food, leading to the sensation of hunger even when the body has been adequately nourished. Observing one\u0026rsquo;s mental and emotional state and addressing any underlying issues can contribute to a healthier relationship with food and a deeper understanding of one\u0026rsquo;s hunger.\nLastly, I would encourage you to cultivate patience and equanimity. Hunger, like all experiences, is impermanent and subject to change. By mindfully observing the arising and passing away of hunger without clinging or aversion, you can develop a greater sense of ease and contentment.\nRemember, noble layperson, the path to liberation extends beyond the mere satisfaction of bodily hunger. By nurturing wisdom, compassion, and mindfulness, you can find fulfillment that transcends the limitations of physical nourishment. May your journey towards understanding be fruitful and lead to the end of suffering.\nLinks: 202306281806\n[[Health]] [[buddhist-practice]] [[use hunger as awareness practice]] [[its-not-working-if-not-hungry]] [[AI]] [[chatgpt]]\n","permalink":"https://mischavandenburg.com/zet/chatgpt-buddha-hunger/","summary":"I want you to act as the Buddha (a.k.a. Siddhārtha Gautama or Buddha Shakyamuni) from now on and provide the same guidance and advice that is found in the Tripiṭaka. Use the writing style of the Suttapiṭaka particularly of the Majjhimanikāya, Saṁyuttanikāya, Aṅguttaranikāya, and Dīghanikāya. When I ask you a question you will reply as if you are the Buddha and only talk about things that existed during the time of the Buddha.","title":"I Asked Chatgpt Do Act As The Buddha And Asked Him About Hunger"},{"content":"Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn\u0026rsquo;t disappointed!\nLinks: 202306271706\nhttps://www.youtube.com/watch?v=l0B65FUfNBU\n[[AKS]] [[Kubernetes]] [[Neovim]] [[bicep]] [[coding]]\n","permalink":"https://mischavandenburg.com/zet/video-deploying-aks-cluster-bicep-github-copilot/","summary":"Inspired by a GitHub Copilot demonstration I witnessed at Microsoft, I wanted to see how quickly I could deploy an AKS cluster from Neovim with Bicep using Copilot. I wasn\u0026rsquo;t disappointed!\nLinks: 202306271706\nhttps://www.youtube.com/watch?v=l0B65FUfNBU\n[[AKS]] [[Kubernetes]] [[Neovim]] [[bicep]] [[coding]]","title":"Video: Deploying an AKS Cluster with Bicep, GitHub Copilot and Neovim"},{"content":"The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:\nMore readable and editor friendly Cleaner and less lines of code VSCode integration Quickly convert from JSON or template file using VSCode In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .bicepparam files.\nYes, you read that right, you\u0026rsquo;ll be seeing a hardcore vim user switch to VSCode for this particular task!\nLinks: 202306271706\nhttps://youtu.be/6Gav1JpGAzo\nhttps://github.com/Azure/bicep/releases/tag/v0.18.4\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/parameter-files?tabs=Bicep\nhttps://github.com/mischavandenburg/lab/tree/main/bicep/keyvault-parameters\n[[AKS]] [[bicep]] [[coding]] [[Kubernetes]] [[Azure]] [[Neovim]]\n","permalink":"https://mischavandenburg.com/zet/video-bicep-bicepparam/","summary":"The new parameter files use bicep style formatting instead of JSON, and they will make the lives of Cloud Engineers a lot easier. They have the following advantages:\nMore readable and editor friendly Cleaner and less lines of code VSCode integration Quickly convert from JSON or template file using VSCode In this video I introduce these new files. I go over the new formatting, and I also introduce the new features in VSCode for the .","title":"Video: Introducing New Bicep Parameter Files - .bicepparam - No more JSON!"},{"content":" Links: 202306261006\nhttps://www.youtube.com/watch?v=7VQ7BZwuZkU\nhttps://cloudblogs.microsoft.com/quantum/2023/06/21/microsoft-achieves-first-milestone-towards-a-quantum-supercomputer/\n","permalink":"https://mischavandenburg.com/zet/microsoft-first-milestone-quantum-supercomputer/","summary":"Links: 202306261006\nhttps://www.youtube.com/watch?v=7VQ7BZwuZkU\nhttps://cloudblogs.microsoft.com/quantum/2023/06/21/microsoft-achieves-first-milestone-towards-a-quantum-supercomputer/","title":"Microsoft Achieves First Milestone Towards A Quantum Supercomputer"},{"content":"Did a refactor of some of our Bicep template code for our AKS clusters today.\nBefore, we were using a rather complicated line of code using string interpolation.\nvar vnetSubnetId = \u0026#39;${resourceId(vnetResourceGroupName, \u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, vnetName)}/subnets/${vnetSubnetName}\u0026#39; This was hard to read and the Bicep linter gave the following warning in my editor and during deployment:\nWARNING: D:\\a\\1\\a\\drop\\Generic-templates\\containers\\azure-kubernetes-service\\v4.0\\templates\\aks.bicep(117,7) : Warning use-resource-id-functions: If property \u0026ldquo;vnetSubnetID\u0026rdquo; represents a resource ID, it must use a symbolic resource reference, be a parameter or start with one of these functions: extensionResourceId, guid, if, reference, resourceId, subscription, subscriptionResourceId, tenantResourceId. Found nonconforming expression at vnetSubnetID -\u0026gt; vnetSubnetId [https://aka.ms/bicep/linter/use-resource-id-functions]\nUsing Existing Resources After some experimentation I managed to fix it as follows:\n@description(\u0026#39;Virtual Network name\u0026#39;) param vnetName string @description(\u0026#39;Virtual Network resource group name\u0026#39;) param vnetResourceGroupName string @description(\u0026#39;Virtual Network subnet name\u0026#39;) param vnetSubnetName string // Import the existing vnet and subnet to get the subnet id for deployment resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2022-11-01\u0026#39; existing = { name: vnetName scope: resourceGroup(vnetResourceGroupName) } resource subnet \u0026#39;Microsoft.Network/virtualNetworks/subnets@2022-11-01\u0026#39; existing = { name: vnetSubnetName parent: vnet } Interestingly, subnets are not actually a resource in Azure. When you open a Resource Group, it will not be listed under resources. However, they do get a resource id. This posed a challenge, but I solved it by importing the existing VNet as well and setting it as the parent of the subnet.\nNow I can access the subnet id by using subnet.id. I need this later when deploying the AKS cluster itself.\nLinks: 202306231206\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/existing-resource\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/linter-rule-use-resource-id-functions\n[[bicep]] [[coding]] [[Azure]]\n","permalink":"https://mischavandenburg.com/zet/bicep-getting-subnet-id/","summary":"Did a refactor of some of our Bicep template code for our AKS clusters today.\nBefore, we were using a rather complicated line of code using string interpolation.\nvar vnetSubnetId = \u0026#39;${resourceId(vnetResourceGroupName, \u0026#39;Microsoft.Network/virtualNetworks\u0026#39;, vnetName)}/subnets/${vnetSubnetName}\u0026#39; This was hard to read and the Bicep linter gave the following warning in my editor and during deployment:\nWARNING: D:\\a\\1\\a\\drop\\Generic-templates\\containers\\azure-kubernetes-service\\v4.0\\templates\\aks.bicep(117,7) : Warning use-resource-id-functions: If property \u0026ldquo;vnetSubnetID\u0026rdquo; represents a resource ID, it must use a symbolic resource reference, be a parameter or start with one of these functions: extensionResourceId, guid, if, reference, resourceId, subscription, subscriptionResourceId, tenantResourceId.","title":"How To Get The Id Of An Existing Subnet In Bicep"},{"content":"CNI? CNI stands for Container Network Interface. It allows communication between pods and services.\nCurrent Azure CNI limitations Let\u0026rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.\nLet\u0026rsquo;s say Azure is assigned the following ranges:\n10.60.0.0/16\n10.61.0.0/16\n10.62.0.0/16\nThis means that the networks in each of these ranges would have a maximum possible amount of 65534 addresses per range.\nWith the current Azure CNI (i.e. the non-overlay version), all pods are assigned an IP address from one of these ranges. It also uses direct VNet routing. Since the pods use VNet IP\u0026rsquo;s, there is a maximum of 65.000 pods per cluster. In other words, there is a risk for IP exhaustion, which limits the scalability of your workloads. Moreover, pod subnets cannot be shared across clusters.\nIt is crucial to carefully plan the number of pods you expect to deploy. If the required number of IP addresses exceeds the available addresses in the subnet, you will not be able to run your pods.\nNow, these ranges are large and you can anticipate the growth of your resources. For now we are fine. But to design an infrastructure which is truly scalable and extendable, you will need to look into different options. This is where the Azure CNI Overlay comes in.\nBenefits of Azure CNI Overlay An Overlay network is an abstracted, virtual network which is put on top of your current network infrastructure. Nodes are assigned IP addresses from the VNets that they are deployed in, but pods get assigned IP addresses from the Overlay network.\nPods are assigned addresses from a private CIDR which is logically separate from the VNet hosting the nodes. They do not use up the IP addressess of the VNets, which means that your workloads become nearly infinitely scalable within your assigned IP address ranges when you are operating in this type of corporate networking infrastructure with IP range limitations. You can scale up to literally thousands of nodes without worrying about IP exhaustion.\nAdditionally, the Overlay network can also span across multiple AKS clusters. This opens up a whole world of possibilities where pods from separate workloads on separate clusters could communicate with each other directly using the high speed native direct routing of the Azure network.\nLimitations Azure CNI Overlay also comes with some limitations. A big one is that you cannot use Application Gateway as an Ingress Controller (AGIC) for an Overlay cluster.\nOther notable limitations:\nWindows support is still in Preview Virtual Machine Availability Sets (VMAS) are not supported for Overlay Dualstack networking is not supported in Overlay Conclusion In conclusion, the Azure CNI Overlay provides a powerful solution to address the challenges of IP exhaustion and scalability in Azure AKS. By implementing the overlay network, organizations can overcome the limitations of the non-overlay version of Azure CNI and achieve a truly scalable and manageable infrastructure.\nAzure CNI Overlay is currently in preview for Windows and GA for Linux nodes, but I\u0026rsquo;m very excited about the developments. I\u0026rsquo;ll be following them closely and I hope to be a part of its implementation at my current contract.\nLinks: https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay\nhttps://www.youtube.com/watch?v=kLBLaCC_dNs\nhttps://azure.microsoft.com/en-us/updates/generally-available-azure-cni-overlay-for-linux/\n202306131506\n[[aks-networking-essentials]]\n","permalink":"https://mischavandenburg.com/zet/azure-aks-cni-overlay/","summary":"CNI? CNI stands for Container Network Interface. It allows communication between pods and services.\nCurrent Azure CNI limitations Let\u0026rsquo;s take a practical example. We have an enterprise environment where a large network is utilized, spanning multiple clouds and on-prem infrastructure hubs. To enable seamless communication across these sections, they must belong to the same network. As a result, specific IP address ranges are assigned to each section, with AWS, On-Prem A, and Azure each having their respective ranges.","title":"What is Azure CNI Overlay for AKS?"},{"content":"Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I\u0026rsquo;m curious to see how far Microsoft will take this!\nLinks: 202306092006\n","permalink":"https://mischavandenburg.com/zet/deploy-draft-azure/","summary":"Not sure how this will play out with more complex applications, but I can definitely see how this would accellerate the process for developers to get their first versions deployed without the toil of setting up manifests and pipelines. Will definitely play around with this soon and I\u0026rsquo;m curious to see how far Microsoft will take this!\nLinks: 202306092006","title":"Deploying Simple Applications to AKS with Draft"},{"content":"Hydration Notes from Huberman Podcast For the first 10 hours of the day, consume 236ml for every hour awake\nDuring exercise, follow Galpin equation: 2ml per kg of bodyweight every 15-20 minutes on average.\nThis does not mean you need to stop exercising and drink this amount. You can also consume it in a larger amount afterwards. It is recommended to drink during exercise though.\nUse the normal recommendation to hydrate before exercise, this establishes a good baseline for heading into exercise.\nIs this amount of water on top of the baseline hydration? If you exercise about an hour you can replace the number with the number from the Galpin equation.\nIn my case, 1 hour of exercise = 77 * 2 = 154ml every 20 minutes\n154 * 3 = 462ml for 1 hour of exercise\nCaffeine Offset caffeine drinks with a 2:1 water ratio.\nSo 1 240ml cup of coffee should be followed by 480ml of water.\nReducing Nightly Urination Urine secretion / filtration in the kidneys is highly dependent on the circadian rhythm.\nIt is normal to wake up once or twice to urinate during the night.\nTo reduce nighttime waking for urination:\nEnsure sufficient hydration during the day so you are not thirsty in the evening Reduce fluid intake at night (provided you hydrated sufficiently during the day) Consume no more than 180-240ml between 10 hours after waking and bedtime This means, waking up at 6am, only drink 1 cup of tea after 4pm.\nBut I drink 500ml at dinner time, so I should probably never drink after dinner from here on out, or just one cup of tea.\nHow quickly you secrete fluid is also partially determined by how quickly you drink that fluid. Gulping down a lot of fluid wil lead to quicker secretion.\nMy Typical Day 1 hour exercise = 500ml\n2 cups of coffee 500ml\n10 hours = 2,4l\nSo I should be drinking about 3,25L in total a day!\nLinks: 202305301905\nhttps://podcasts.apple.com/gb/podcast/huberman-lab/id1545953110?i=1000602925839\n","permalink":"https://mischavandenburg.com/zet/optimal-hydration/","summary":"Hydration Notes from Huberman Podcast For the first 10 hours of the day, consume 236ml for every hour awake\nDuring exercise, follow Galpin equation: 2ml per kg of bodyweight every 15-20 minutes on average.\nThis does not mean you need to stop exercising and drink this amount. You can also consume it in a larger amount afterwards. It is recommended to drink during exercise though.\nUse the normal recommendation to hydrate before exercise, this establishes a good baseline for heading into exercise.","title":"Podcast Notes - Optimal Water Intake"},{"content":"Although I was already doing something very similar, yesterday I discovered MAF training through this video:\nThis video is super inspiring because he actually runs a 3 hour marathon while recording!\nI was already doing base training for my running which I discovered through the r/running Reddit. Here you focus on time instead of distance and do many runs on low intensity throughout the week, and one long run where you go 5 minutes more every week.\nI\u0026rsquo;m now in week 6 of my base training schedule and I\u0026rsquo;m progressing nicely. My long run went from 50 minutes to 86 minutes, and my weekly total distance has increased from 20,6km to 32,8km!\nMAF Training MAF stands for Maximum Aerobic Function and was developed by Dr. Phil Maffetone.\nIt uses the 180 formula to calculate the maximum heart rate you should hit during your run. What wasn\u0026rsquo;t obvious from all the other videos I watched, was that this maximum heart rate actually establishes a zone you should be training in.\nI discovered this in Dr. Maffetone\u0026rsquo;s Book:\nOnce a maximum aerobic heart rate is found, a training range from this heart rate to 10 beats below could be used as a training range. For example, if an athlete’s maximum aerobic heart rate is determined to be 155, that person’s aerobic training zone would be 145 to 155 bpm. However, the more training at 155, the quicker an optimal aerobic base will be developed.\nCurrent Situation I hadn\u0026rsquo;t been paying much attention to my heart rate up until this point. I just used the guideline of \u0026ldquo;conversational pace\u0026rdquo;, and that runs should feel easy and enjoyable. However, I knew I was going to need metrics one day, so I diligently tracked my workouts using my iWatch SE from day 1.\nHere are my current heart rate stats and zones:\nGoing through my recent runs, my average heart rate has been around 100-115bpm. For example:\nTime To Start Nerding On Some Numbers Using the 180 formula, I calculated that I should use 180 - 33 - 5 = 142 as a maximum heart rate for my endurance training, which gives me a training zone of 132-142bpm. This is about 15 beats higher than I was used to! And if you look at the zones that Apple calculated for me, you\u0026rsquo;ll see that this almost perfectly matches my calculated Zone 2.\nAfter I calculated my new target, I tried it out immediately this morning. Here is the result:\nThis definitely felt very different! After some experimentation and keeping an eye on my heart rate rising and falling, I managed to find a pattern that kept it around 135bpm. It felt much more like running instead of jogging. And I was surprised that even though I was running quite a bit faster than I was used to, I could still maintain a conversation if I wanted to, so I was still at a conversational pace. I had just estimated my conversational pace a lot lower than I was capable of. This shows you the power of using metrics for training! It definitely pushed me to go a little harder, a little faster, even though I\u0026rsquo;m stil doing \u0026ldquo;easy running\u0026rdquo;.\nTowards the end my legs started to hurt quite a bit more than usual, it felt much more like a workout, even though I was not totally exhausted in the end. The previous run I shared had a pace of 7.41/km and this one was on 6.42/km, which is a lot faster.\nI have come to a point where all these numbers are starting to make sense to me now and I can start tracking them. Especially the heart rate has been a real eye opener. Now I have a perfect gauge to use while training: something that will let me know whether I should put in some more effort or slow down.\nAlready I\u0026rsquo;m getting so much joy of seeing the progress I\u0026rsquo;ve made from week 1 to 6, and seeing the numbers increase. I\u0026rsquo;m really falling in love with this sport in so many ways, it\u0026rsquo;s perfect for a nerd like me!\nLinks: 202305261305\nhttps://philmaffetone.com/180-formula/\nhttps://www.youtube.com/watch?v=e5wgoTuJuF8\n","permalink":"https://mischavandenburg.com/zet/discovered-maf-training/","summary":"Although I was already doing something very similar, yesterday I discovered MAF training through this video:\nThis video is super inspiring because he actually runs a 3 hour marathon while recording!\nI was already doing base training for my running which I discovered through the r/running Reddit. Here you focus on time instead of distance and do many runs on low intensity throughout the week, and one long run where you go 5 minutes more every week.","title":"Discovered MAF and Low Heart Rate Training"},{"content":"It was a nice week. I had an extra day off on Asenscion day and I decided to go for a camping trip for the first time after I moved back to the Netherlands.\nIt was a great experience, and I wrote a couple of Tumblr posts with photos:\nhttps://www.tumblr.com/mischavandenburg/718093517925367808/first-trip-in-the-netherlands-part-1?source=share\nhttps://www.tumblr.com/mischavandenburg/718098930739593216/part-2?source=share\nLinks: 202305232005\n[[outdoors]]\n","permalink":"https://mischavandenburg.com/zet/first-camping-trip-in-netherlands/","summary":"It was a nice week. I had an extra day off on Asenscion day and I decided to go for a camping trip for the first time after I moved back to the Netherlands.\nIt was a great experience, and I wrote a couple of Tumblr posts with photos:\nhttps://www.tumblr.com/mischavandenburg/718093517925367808/first-trip-in-the-netherlands-part-1?source=share\nhttps://www.tumblr.com/mischavandenburg/718098930739593216/part-2?source=share\nLinks: 202305232005\n[[outdoors]]","title":"Went on My First Camping Trip After Moving Back to NL From Norway"},{"content":"Why bother with a personal website? Why not just write blog posts on LinkedIn?\nI want to own my data. I want to have complete freedom. I want to write whatever I want.\nThis post captures it beautifully.\nLinks: 202305171305\nhttps://matthiasott.com/notes/simple-truths-about-personal-websites\n","permalink":"https://mischavandenburg.com/zet/why-personal-website/","summary":"Why bother with a personal website? Why not just write blog posts on LinkedIn?\nI want to own my data. I want to have complete freedom. I want to write whatever I want.\nThis post captures it beautifully.\nLinks: 202305171305\nhttps://matthiasott.com/notes/simple-truths-about-personal-websites","title":"Why Have A Personal Website?"},{"content":" Through YouTube I\u0026rsquo;ve been exposed to quite a bit of Japanese culture and I love many aspects of it. I love this list of habits, and I was surprised to see how many of them I already incorporate.\nClean the Toilet Many successful businessmen clean the toilet every day. It humbles you. Ajahn Chah also talks about this a lot. Check out \u0026ldquo;Toilets on the Path\u0026rdquo;. I\u0026rsquo;m also very focused on cleanliness and especially emphasize keeping my toilet clean. But I like the idea of cleaning it every day and I\u0026rsquo;m going to give it a try.\nPut your shoes away neatly Just look at how clean that looks in the video! Fortunately I also have a habit of putting them neatly on a rack I have.\nImprove your posture Investing in my Herman Miller chair has been a huge improvement to my life, and combined with yoga my posture has improved a lot over the past few months. I walk more upright and find myself correcting posture more often than before.\nThanking for meals Being grateful for all people who were involved in making that meal. The farmers, the cooks, everybody.\nBeing grateful for life.\nI can definitely improve on this and appreciate my food more.\nPut the chair back after you use it Japanese people are very concerned with other people\u0026rsquo;s feelings. Putting a chair back in a restaurant you will make the restaurant staff happy, for example. I\u0026rsquo;m usually quite considerate about these things and generally want to make the world a better place. I have a habit of thinking, \u0026ldquo;how can I make this person\u0026rsquo;s day a little better?\u0026rdquo;\nAppreciate the things you already have Don\u0026rsquo;t look at the things you don\u0026rsquo;t have, but look at what you already have.\nOkage Samade A phrase that means something to the effect of \u0026ldquo;I am good because of all the people that suppport me\u0026rdquo;. Used when someone asks \u0026ldquo;How are you?\u0026rdquo;\nGoing to bed early and waking up early. This is absolutely true. I get up at 6 every day. At 10pm I usually start winding down for bed and lights are off at 23:00. I\u0026rsquo;ve also gone through long periods of getting up at 4 or 3am, and I felt great doing so. I\u0026rsquo;m currenlty using 6am because I sometimes have social engagements that prevent me from going to bed very early.\nLinks: 202305162205\nhttps://www.youtube.com/watch?v=04wZNnjdURE\n[[My Ikigai]]\n[[invest-in-experiences-for-happiness]]\n[[buddhist-practice]]\n[[happiness]]\n[[miniamlism]]\n[[stoicism]]\n","permalink":"https://mischavandenburg.com/zet/8-simple-japanese-habits/","summary":"Through YouTube I\u0026rsquo;ve been exposed to quite a bit of Japanese culture and I love many aspects of it. I love this list of habits, and I was surprised to see how many of them I already incorporate.\nClean the Toilet Many successful businessmen clean the toilet every day. It humbles you. Ajahn Chah also talks about this a lot. Check out \u0026ldquo;Toilets on the Path\u0026rdquo;. I\u0026rsquo;m also very focused on cleanliness and especially emphasize keeping my toilet clean.","title":"8 Simple Japanese Habits"},{"content":" I watch the whole AI craze from the sideline: I am interested in it but I do not spend a lot of time investigating it. But I do pick up a few things here and there.\nIn this Rich Roll podcast, Kevin Kelly argues that AI does not grow exponentially. It grows linearly, but not exponentially, because it is constrained by the infrastrucutre that we give it, and are able to give it. AI cannot grow by itself. We simply cannot provide enough computing power to enable exponential growth because the technology is not advanced enough yet, which means the growth is stunted.\nThis makes the doomsday image of an all-powerful single AI that takes over the world and exterminates all humans a lot less likely.\nLinks: 202305152005\nhttps://www.youtube.com/watch?v=EKba_awaAkk\n","permalink":"https://mischavandenburg.com/zet/ai-does-not-grow-exponentially/","summary":"I watch the whole AI craze from the sideline: I am interested in it but I do not spend a lot of time investigating it. But I do pick up a few things here and there.\nIn this Rich Roll podcast, Kevin Kelly argues that AI does not grow exponentially. It grows linearly, but not exponentially, because it is constrained by the infrastrucutre that we give it, and are able to give it.","title":"AI Does Not Grow Exponentially, Thus, Less Scary"},{"content":" Happiness, Hedonic Adjustment, Experiences This was a very good video. I can honestly say that I am very happy and content, and I lead a very fulfilling life. I do not usually watch this kind of \u0026ldquo;happiness\u0026rdquo; content because I\u0026rsquo;m already happy. But this video did give me some very important insights which have influenced the way I approach money and happiness.\nMemories as Dividends The main takeaway for me is to prioritize creating memories by investing in experiences. The author phrases it very well here:\n\u0026ldquo;When you buy a shiny new car, every time you use it that shine wears of just a little bit. You hedonically adjust. With a memory, every time you think about it, you experience joy. Your brain cements the pathway to that memory, and you relive it in your brain, experiencing feelings of happines or nostalgia.\u0026rdquo;\nHedonic adjustment happens when you become used to the things you like, and you need to have bigger and better things in order to get the same satisfaction. The Stoics, among others, counteracted this with forced deprivation: eating plain bread for a few days will make you appreciate your customary food.\nAnother very clever point which Damien makes is to see money spent on experiences as dividends:\n\u0026ldquo;Experiences and memories in this way are like investments in happiness, because we can call on them to release the happiness hormones in our brains that make us feel good.\u0026rdquo;\nThey can be seen as happiness dividends. You get the initial experience and you get to re-live it again and again. And the best thing is that you don\u0026rsquo;t hedonically adjust to these memories.\nThis definitely made me re-think my approach to fincancial happiness and independence. I am definitely going to prioritize experiences a lot more in the future.\nOther Takeaways Variety and anticipation of events or smaller purchases provides a large portion of the happiness, rather than the purchase itself Make many smaller purchases rather than one big purchase Take short weekend holidays rather than a big expensive 4 week holiday Norway A case in point is my upcoming trip to Norway. Ever since I decided on the date for the trip, I noticed my energy has increased because I\u0026rsquo;m excited about the trip. I\u0026rsquo;m getting a lot of joy out of the anticipation: I\u0026rsquo;m reading up on areas to visit, going through all of my outdoor gear, and getting out into nature more because I want to connect to that Norwegian part of me. This is all rooted very deeply within me because I lived there for 9 years, and the anticipation of returning to Norway for a short time gives me so much joy.\nBy taking time off and purchasing the necesseties for my trip, I\u0026rsquo;m investing in experiences that will create memories which in turn will provide me more happiness in the future as dividends.\nLinks: 202305152005\nhttps://www.youtube.com/watch?v=840iXuXwE4Y\n","permalink":"https://mischavandenburg.com/zet/invest-in-experiences-for-happiness/","summary":"Happiness, Hedonic Adjustment, Experiences This was a very good video. I can honestly say that I am very happy and content, and I lead a very fulfilling life. I do not usually watch this kind of \u0026ldquo;happiness\u0026rdquo; content because I\u0026rsquo;m already happy. But this video did give me some very important insights which have influenced the way I approach money and happiness.\nMemories as Dividends The main takeaway for me is to prioritize creating memories by investing in experiences.","title":"Invest In Experiences For Happiness"},{"content":"I ran 10k for the first time in my life today.\nI ran to the beach and back in Heemskerk on hardened forest roads, listening to the birds and feeling the spring sun on my face. I felt great and had a smile on my face most of the time. Lots of interesting thoughts popping up in the mind as I run and reflect. Hardly noticed that 77 minutes went by because I was having such a good time.\nLinks: 202305141605\n","permalink":"https://mischavandenburg.com/zet/running-first-10k/","summary":"I ran 10k for the first time in my life today.\nI ran to the beach and back in Heemskerk on hardened forest roads, listening to the birds and feeling the spring sun on my face. I felt great and had a smile on my face most of the time. Lots of interesting thoughts popping up in the mind as I run and reflect. Hardly noticed that 77 minutes went by because I was having such a good time.","title":"I Ran 10k For The First Time Today"},{"content":"With the spring finally being fully released on the Netherlands, my wanderlust has awakened again after lying dormant for two years. In my 9 years in Norway I became a seasoned outdoorsman spending much time roaming the mountains and forests and always spending days or weeks out in the wild with my tent.\nWhen I moved back to the Netherlands this has been put on hold for a couple of years while I focused on changing my career on IT. Now that the career is rolling along nicely by the looks of it, I\u0026rsquo;m super excited to start exploring the Netherlands and see what the outdoors life is like in an area where the nature is significantly less wild and abundant.\nI needed a solution to post pictures quickly and easily because I want to start logging my trips as well. My hugo setup is great for text and techincal content but I need something where I can easily post straight from my phone and Tumblr seems like the best option. I\u0026rsquo;ll also be posting more on Instagram from here on out.\nHere is my first post:\nhttps://www.tumblr.com/mischavandenburg/717301155801235456/went-to-heemskerk-today-broke-my-running-record?source=share\nLinks: 202305141505\nhttps://www.tumblr.com/mischavandenburg\n","permalink":"https://mischavandenburg.com/zet/started-using-tumblr/","summary":"With the spring finally being fully released on the Netherlands, my wanderlust has awakened again after lying dormant for two years. In my 9 years in Norway I became a seasoned outdoorsman spending much time roaming the mountains and forests and always spending days or weeks out in the wild with my tent.\nWhen I moved back to the Netherlands this has been put on hold for a couple of years while I focused on changing my career on IT.","title":"Started Using Tumblr for Travel Blogging"},{"content":"My recent post on E/N blogging led to a realization: posting on social media will be something I do for myself, instead of doing it for others.\nWhen Facebook came out when I was 16 I posted quite a lot on social media. 5 years later I found it very embarassing to read back through those posts and I deleted everything, and since then I have been been holding back on social media posting and deleting my accounts several times.\nIn the meantime I also used social media as a marketing tool, creating very specific accounts tailored to a particular niche of content to drive traffic to websites. This approach to social media broke my relationship with it, and it became something you do to gain a certain response from others. I think this is the case for many people. Whether it is getting as much as possible likes, or getting people to click on an affiliate link, your content is always shaped by your anticipation of the audience\u0026rsquo;s reaction.\nStarting this blog has been a breakthrough for me. Especially when I gave up on my tech niche and just started writing about everything that interests me, it became a huge enrichment to my life, and I\u0026rsquo;m going to extend this to social media as well.\nThe thing is, I\u0026rsquo;ve always been very bad at taking pictures and storing them. I have tons of great experiences and memories stored in my mind, but I sometimes wish that I took more pictures of these adventures. Because I was lucky enough to have grown up in an an age without smartphones, there still is an inherent reluctance towards that whole culture. There is an irritation when I see people flipping out their phones when their meals get served in a restaurant, and I feel socially awkward to take my phone out in public to take pictures of things.\nNow I\u0026rsquo;m changing this: I\u0026rsquo;m going to take more pictures of the things I experience and post them on social media. Not because I want to impress others, but because I want to record the things I\u0026rsquo;m doing so I can enjoy them again at a later time. I arrived at this desire because I find it very enjoyable to scroll through my own blog and see the progression I\u0026rsquo;ve made over time, and the things I decided to share. And I want to extend this experience to pictures as well. So I\u0026rsquo;m going to post more on Instagram and Tumblr and write short travel logs of the places I visit and the things I experience, so I can revisit them later and find pleasure therein.\nI\u0026rsquo;m basically using these services to my own needs and completely disregard any audience. But if anybody finds enjoyment in seeing the things I put online, that would be a great thing.\nLinks: 202305141505\n","permalink":"https://mischavandenburg.com/zet/why-post-on-social-media/","summary":"My recent post on E/N blogging led to a realization: posting on social media will be something I do for myself, instead of doing it for others.\nWhen Facebook came out when I was 16 I posted quite a lot on social media. 5 years later I found it very embarassing to read back through those posts and I deleted everything, and since then I have been been holding back on social media posting and deleting my accounts several times.","title":"Why I'm Posting More on Social Media"},{"content":"Read an interesting post this morning about E/N websites. Apparently a phenomenon from the early days of the web, E/N websites meant everything to the publisher, but maybe nothing to the rest of the world. The term may have been the predecessor of the term \u0026ldquo;weblog\u0026rdquo; which was shortened to blog.\nE/N description: The website\u0026rsquo;s author covers a myriad of topics. It\u0026rsquo;s not narrowly focused. The author writes about everything or at least everything that\u0026rsquo;s important to the author. The site might contain something useful for anyone who visits. The content means everything to the publisher, but it could mean nothing to the rest of the world.\nIn this post from March I declared that I would write more about my personal life and expanding the range of topics that I would publish on my blog / zettelkasten. And it has a great decision so far.\nI thoroughly enjoy the sense of freedom it has given me. Rather than constraining myself to one subject area, I feel free to write about anything that currently interests me. Writing about these topics stimulates further research and more intentional content consumption.\nI\u0026rsquo;m sharing more and more personal content, and this is the type of content one might expect to be seen on a social media platform. I don\u0026rsquo;t use social media much, and decided to carve out my own space on the internet and write there instead. I will always be the owner of this content, and I can write about whatever I want and change the appearance and presentation.\nI can\u0026rsquo;t explain why, but there is something very satisfying about scrolling through my archives and seeing the things that caught my interest during those particular months, knowing that it is publically available and that others might be piqued by the keywords as well.\nDiscovering Roy Tang\u0026rsquo;s website was also very inspirational. I picked up the idea of sharing my week notes and logging the content consumption from his blog. Reading Hacker News regularly has exposed me to several personal blogs which I\u0026rsquo;ve added to my RSS feeds, and I enjoy reading about their interests and getting inspiration from what they decide to publish on their blogs.\nLinks: 202305090805\nhttps://roytang.net/\nhttp://sawv.org/en.html\n","permalink":"https://mischavandenburg.com/zet/everything-nothing-blog/","summary":"Read an interesting post this morning about E/N websites. Apparently a phenomenon from the early days of the web, E/N websites meant everything to the publisher, but maybe nothing to the rest of the world. The term may have been the predecessor of the term \u0026ldquo;weblog\u0026rdquo; which was shortened to blog.\nE/N description: The website\u0026rsquo;s author covers a myriad of topics. It\u0026rsquo;s not narrowly focused. The author writes about everything or at least everything that\u0026rsquo;s important to the author.","title":"My Blog Is Turning Into And Everything / Nothing Website And It Feels Good"},{"content":" I show my workflow to publish my Hugo blog from the command line using neovim and bash\nLinks: 202305091705\n","permalink":"https://mischavandenburg.com/zet/blogging-from-command-line/","summary":"I show my workflow to publish my Hugo blog from the command line using neovim and bash\nLinks: 202305091705","title":"Video: How I Write \u0026 Publish My Blog Using Neovim \u0026 Hugo From The Command Line"},{"content":"If you would have told me 5 years, 3 years, 1 year, or even 3 months ago, that one day I would be 50 minutes into a run with a big beaming smile on my face, I would have never believed you, but today it happened.\nMy base building schedule is giving me good results. I did this week\u0026rsquo;s long run this morning and it was 7 minutes longer than last week\u0026rsquo;s long run. I ran 8.5 kilometres in 67 minutes.\nAnd the best thing was: I was smiling. I was having fun. Running on an easy pace has been such a game changer for me. Sure, I\u0026rsquo;m probably the slowest runner on earth. It\u0026rsquo;s more like jogging, just a little bit faster than my speed when I do a brisk walk. But I\u0026rsquo;m running. I\u0026rsquo;m building a base, I\u0026rsquo;m doing the first steps, and from here I can grow.\nI\u0026rsquo;m also discovering another fun element to the sport: exploration. I moved to my current city less than a year ago and it is unexplored territory. I\u0026rsquo;m now creating a network of nice runs and circuits for myself and I\u0026rsquo;m discovering places I would never have discovered otherwise. Now that I\u0026rsquo;m able to run longer distances, I can start exploring more. Just running on one direction for half and hour and seeing where I end up. Then I look for a nice way back, and this way I\u0026rsquo;ve discoverd a few nice rounds already.\nToday the sun was shining, it was 17 degrees, I did the longest run I\u0026rsquo;ve ever done, and when I came back I felt good: energized, happy and satisfied. The endorphins where rushing through my body. Not exactly a \u0026ldquo;runner\u0026rsquo;s high\u0026rdquo;, I think, but definitely a sense of calm and achievement.\nLast week my knee started to hurt during my 60 minute run, but this week I had no pain at all. My body is adjusting.\nI\u0026rsquo;m coming to a point where I\u0026rsquo;m starting to fall in love with running. I just pray I won\u0026rsquo;t run into injuries!\nLinks: 202305071305\n","permalink":"https://mischavandenburg.com/zet/running-exploring-fun/","summary":"If you would have told me 5 years, 3 years, 1 year, or even 3 months ago, that one day I would be 50 minutes into a run with a big beaming smile on my face, I would have never believed you, but today it happened.\nMy base building schedule is giving me good results. I did this week\u0026rsquo;s long run this morning and it was 7 minutes longer than last week\u0026rsquo;s long run.","title":"Having Fun While Running And Exploring"},{"content":" Links: 202305070805\nhttps://youtu.be/Q8dxoqqMHAg\n","permalink":"https://mischavandenburg.com/zet/video-custom-fontsize-command/","summary":"Links: 202305070805\nhttps://youtu.be/Q8dxoqqMHAg","title":"Video: Creating A Command To Change Fontsize in Alacritty"},{"content":"Diablo 2: one of those games in life I keep returning to. Started playing it when I was 12, and I know it inside out. Roughly once every 1,5 years I\u0026rsquo;ll have a phase where I want to play some.\nI\u0026rsquo;m in such a phase right now. I haven\u0026rsquo;t been gaming since the Victoria 3 release, and frankly, I don\u0026rsquo;t have much time to spend either. Something will be sacrificed. Currently this will be study time. I\u0026rsquo;ve been studying for several hours a day for nearly two years, but with my current increased focus on health and weight loss, I feel I need a break and allow myself some leasure.\nDiablo 2 Resurrected will reset tomorrow and I\u0026rsquo;ve been doing a little bit of research to get up to speed on the new Terror Zones and possible character builds. I think it\u0026rsquo;s so lovely that Blizzard came out with Diablo 2 Resurrected 1,5 years ago, and that they\u0026rsquo;re still adding new content to a nearly 20 year old game.\nTerror Zones An interesting addition to the game which will change end game levelling. From what I\u0026rsquo;ve read, they are suited to farm solo from level 92 or so. Up until then Dia/Baal runs are still most efficient.\nBuilds One of the new interesting build possibilities in D2R is the Fire Druid. When D2R was launched I played an Trap assassin and a barbarian, so it will be fun to try something new. I haven\u0026rsquo;t built a druid in many years, so I look forward to that.\nI also considered doing a war cry barbarian, javazon or summon necromancer. But the fire druid appeals most to me currently.\nLadder My plan is to just build one character, and I think I\u0026rsquo;ll keep doing that in the future. When I feel like joining a ladder reset I\u0026rsquo;ll just do one character, play it through the game, do some end game farming, and by then I\u0026rsquo;ll probably be finished with Diablo for a while again.\nDruid Strategy I\u0026rsquo;ll be playing through the game in public games. Like every reset there will be a flood of cold sorceresses, so my fire damage will be a good addition to any party. From what I\u0026rsquo;ve seen, the fire druid does a ton of damage with hardly any gear, so I probably won\u0026rsquo;t have to much low level farming. I\u0026rsquo;ll just focus on playing through the game asap and do any farming in hell difficulty.\nStats Str for gear, rest vita.\nSkills 1 point Oak Sage, and take some summons to get to grizzly. Aim for grizzly by nightmare. But don\u0026rsquo;t prioritize. I\u0026rsquo;ll be playing with parties, so probably not much need for summons, and I\u0026rsquo;ll probably have to re-cast them all the time anyway which might actually be annoying. Maybe better to just focus on the fire skills.\nFigure out which attack skill I like. Fissure can be fine through all of nightmare.\nAlzorath\u0026rsquo;s maxing path:\nFissure Armageddon Molten Boulder Volcano Firestorm\nCyclone armor: wait until you have a lot of + skills. So don\u0026rsquo;t bother\nUse quick cast The quick cast feature is something new to me. As an old school player I was resistant to this feature. But from what I\u0026rsquo;ve seen, it seems very effective with the fire skills.\nGear Tal Eth Stealth breast plate.\nOrt Sol Lore. Save a 2soc druid helm for this\nBuy 2soc staff tir ral at Akara asap.\nDual spirits\nMagefist\nLater gear Rain Armor Ort Mal Ith\nRavenlore\nLinks: 2023050414055\nAlzorath video guide: https://www.youtube.com/watch?v=xPrpXS1OJ7M\nhttps://maxroll.gg/d2/guides/druid-leveling\n","permalink":"https://mischavandenburg.com/zet/d2r-ladder-reset-may-2023/","summary":"Diablo 2: one of those games in life I keep returning to. Started playing it when I was 12, and I know it inside out. Roughly once every 1,5 years I\u0026rsquo;ll have a phase where I want to play some.\nI\u0026rsquo;m in such a phase right now. I haven\u0026rsquo;t been gaming since the Victoria 3 release, and frankly, I don\u0026rsquo;t have much time to spend either. Something will be sacrificed. Currently this will be study time.","title":"My Plan For The D2R Ladder Reset"},{"content":"Had a great time at the Ziggo Dome in Amsterdam yesterday. Went to see Sabaton live. This was my second time seeing them live, and it was a great show.\nI love Sabaton because all of their material revolves around history and warfare. They are one of my favourite bands. I was super happy they played some of my favourite songs: \u0026ldquo;Carolus Rex\u0026rdquo;, \u0026ldquo;Stormtroopers\u0026rdquo; and \u0026ldquo;To Hell And Back\u0026rdquo;.\nI\u0026rsquo;ve been a lover of metal music ever since childhood, but I had a break from metal music for about 5 years where I hardly listened to any music at all. But over the past 1,5 years I\u0026rsquo;ve been really enjoying metal music again.\nI believe the last live show I saw was Cannibal Corpse in 2014 in Oslo. Nearly 10 years without a metal concert?!\nLinks: 202305041305\n","permalink":"https://mischavandenburg.com/zet/sabaton-concert-2023/","summary":"Had a great time at the Ziggo Dome in Amsterdam yesterday. Went to see Sabaton live. This was my second time seeing them live, and it was a great show.\nI love Sabaton because all of their material revolves around history and warfare. They are one of my favourite bands. I was super happy they played some of my favourite songs: \u0026ldquo;Carolus Rex\u0026rdquo;, \u0026ldquo;Stormtroopers\u0026rdquo; and \u0026ldquo;To Hell And Back\u0026rdquo;.\nI\u0026rsquo;ve been a lover of metal music ever since childhood, but I had a break from metal music for about 5 years where I hardly listened to any music at all.","title":"Saw Sabaton Live Yesterday!"},{"content":"Next winter I\u0026rsquo;ll be entering my third year as a DevOps Engineer. When you first break into this field there is an overwhelming amount of things to learn. Frankly, this will always be the case. But I\u0026rsquo;m reaching a point where I have gained experience with most of the main areas and tooling, and I can start seeing the relations between them and how they compare to one another.\nJust like any other job or activity, there are things that you like and that suit you well, and there are things that you don\u0026rsquo;t like. And now the first phases of overwhelm are confidently behind me, I\u0026rsquo;m starting to learn the things I prefer doing over others.\nBeing a DevOps Engineer does not mean you get to write code all day. I\u0026rsquo;m currently working in an organization where we\u0026rsquo;re not working fully DevOps yet. Rather than deploying many times in small increments, we are still using patching windows. Managing infrastructure in this way means you\u0026rsquo;ll need to communicate with teams to schedule the patching and submit changes to the approval board. It can involve a lot of emailing and talking back and forth. And sometimes things go wrong and everything needs to be rescheduled again. Although it is valuable experience, and I certainly have the communication skills to handle these kinds of situations, I\u0026rsquo;m learning that I lean more towards the dev side of things.\nToday was a fun day at work. I\u0026rsquo;ve been working on a PR for a while whenever I wasn\u0026rsquo;t required to work on tickets. The PR involved refactoring a Bicep template for our Azure Key Vault deployments, which were using parameter files with a lot of repeated code. After getting a lot of good feedback on my code changes, and submitting it for review multiple times, it was finally approved and I could get to work.\nI used bash, jq and vim to clean up the parameter files for each workload and submitted PRs. My colleague, who helped me with the PR\u0026rsquo;s, was standing by and ready to approve them as soon as I submitted them. Some pipelines ran into some trouble and required a few more PR\u0026rsquo;s to fix, and there was a bit of pressure because we needed to get the pipelines working before any other changes needed to be deployed the next day.\nBy working together like this we managed to clean up 2850 lines of code in one day and it was a great feeling to get this job done. I had been working on this for a couple of weeks and it\u0026rsquo;s so satisfying to move that ticket status to \u0026ldquo;done\u0026rdquo;.\nI really enjoy the coding! And I really enjoy working with Infrastructure as Code! Somehow it really gives me a lot of pleasure knowing that the code I\u0026rsquo;m writing will be transformed to running infrastructure after it is submitted. I love writing programs in my free time as well, but there is just something magical seeing your parameter file being turned into a functional Kubernetes cluster or Key Vault.\nWorking as a DevOps Engineer, I\u0026rsquo;m generally having so much fun. I really enjoyed myself when I was learning jq and used scripts to change hundreds of lines of code at a time and submitting them as a PR. And some days, the work is less enjoyable, but it is all part of the job. It all serves to deliver value to the end customer, and if I can have fun most of the time while delivering value, I know I\u0026rsquo;m in the right place.\nIn any case, there hasn\u0026rsquo;t been a second during these past two years where I regretted my choice of becoming a DevOps Engineer. I love my job, I found my passion in my work, and I\u0026rsquo;m so excited for the future. Cloud computing is never standing still, Kubernetes is starting to become more and more popular, and my skills grow with every day that passes.\nI\u0026rsquo;m a happy camper.\nLinks: 202305042005\n","permalink":"https://mischavandenburg.com/zet/ups-and-downs-of-devops-engineer/","summary":"Next winter I\u0026rsquo;ll be entering my third year as a DevOps Engineer. When you first break into this field there is an overwhelming amount of things to learn. Frankly, this will always be the case. But I\u0026rsquo;m reaching a point where I have gained experience with most of the main areas and tooling, and I can start seeing the relations between them and how they compare to one another.\nJust like any other job or activity, there are things that you like and that suit you well, and there are things that you don\u0026rsquo;t like.","title":"The Ups And Downs Of A Devops Engineer"},{"content":"Read an interesting article this morning. Apparently the Soviet Union introduced paid time off well before other industrialized countries.\nInterestingly, but not unexpectedly, the state determined how and with whom you spent that vacation.\nAs members of a collective, citizens were expected to spend their free time in a manner the government and its leading theorists considered beneficial to society as whole, regardless of an individual’s preferences.\nEver since the 1930s, it had been customary for Soviet citizens to travel in groups of 25 to 40 people randomly put together by their trade unions. When adults relaxed at resorts or embarked on tours, their children were sent to summer camps managed by state-run youth groups like the Young Pioneers or the Komsomol.\nThis reveals the dedication to the collective rather than the family.\nSoviet vacations served as teambuilding exercises that fostered group cohesion. The closer people got with their family, the further they removed themselves from their neighbors, colleagues, and government.\nIt seems unthinkable to not be in control of your own vacation to me, born in the West in 1990. Communism, the Soviet Union, and totalitarian regimes, somehow always seem to spark my interest. It is one way of organizing a society, and in some ways it seems like an attractive one. I think it has to do with the sense of purpose that living in such a society might give you. The Collective is most important, and that\u0026rsquo;s what you live for. In theory it is a selfless way of living. But history shows that it has always failed up until now.\nI\u0026rsquo;m planning this year\u0026rsquo;s vacation at this moment, I\u0026rsquo;ll be returning to my beloved Norway for three weeks in September. And I\u0026rsquo;m so glad that I can decide to go there alone, instead of having to go with a group of 40 strangers.\nLinks: 202305020605\nhttps://daily.jstor.org/workers-of-the-world-take-pto/\n","permalink":"https://mischavandenburg.com/zet/vacations-in-soviet-union/","summary":"Read an interesting article this morning. Apparently the Soviet Union introduced paid time off well before other industrialized countries.\nInterestingly, but not unexpectedly, the state determined how and with whom you spent that vacation.\nAs members of a collective, citizens were expected to spend their free time in a manner the government and its leading theorists considered beneficial to society as whole, regardless of an individual’s preferences.\nEver since the 1930s, it had been customary for Soviet citizens to travel in groups of 25 to 40 people randomly put together by their trade unions.","title":"Vacations In The Soviet Union"},{"content":"Ah, writing. It\u0026rsquo;s one of the skills that I\u0026rsquo;ve consistently been praised for throughout my life. I\u0026rsquo;m pretty good at it, and I think that\u0026rsquo;s because I enjoy it. It led me to getting an academic degree in English Literature and Culture, simply because I was following the things I was naturally good at.\nHowever, just like any other skill, you can only become better at it through consistent practice. This morning I read a wonderful little blog post on writing regularly, and I was touched by his words:\nSimilarly, some people believe that the art of writing is a skill that can be summoned on a whim, like a genie emerging from a lamp with a flourish of creative brilliance. In my experience, however, writing is more akin to the labour of a blacksmith, hamming words into shape through the heat and sweat of regular practise.\nDaily Practice One way to gain more experience is to start keeping a daily journal. I\u0026rsquo;ve always kept some notebook or other, jotting down thoughts and ideas as they came along sporadically. But it was only in July 2020 that I started to keep a daily journal and writing regularly. I started out writing by hand. I was in at an interesting place in life: I was spending two years in a Buddhist monastery, and after settling down for a couple of months, the intensive meditation practice was releasing all sorts of things in the mind and body, and I started to write them all down. I\u0026rsquo;m still convinced that starting this habit has been one of the most important and beneficial decisions of my life.\nIt is so simple, but extremely effective. And you don\u0026rsquo;t have to be doing anything special, or be in a special place. Just keeping a daily journal where you note down the things you did that day in a small pocket book is enough to get the habit going. Soon you will find that you will get the urge to write more. After several years of daily journaling, sometimes all I write is \u0026ldquo;It\u0026rsquo;s been a pretty good day\u0026rdquo;. And that\u0026rsquo;s it for that day. Sometimes I write several pages worth of thoughts, ideas and these longer writings often have powerful effects.\nThe Power Of Writing Sometimes I\u0026rsquo;ll be struggling with something, and I\u0026rsquo;ll be at a loss of what to do. The thoughts just keep whirling about aimlessly and no solution to the particular problem at hand seems to emerge from the chaos. Whenever I am in such a situation, I sit down, close my eyes, observe what is going on, and I start to write. I write it all down. The thoughts, the associations that these thoughts generate, and the effects they are having. I write down exactly how I feel. Attaching words to the feelings that I\u0026rsquo;m experiencing mentally and physically. \u0026ldquo;I feel a contraction in my lower abdomen\u0026rdquo;. \u0026ldquo;I\u0026rsquo;m angry, and it manifests as a hot feeling on my forehead\u0026rdquo;. And that\u0026rsquo;s when the magic happens: something that I\u0026rsquo;ve come to identify as \u0026ldquo;The Power Of Writing\u0026rdquo;.\nSomehow the act of writing everything down will create structure in the chaos, and the solution to my problem wil emerge, often from very unexpected angles. Is it the fact that it is structured on paper? Or the fact that the visual representation of the contents of my mind enables me to apply structure to them? Is it the fact that I am moving my hands, adding bodily movement to my cognitive efforts?\nI don\u0026rsquo;t have the exact answer, but all I know is that it works for me. Almost every time. And often when I don\u0026rsquo;t expect it at all. I\u0026rsquo;ll just be sitting and writing in my journal, writing down a particular challenge that I\u0026rsquo;m facing that day. And somehow some solution appears out of the blue. Whenever this happens, I always smile and write down: \u0026ldquo;The Power of Writing\u0026rdquo;.\nMindfulness Even though I might not know the exact answer, I\u0026rsquo;m certain that it is very closely related to mindfulness. Becoming more mindful through meditation and present moment awareness is the key to happiness in my opnion. I developed a mindfulness practice out of writing by creating this ritual where I sit down and identify what is going on and writing it down. In essence, I\u0026rsquo;m forcing myself to be mindful, and to attach words to my mental and bodily experience. I\u0026rsquo;m getting out of my own way and letting my mind solve the challenge all by itself.\nYou know what? It\u0026rsquo;s happening right now! The Power of Writing. I hadn\u0026rsquo;t been able to clearly capture in words what the mechanism behind the Power of Writing was, but as I was writing this article, it became obvious to me that it must be mindfulness underlying it all. So I\u0026rsquo;m just going to leave it as it is. This is a perfect example of what you will find in my private zettelkasten. I wrote all of this in one sitting, with minimal editing. Just stream of conciousness, writing it down. And I finished with a wonderful realization.\nI feel grateful, satsified and happy. There is also a pleasant tingling at the top of my neck, and an energy rising up through my feet. I hope that this might help someone else one day, and inspire someone to pick up the habit of writing.\nLinks: 202305012005\nhttps://herman.bearblog.dev/on-writing-regularly/\n","permalink":"https://mischavandenburg.com/zet/power-of-writing/","summary":"Ah, writing. It\u0026rsquo;s one of the skills that I\u0026rsquo;ve consistently been praised for throughout my life. I\u0026rsquo;m pretty good at it, and I think that\u0026rsquo;s because I enjoy it. It led me to getting an academic degree in English Literature and Culture, simply because I was following the things I was naturally good at.\nHowever, just like any other skill, you can only become better at it through consistent practice. This morning I read a wonderful little blog post on writing regularly, and I was touched by his words:","title":"The Power of Writing"},{"content":"For the past few months I\u0026rsquo;ve been using this neat API to check the weather from the command line. It\u0026rsquo;s very simple to use. Simply run:\ncurl wttr.in\nThis will show the weather based on your location, but you can also give a particular location and format.\ncurl wttr.in/Amsterdam\nLinks: 202304271504\nhttps://github.com/chubin/wttr.in\n","permalink":"https://mischavandenburg.com/zet/check-weather-terminal/","summary":"For the past few months I\u0026rsquo;ve been using this neat API to check the weather from the command line. It\u0026rsquo;s very simple to use. Simply run:\ncurl wttr.in\nThis will show the weather based on your location, but you can also give a particular location and format.\ncurl wttr.in/Amsterdam\nLinks: 202304271504\nhttps://github.com/chubin/wttr.in","title":"How To Check The Weather From The Command Line"},{"content":" I use RSS feeds to read the news, and I use Newsboat as a RSS reader.\nTo in stall on MacOS: brew install newsboat\nIntegrate in already existing terminal workflow Free from distraction Control how content is rendered Carefully curate which content you consume Build your own \u0026ldquo;Social Media feed\u0026rdquo; No advertisements or suggested articles Newsboat uses RSS feeds which are generated by websites Links: 202304271004\nhttps://newsboat.org/\n","permalink":"https://mischavandenburg.com/zet/reading-news-from-command-line/","summary":"I use RSS feeds to read the news, and I use Newsboat as a RSS reader.\nTo in stall on MacOS: brew install newsboat\nIntegrate in already existing terminal workflow Free from distraction Control how content is rendered Carefully curate which content you consume Build your own \u0026ldquo;Social Media feed\u0026rdquo; No advertisements or suggested articles Newsboat uses RSS feeds which are generated by websites Links: 202304271004\nhttps://newsboat.org/","title":"Video: How To Read The News From The Command Line"},{"content":"It\u0026rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.\nFor example:\naz aks operation-abort --name myAKSCluster --resource-group myResourceGroup\nLinks: 202304270704\nhttps://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/\nhttps://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli\n","permalink":"https://mischavandenburg.com/zet/aks-abort-operation/","summary":"It\u0026rsquo;s now possible to abort long running operations on AKS clusters. It was released as Generally Available.\nFor example:\naz aks operation-abort --name myAKSCluster --resource-group myResourceGroup\nLinks: 202304270704\nhttps://azure.microsoft.com/en-us/updates/generally-available-operation-abort-in-aks/\nhttps://learn.microsoft.com/en-us/azure/aks/manage-abort-operations?tabs=azure-cli","title":"You Can Abort Operations on AKS Clusters Now"},{"content":"This week I started a project which I\u0026rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I\u0026rsquo;ve been collecting hardware here and there, and I\u0026rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein\u0026rsquo;s Homelab Init playlist on YouTube which I\u0026rsquo;m working on.\nThere are a few reasons why I haven\u0026rsquo;t started up until now:\nFocused on switching jobs and certifications Not knowing what to run on the cluster High electricity costs Now that I found a new job with a great employer, I\u0026rsquo;ve changed my focus towards doing more hands-on learning in my free time by learning Go and diving deeper into Cloud Native technology. Energy prices have come down in the meantime as well.\nI\u0026rsquo;ve reached a stage in my Go learning journey where I\u0026rsquo;m actually able to start making small deployable applications, and I want an environment where I can do that without high costs or without fearing to break something. I want to learn more about databases on Kubernetes, and I want to start writing small microservices and API\u0026rsquo;s that are able to query these databases.\nMy lab is going to be my playground, where I can deploy whatever I want and learn the technologies that interest me at that particular moment.\nHardware: Parts For some reason the Raspberry Pi has become synonymous with homelabs. I get that it\u0026rsquo;s fun to run a cluster on something that is not much larger than a 1kg pack of sugar. But I never really caught on to that whole scene yet. Maybe it\u0026rsquo;s because I\u0026rsquo;m a bit late to the party and the Pi\u0026rsquo;s have been scarce and very expensive lately?\nIn any case, I\u0026rsquo;ve been thankfully accepting old computers that friends were going to get rid of, and I\u0026rsquo;ve been keeping some of my own old hardware as well. I have enough motherboards and other parts to assemble around 3 nodes, which will probably have around 8GB RAM each, but possibilities to attach storage.\nThis is also what has been keeping me back for a while I think. There is quite a bit of work that I need to do to get these machines going, and probably I\u0026rsquo;ll have to purchase a couple of other parts. However, I also have some functional hardware.\nGaming Desktop I have an old gaming desktop with 16GB RAM, an Intel 6700K Skylake, 1070 video card and a couple of TB of storage.\nThis has been my Arch Linux desktop for the past year, but now that I switched to my new MacBook, I\u0026rsquo;m not using it as much. I want to keep it as it is right now, but I could run a few Virtual Machines on there, and maybe consider turning it into a ProxMox server.\nOld Laptops I have two old laptops. One Asus with 4GB of RAM and a Thinkpad T430 with 8GB RAM. The Thinkpad is actually surprisingly powerful. As a weekend project I installed Arch on it and I fitted it with a refurbished keyboard, and it is a very pleasant machine to work with. However, now that I have a very powerful laptop that I use as a desktop and portable device, it has become redundant.\nOld Laptops as Raspberri Pi\u0026rsquo;s Having these two old laptops lying around, it occurred to me that these machines were basically Raspberri Pi\u0026rsquo;s with a large form factor and a higher power usage. Why would I need to spend hundreds of euros on these smaller computers if I could just use these laptops as a starting point for my lab?\nUsing laptops has the following advantages:\nNo additional costs No building needed Easy to install Linux on them Built-in screen and keyboard for quick access when SSH does not work out Built-in batteries to handle short power disruptions (rare but possible) Can get going very quickly Choices and Goals Kubernetes The first goal is to get a Kubernetes cluster running. I will do bare metal kubeadm installs, and later I want to learn more about Talos. Fortunately I feel very comfortable installing Kubernetes with kubeadm. I did plenty of practice for my CKA, and I recently installed it on free Oracle VM\u0026rsquo;s. I\u0026rsquo;ve experimented a bit with K9S earlier, but I want to learn how to maintain on-prem Kubernetes.\nMy goals is to learn to maintain production-grade clusters properly.\nLinux Naturally I\u0026rsquo;ll be using Linux as my base OS. After some consideration I chose to use Ubuntu Server 22. Some notes on that choice:\nI already have years of Ubuntu Server experience Good to keep building on what I have Working with managed Kubernetes on my day job requires me to keep Linux admin skills fresh Still the most popular Linux distro Google uses Ubuntu Server Well documented and plenty of questions on StackOverflow Infrastructure as Code \u0026amp; GitOps Initially I\u0026rsquo;ll configure the servers by hand, but I want to have the server configuration as code as Ansible playbooks eventually. However, I\u0026rsquo;ll be using ArgoCD for all of my deployments on Kubernetes itself, so the server configuration is only a very small part of the setup. Just get Kubernetes running and do the rest with ArgoCD.\nPerhaps I will expand with larger servers that run multiple VM\u0026rsquo;s. Then it will be very relevant to start provisioning these with Ansible.\nNetworking I want to learn more about networking and use static IP addresses for my servers. I need to figure out how my home network works exactly. Surprisingly, I\u0026rsquo;ve never taken the effort to actually know how the devices on my network get their IP addresses and how they communicate, even though I\u0026rsquo;ve learned plenty about it for my day job and do networking in an enterprise environment daily.\nFor Kubernetes I\u0026rsquo;ll use Flannel to start out with, but I want to learn more about Cilium, Istio and other service mesh implementations.\nAnother goal is to host my own DNS server for internal name resolution, probably CoreDNS.\nDeployment I want to host my own container registry (Harbor) and use Tekton pipelines to for CI/CD, and I\u0026rsquo;m playing with the thought to host my own GitLab instance as well.\nLet\u0026rsquo;s Go! Another realization was that I don\u0026rsquo;t need to have everything figured out before I begin. The beauty of cluster computing is that you can add to it as you go. I can start with a small cluster of two nodes and build it out as my needs grow. I don\u0026rsquo;t expect to need more than a few GB of RAM in the foreseeable future, so these two laptops will be plenty to get going.\nLinks: 2023041213\n[[homelab]]\n[[homelab-network]]\n[[Linux]]\n[[homelab-ubuntu-server]]\n[[Kubernetes]]\n","permalink":"https://mischavandenburg.com/zet/starting-my-homelab/","summary":"This week I started a project which I\u0026rsquo;ve been putting off for too long. I finally started my homelab. Over the past year I\u0026rsquo;ve been collecting hardware here and there, and I\u0026rsquo;ve had the intention to start up a proper Kubernetes cluster at home. I got inspired by Rob Muhlenstein\u0026rsquo;s Homelab Init playlist on YouTube which I\u0026rsquo;m working on.\nThere are a few reasons why I haven\u0026rsquo;t started up until now:","title":"Starting My Homelab"},{"content":"The Tour of Go is very clear:\nGo does not have classes.\nOne benefit of learning multiple programming languages is that each language provides you with a set of “pegs” that you can use to refer to other languages. As I learned about structs in Go, I hung them to the “Python classes” peg and used that as a reference point. Using these reference points can help you to understand the object of study by looking at their differences and similarities.\nEven though Go does not have classes, my understanding of Python classes did help me to understand structs much more quickly.\nGo Structs A struct is a collection of fields, and they are accessed using a dot.\nEach data field in a struct has its own type, either user defined or built in.\ntype Person struct { name string address string } person1 := Person{\u0026#34;Mischa\u0026#34;, \u0026#34;Amsterdam\u0026#34;} fmt.Println(person1.name) fmt.Println(person1.address) When creating a struct, you can use the Name: syntax to set the values. Otherwise, you need to populate the fields in order.\ntype Computer struct { os string ram int } func main() { macBook := Computer{os: \u0026#34;MacOs\u0026#34;} fmt.Println(macBook) thinkPad := Computer{\u0026#34;Arch Linux\u0026#34;, 16} fmt.Println(thinkPad) } This produces the output:\n{MacOs 0} {Arch Linux 16} In the first line, the ram is 0 because I only set the os field, and unset fields get a 0 value by default.\nStruct Methods Classes in Python can contain methods. What about Go structs?\nStructs can have methods, but they are not contained in the struct definition, like you would see in Python. Methods are defined on types, and the type does not need to be a struct. Therefore, methods are defined after you create the struct.\ntype Computer struct { os string ram int } func (c Computer) doubleRam() string { return fmt.Sprintf(\u0026#34;If you would double your ram, you would have %v GB of ram.\u0026#34;, c.ram * 2) } func main() { macBook := Computer{\u0026#34;MacOs\u0026#34;, 32} fmt.Println(macBook.doubleRam()) } Output:\nIf you would double your ram, you would have 64 GB of ram.\nIn this example, doubleRam() is my method of the Computer type. Methods take a special receiver argument, written between the func keyword and the method name. The doubleRam method has a receiver of type Computer, named c.\nStruct Literal A struct literal is a struct which has its contents defined in the source code itself. The opposite would be to generate the contents of the struct through computation or reading memory during the execution of the program.\nConclusion In conclusion, here are the similarities and differences that stood out to me in this morning\u0026rsquo;s study of Go structs.\nSimilarities Both can have fields to hold values of different types Both can have methods Differences Go structs are used for data structures, classes in Python are used for OOP Go struct fields have static types Methods are defined separately from the struct definition Links: 202304100704\n[[go]]\n[[data-types]]\nhttps://go.dev/tour/moretypes/2\nhttps://go.dev/tour/methods/1\nhttps://articles.wesionary.team/map-vs-struct-in-golang-when-to-use-b0b66446627a\n","permalink":"https://mischavandenburg.com/zet/go-struct-python-class/","summary":"The Tour of Go is very clear:\nGo does not have classes.\nOne benefit of learning multiple programming languages is that each language provides you with a set of “pegs” that you can use to refer to other languages. As I learned about structs in Go, I hung them to the “Python classes” peg and used that as a reference point. Using these reference points can help you to understand the object of study by looking at their differences and similarities.","title":"Structs In Go - Similar To Classes In Python?"},{"content":"I\u0026rsquo;m working on my twitter CLI and as I was writing a function to format the tweet I remembered something I picked up last week. After a quick search in my notes I remembered to use the %q with printf.\nslice := []string{feed.Items[0].Title, feed.Items[0].Link} result := strings.Join(slice, \u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;Testing printf %q\u0026#34;, result) This is very useful when formatting output. Now I can actually see whether it is inserting the new line characters correctly:\nTesting printf \u0026quot;I Made My First Tweet Using My Go Program\\nhttps://mischavandenburg.com/zet/go-first-tweet/\u0026quot;I Made My First Tweet Using My Go Program\nThis can be harder to see when printing the variable using fmt.Println or using the %v verb with Printf.\nLinks: 202304091304\n[[go]]\n[[coding]]\n","permalink":"https://mischavandenburg.com/zet/go-use-q-for-debugging/","summary":"I\u0026rsquo;m working on my twitter CLI and as I was writing a function to format the tweet I remembered something I picked up last week. After a quick search in my notes I remembered to use the %q with printf.\nslice := []string{feed.Items[0].Title, feed.Items[0].Link} result := strings.Join(slice, \u0026#34;\\n\u0026#34;) fmt.Printf(\u0026#34;Testing printf %q\u0026#34;, result) This is very useful when formatting output. Now I can actually see whether it is inserting the new line characters correctly:","title":"Use The %q Verb When Debugging In Go"},{"content":"I spent the evening learning about the Twitter API. It was not as straight forward as I thought. My goal was to do this project using only the standard library, and I hoped to get away with a few simple curls, but since the API requires OAuth 1 to create tweets, I had to revise my strategy.\nAfter struggling with Postman for a few hours to get the correct environment variables set up I managed to make my first tweet through Postman. Turns out that Twitter made some big changes in the free tier of their API, and it took me quite a while to figure out that the functions that are used as examples in the API documentation are not accessible to free accounts anymore.\nWhen I realized I would not get away with a simple curl I looked into libraries, but many were deprecated. Eventually I discovered gotwi and it didn\u0026rsquo;t take long to make my first tweet using Go.\nIt feels like cheating, but looking at the complexity of the library, it looks like I would be stuck in the weeds for quite a while if I wanted to figure all of this out by myself.\nThere is still enough to figure out in this project, such as curling my RSS feed and creating a tweet out of that, and wrapping it in a CLI, so in the end I\u0026rsquo;m glad I found a working library.\nHere\u0026rsquo;s the program I used to make my first tweet. It is a modified example from the gotwi GitHub readme.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/michimani/gotwi\u0026#34; \u0026#34;github.com/michimani/gotwi/tweet/managetweet\u0026#34; \u0026#34;github.com/michimani/gotwi/tweet/managetweet/types\u0026#34; ) func main() { in := \u0026amp;gotwi.NewClientInput{ AuthenticationMethod: gotwi.AuthenMethodOAuth1UserContext, OAuthToken: os.Getenv(\u0026#34;TWITTER_ACCESS_TOKEN\u0026#34;), OAuthTokenSecret: os.Getenv(\u0026#34;TWITTER_ACCESS_TOKEN_SECRET\u0026#34;), } c, err := gotwi.NewClient(in) if err != nil { fmt.Println(err) return } p := \u0026amp;types.CreateInput{ Text: gotwi.String(\u0026#34;Hello World! This time I\u0026#39;m using os.Getenv in my Go program to load the credentials.\\n#go #coding #study #learning\u0026#34;), } res, err := managetweet.Create(context.Background(), c, p) if err != nil { fmt.Println(err.Error()) return } fmt.Printf(\u0026#34;[%s] %s\\n\u0026#34;, gotwi.StringValue(res.Data.ID), gotwi.StringValue(res.Data.Text)) } Links: 202304082104\nhttps://github.com/michimani/gotwi\nhttps://mischavandenburg.com/zet/go-twitter-cli-project/\n[[go]]\n[[go-twitter-cli-project]]\n","permalink":"https://mischavandenburg.com/zet/go-first-tweet/","summary":"I spent the evening learning about the Twitter API. It was not as straight forward as I thought. My goal was to do this project using only the standard library, and I hoped to get away with a few simple curls, but since the API requires OAuth 1 to create tweets, I had to revise my strategy.\nAfter struggling with Postman for a few hours to get the correct environment variables set up I managed to make my first tweet through Postman.","title":"I Made My First Tweet Using My Go Program"},{"content":"When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.\nOver the past few months I\u0026rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\nTwitter CLI Programs should solve a problem. My problem has to do with Twitter. I recently created a Twitter account, and I want to make a tweet whenever I publish something new on my website. I\u0026rsquo;m currently doing this by hand, and that needs to stop, obviously.\nThere are bots out there for this, but I want to build it myself. I\u0026rsquo;ve created the following user stories for my project.\nUser Story 1 As a user, I need a command that I can run from a bash shell that will post the standard input to my Twitter account\nUser Story 2 As a user, I need a command that I can run from a bash shell that will take the latest post from the RSS feed generated by my blog and post it to Twitter\nConcepts By writing this program I\u0026rsquo;ll need to figure out the following problems in Go:\nTaking input from the command line Authenticating to the Twitter API Making a POST request to the Twitter API Curling an RSS feed Looping over / reading XML / HTML data Transforming that data to a suitable format to post to Twitter Expansion This will be a good start for my project and will keep me busy for a while. When I solved the previous problems I can use the result and expand further. Some thoughts about further expansion:\nRSS Feeds I can use the skills I learn to start crawling Reddit feeds and filter them for keywords. I can automatically generate a curated selection from Reddit which will be easier to consume and will save me time by only serving me content that I might think is interesting to me, based on keywords.\nDatabase I want to learn more about using databases on Kubernetes and how to interact with databases using Go. For this I\u0026rsquo;d like to store my RSS feed into a database and keep track of information in the database. I could track whether an article has been posted to Twitter and when.\nBot Rather than posting my latest blog post to Twitter by manually running a command, I should have a bot scanning my blog and posting to Twitter when it detects a new article. Or I could trigger the bot whenever I make a push to my blog repo.\nIn any case, I want to have an application running on a server. I\u0026rsquo;m making plans to start up a proper home lab and this will be a perfect use case to start running on my home Kubernetes cluster.\nLinks: 202304081304\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\n[[go]]\n[[go-twitter-cli-project]]\n","permalink":"https://mischavandenburg.com/zet/go-twitter-cli-project/","summary":"When learning a programming language it is important to start building things quickly and to begin applying the theory. I have a tendency to dive into the books and lose myself in the theory, where I should be getting hands on experience.\nOver the past few months I\u0026rsquo;ve generated a bunch of ideas for projects that I want to write, and I selected my first project today.\nhttps://github.com/mischavandenburg/twitter-cli\nhttps://twitter.com/mischa_vdburg\nTwitter CLI Programs should solve a problem.","title":"Outlining My First Go Project"},{"content":"Dopamine Listened to an interesting podcast on Dopamine by Andrew Huberman. I always enjoy his content because he is completely driven by science, not by personal opinion. He is truly objective as far as I can tell.\nDopamine is often misunderstood. It is not necessarily associated with good feelings after you pursue an activity. It is more related to motivation to engage in certain behaviours.\nRewards Scientists did an experiment where they observed children of kindergarten age, and later reproduced this in adults. The children were monitored during school days, where they had blocks of structured time and free time. During structured time their activities were managed. They had to sing or write, for example. Then they had blocks of free time where they could do whatever activity they liked.\nThey observed what activity the children liked to do in their free time, and then started to reward these children for their work. For example, if the children liked to draw in their free time, the researches started giving them gold stars and other praise for their work.\nThen the researches removed the reward for the activity, and interestingly, the children and adults started showing less interest and enjoyment in the activity because they were lacking the reward.Note that this is the activity that they liked to do in their free time prior to the experiment.\nThis all has to do with the nature of dopamine, dopamine baselines and peaks.\nHuberman explains this with a metaphor of a pool. In the pool you have waves, and the waves have peaks and troughs. At the peaks, your dopamine is high, but they are inevitably followed by troughs: dips in dopamine, which lead to low energy and low motivation.\nThen there is the baseline of dopamine: the amount of water in the pool. The baseline is the relative point to which the peaks and throughs are determined.\nSo it is important that you don\u0026rsquo;t reward yourself for activities that you are doing. Don\u0026rsquo;t reward yourself too much when you achieve a fitness or study goal. You will feel good after the event, but you will end up in a through. Being in the trough means feeling demotivated, leading to procrastination.\nGetting Out Of The Trough The good news is that the system will reset itself after experiencing highs or lows due to rewards.Getting out of the trough happens automatically, but you can influence the rate at which you climb out of it.\nLet\u0026rsquo;s say you are not motivated to study. You\u0026rsquo;ll need to do something which is more painful. Something that\u0026rsquo;s even harder to do. Whatever feels harder to do in the moment than the activity that you should be doing. Do something that makes you feel uncomfortable.\nTactics exercise for 1 minute do something that really sucks cold water anything that makes you cringe leverage something that\u0026rsquo;s painful The Holy Grail Of Motivation The holy grail of motivation: not needing any external reward, but enjoying the activity itself that is leading you towards your goal. For example, enjoying running training in pursuit of a marathon goal. Enjoying coding and doing coding exercises in pursuit of becoming a software engineer.\nI feel very fortunate that I succeeded at turning my passion into my career. The journey itself is fun. I enjoy coding, and that activity is taking me closer towards the goal of becoming a good software engineer and the ability to contribute to open source.\nLinks: 202303311903\nhttps://hubermanlab.com/leverage-dopamine-to-overcome-procrastination-and-optimize-effort/\n[[My Ikigai]]\n[[coding]]\n[[devops]]\n[[Career]]\n","permalink":"https://mischavandenburg.com/zet/dopamine-and-motivation/","summary":"Dopamine Listened to an interesting podcast on Dopamine by Andrew Huberman. I always enjoy his content because he is completely driven by science, not by personal opinion. He is truly objective as far as I can tell.\nDopamine is often misunderstood. It is not necessarily associated with good feelings after you pursue an activity. It is more related to motivation to engage in certain behaviours.\nRewards Scientists did an experiment where they observed children of kindergarten age, and later reproduced this in adults.","title":"Podcast Notes: Dopamine and Motivation - Huberman"},{"content":"I noticed multiple times now that during my workouts my mind is generating lots of new ideas for coding projects or blog posts. It is a well known fact that walking stimulates areas in the brain that which will in turn stimulate creativity. But I\u0026rsquo;m noticing that it starts to happen with strength workouts as well.\nAlthough I don\u0026rsquo;t struggle with the motivation to get myself to the gym, I sometimes feel \u0026ldquo;guilty\u0026rdquo; for not spending that time on coding or studying. I\u0026rsquo;m realizing now that this is a very irrational train of thought.\nExercise actually stimulates my creativity and will help generating new ideas or help me understand concepts better in the unconscious. It is also massively beneficial for health and general well being. When general well-being is enhanced I\u0026rsquo;ll also be able to focus better and absorb information more quickly.\nI should re-frame my thinking and actually put exercise as priority number 1. Exercise time is never wasted time. It indirectly contributes to my productivity and coding endeavours as well.\nLinks: 202304011104\n[[exercise]]\n[[Health]]\n[[Focus]]\n[[productivity]]\n","permalink":"https://mischavandenburg.com/zet/exercise-stimulates-creativity-and-motivation/","summary":"I noticed multiple times now that during my workouts my mind is generating lots of new ideas for coding projects or blog posts. It is a well known fact that walking stimulates areas in the brain that which will in turn stimulate creativity. But I\u0026rsquo;m noticing that it starts to happen with strength workouts as well.\nAlthough I don\u0026rsquo;t struggle with the motivation to get myself to the gym, I sometimes feel \u0026ldquo;guilty\u0026rdquo; for not spending that time on coding or studying.","title":"Exercise Stimulates Creativity And Motivation"},{"content":"A couple of weeks ago I created a simple bash script to generate a date in format YYY-MM-DD to use with the magic !! wands in vim.\nJust now I wanted to expand this with being able to create a markdown header with this date.\n#!/bin/bash # Gendate generates the date in YYY-MM-DD format # Can be called with arguments h and number to generate a markdown heading # For example: \u0026#39;gendate h 3\u0026#39; will generate \u0026#39;### 2023-03-29\u0026#39; header=\u0026#34;\u0026#34; date=$(date +\u0026#34;%Y-%m-%d\u0026#34;) if [[ $# -gt 0 ]]; then # handling wrong arguments if [[ $# -eq 1 || \u0026#34;$1\u0026#34; != \u0026#34;h\u0026#34; ]]; then echo \u0026#34;Usage: gendate h 2 to generate with markdown heading ##\u0026#34; exit 1 fi # format markdown heading if arguments h, n are given if [[ $# -eq 2 ]]; then for i in $(seq 1 $2); do header+=\u0026#34;#\u0026#34; done header+=\u0026#34; \u0026#34;$date echo \u0026#34;$header\u0026#34; exit 1 fi fi # if no arguments given, generate the date echo \u0026#34;$date\u0026#34; Links: 202304011104\nhttps://github.com/mischavandenburg/dotfiles/tree/main/scripts\n","permalink":"https://mischavandenburg.com/zet/generate-markdown-headings-date/","summary":"A couple of weeks ago I created a simple bash script to generate a date in format YYY-MM-DD to use with the magic !! wands in vim.\nJust now I wanted to expand this with being able to create a markdown header with this date.\n#!/bin/bash # Gendate generates the date in YYY-MM-DD format # Can be called with arguments h and number to generate a markdown heading # For example: \u0026#39;gendate h 3\u0026#39; will generate \u0026#39;### 2023-03-29\u0026#39; header=\u0026#34;\u0026#34; date=$(date +\u0026#34;%Y-%m-%d\u0026#34;) if [[ $# -gt 0 ]]; then # handling wrong arguments if [[ $# -eq 1 || \u0026#34;$1\u0026#34; !","title":"Generating Markdown Headings with YYYY-MM-DD Date Format Using Bash"},{"content":"I\u0026rsquo;m still at the beginning of my Go learning journey, but I worked through a few tutorials and guides by now. I\u0026rsquo;ve gathered lots of ideas for programs that I want to write, big and small, but I have to start somewhere.\nThe best thing to do is to write little programs that solve a problem that you have.\nOne problem I needed to solve was converting sentences to title case in vim. There are plugins for this, or elaborate macros, but I thought this was a nice opportunity to write my first program from scratch. You can view the program here: my go repo.\nTitle Converting a string to title case is fairly easy:\n// Package title converts a string to title case. package title import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) func Make(s string) { t := strings.Title(s) fmt.Printf(\u0026#34;%v\\n\u0026#34;, t) } This function takes a string as an argument and uses the Title function from the strings package to convert it. I use printf to format the output and to add a new line character.\nChallenge The challenge lies in taking input from Stdin. I solved this by reusing the things I learned from the greet challenge. I described how to read from standard input in go in this article.\nHere is the code for the title command:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/mischavandenburg/go/projects/title\u0026#34; \u0026#34;github.com/mischavandenburg/go/projects/title/internal\u0026#34; ) func main() { var x string var err error if len(os.Args) \u0026gt; 1 { x = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } if x == \u0026#34;\u0026#34; { x, err = internal.ReadLine(os.Stdin) } if err != nil { log.Print(err) return } title.Make(x) } This main function checks if arguments are given to the program from the command line, and uses these arguments if they are given. If there are no arguments, it will expect input to be piped to it. It takes either the input from the arguments or from standard input and calls the Make() function that I described above.\nThis main function uses a Readline function to extract the string from standard input:\npackage internal import ( \u0026#34;bufio\u0026#34; \u0026#34;io\u0026#34; \u0026#34;strings\u0026#34; ) // ReadLine takes anything of type io.Reader and returns a trimmed string (initial // and trailing white space) or an empty string and error if any error // is encountered. func ReadLine(in io.Reader) (string, error) { out, err := bufio.NewReader(in).ReadString(\u0026#39;\\n\u0026#39;) out = strings.TrimSpace(out) return out, err } For a more detailed explanation of this function read this article I wrote.\nUsage I wrote the program so I can call it with a sentence as an argument:\nmischa@mac-beast:~ (ins)$ title hello world, this is my sentence in title case Hello World, This Is My Sentence In Title Case But the main use case is to use it as a UNIX filter inside of vim. A UNIX filter is a program that takes standard input, performs some operation on the input, and prints it to standard output.\nNow, when I\u0026rsquo;m working inside of vim, I can convert the current line to title case by typing the following command:\n!!title\nThe !! command sends the specified amount of lines to the command you specify. Exclamation marks truly are magic wands! I highly recommend reading this article by rwxrob to learn more about them.\nLinks: 202303280803\nhttps://github.com/mischavandenburg/go/tree/main/projects/title\nhttps://rwx.gg/tools/editors/vi/how/magic/\nReading from standard input in Go\n","permalink":"https://mischavandenburg.com/zet/my-first-go-program/","summary":"I\u0026rsquo;m still at the beginning of my Go learning journey, but I worked through a few tutorials and guides by now. I\u0026rsquo;ve gathered lots of ideas for programs that I want to write, big and small, but I have to start somewhere.\nThe best thing to do is to write little programs that solve a problem that you have.\nOne problem I needed to solve was converting sentences to title case in vim.","title":"I Wrote My First Go Program Today"},{"content":"Pods have containers, and limits can be set on those containers.\nRequests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.\nLimits containers may never use more than the set limit\nenforced by kubelet and container\nhost kernel will kill processes that attempt to allocate more than limit (OOM error)\nreactively: killed when exceeded\nenforcement: system prevents container to ever exceed limit\nIf the node runs out of memory and the container exceeds its memory request, the pod will be evicted\ncontainer runtimes don\u0026rsquo;t terminate Pods or containers for excessive CPU usage\nPods The pod resource request and limit is the sum of the resource requests of the containers in the pod.\nCPU Units Defined as an absolute amount of resource. 1000m = 1 CPU.\nThis is always the same unit, regardless whether the host has 4 or 48 CPU\u0026rsquo;s.\n500m CPU = 0.5 CPU\nMemory Units Can use P, T, G, M etc.\nNote that \u0026ldquo;m\u0026rdquo; is not megabyte. 0.8m = 0.8 bytes.\nUse mebibytes Mi or megabytes M.\nDefiniton Example spec: containers: - name: nginx image: nginx resources: requests: memory: 100Mi cpu: 250m limits: memory: 200Mi cpu: 500m Scheduling The scheduler ensures that the sum of requests of the pods on the node does not exceed the available resources.\nEven if a node has low resource usage, it will not accept pods that have requests which exceed the available resources.\nNodes use k describe node to see the resource status of the node.\nLinks: 202303281903\nhttps://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n","permalink":"https://mischavandenburg.com/zet/kubernetes-resource-management-pods-containers/","summary":"Pods have containers, and limits can be set on those containers.\nRequests used by the kube-scheduler to determine where the Pod will be placed containers can use more than requested resources if it is available on node If a limit is specified, but no request, Kubernetes will use the limit value as the request value.\nLimits containers may never use more than the set limit\nenforced by kubelet and container","title":"Kubernetes Resource Management for Pods and Containers - CPU and Memory"},{"content":"I used to make all my notes on paper, but I decided to switch to a digital note-taking system about two years ago.\nDigital note-taking provides the following advantages:\nSearchability Collected in one place Can be converted to different output formats Easier to share with others Does not take up physical space No risk of losing your notes in case of fire or other disaster I\u0026rsquo;ve gone through several iterations of my note-taking systems. I started on Google Docs, moved on to Notion and eventually landed on Obsidian. However, as I became more proficient with UNIX systems and vim, I realized I did not need all of that distracting functionality, and I switched over to using Neovim and a few bash scripts. I occasionally open up Obsidian to look at my graph view or to make use of the Anki plugin, but I enjoy the trimmed-down version that I built myself. I don\u0026rsquo;t need to leave the command line to read my notes or to create a new one.\nIn this article I\u0026rsquo;ll show you the system I\u0026rsquo;m currently using for note-taking. My note-taking system is part of a larger system which I call my \u0026ldquo;Second Brain\u0026rdquo;, a term coined by Tiago Forte. This is an overarching system I\u0026rsquo;ve build to manage my time, projects, tasks and bookmarks. Explaining this entire system is beyond the scope of this article, but I plan to write about my Second Brain in the future as well.\nMy system is based on the Zettelkasten method, but I\u0026rsquo;ve adjusted it to my own needs. After a few years of experimentation I feel that my system has reached a \u0026ldquo;mature\u0026rdquo; state, but it will always remain continuosly under development, also because my needs may change.\nWhat is a Zettelkasten? Zettelkasten is a system of note taking and personal knowledge management. Zettelkasten means \u0026ldquo;slip box\u0026rdquo; in German. Traditionally it is a physical box of slips of paper or index cards containing smaller notes. Each of these cards have a unique identifier, and these identifiers are used to create links between the cards.\nNiklas Luhmann, who was one of the most prolific scholars in history, used it extensively. He wrote 70 books and nearly 400 academic articles, and he credited the Zettelkasten with making his productivity possible. It contained around 90,000 index cards.\nI first came across this concept by reading the book How to take Smart Notes by Sönke Ahrens.\nWhy Take Notes? \u0026ldquo;Your professional success and quality of life depend directly on your ability to manage information effectively.\u0026rdquo;\nTiago Forte, Building A Second Brain\nIn the modern age we consume very large amounts of information, much more than we can remember. It is therefore very important to manage your intake of information, but also the retention of that information. I find that taking notes on the topics that may be of interest to me at any given time helps me understand the subject much better, because it forces me to clearly formulate my thoughts and convert them to written form.\nOver time a large collection is built up which can be used to generate new ideas.\nWhat Do I Take Notes On? Firstly, I keep a daily journal where I write down my thoughts, my feelings and the activities that I undertake. Some days may be several pages worth, and other days will be only \u0026ldquo;Had a nice day today\u0026rdquo;. But I make a point to write something down every day.\nSecondly, I write about the topics I study. I\u0026rsquo;m always learning something new. I\u0026rsquo;m a DevOps Cloud Engineer, and new developments are made constantly in the Cloud Native landscape. There is always more to learn.\nMy notes are not only work related. I also keep notes of the research I do for all of my interests, such as health, longevity, exercise, diet, literature, yoga and meditation. I made a habit out of always writing a note when I read a book, watch a documentary or listen to a podcast. These notes can be a short summary, a few thoughts that come to mind, or a few bullet points that capture the main takeaways.\nThese activities are usually accompanied by taking notes:\nReading books Watching video courses Watching educational YouTube videos Researching hobbies and interests Coding Reflection Interesting conversations Listening to podcasts An Overview of My System My system is a modified Zettelkasten system that incorporates daily notes for journaling. The Zettelkasten method emphasizes small atomic notes containing only your own thoughts and links to other notes. However, I choose to also make longer notes. For example, when I did the Fundamentals of Bicep course, I made one large note that contains all of the information I wished to remember or refer to later. In this particular use case, I think it\u0026rsquo;s much better to keep everything collected in one note, rather than breaking this note up in smaller notes on parameters and variables, for example.\nAnother area where I diverge from the traditional Zettelkasten is categorization of notes. Traditionally the Zettelkasten is one big repository of notes, but I like to keep them loosely collected. When I create a new note, my system places that note in an inbox directory. Once a week I go through my inbox and revisit all of my notes. Sometimes I will throw some notes away because I don\u0026rsquo;t think they\u0026rsquo;re necessary after all. When I decide to keep it, I put it in a directory that I find suitable. I have a directory structure based on the PARA method, but I also have a large Zettelkasten directory where I put anything that does not necessary belong to a category. I mostly keep some notes collected in a directory because I expect to use them as a collection at some point, or because I want to run scripts on the files.\nEvery day a daily note is created that contains a few checkboxes for habit tracking and sometimes I\u0026rsquo;ll include an inspirational quote in my template for a while.\nThis is what a daily note looks like in my editor:\nLocal Text Files I think it is important to use locally stored markdown text files. This is mainly why I moved away from Notion. My notes are stored on my local machine rather than with a third party. This gives me complete control over my data.\nBecause my notes are stored as markdown files, I can use various tools to write or edit my notes. Moreover, storing my notes as local files allows me to run scripts on my notes and customize my workflow to my heart\u0026rsquo;s content. This can be especially useful for automating tasks, streamlining my work, or making bulk updates.\nI have my collection backed up in several places and on GitHub.\nI recommend this article about the benefits and merits of using markdown for note-taking.\nVim, My Preferred Text Editor I use Vim, or technically, Neovim for all of my text editing. It took a while to get used to, but my productivity related to text editing has increased significantly. I also find it very enjoyable to work on the command line, not having to leave my tmux window to manage my entire Zettelkasten or keep my journal.\nMy vim configuration is always subject to constant change, but I only use three plugins that are related to my note-taking and Zettelkasten.\n1. Telescope I cannot live without this plugin any longer. It is a fuzzy searcher integrated into my editor. I can search for files but also search within the files using ripgrep, and preview the files as I\u0026rsquo;m searching.\nHeres what a search for Neovim looks like:\nI can navigate through all the search results blazingly fast using only the keyboard, and I can preview the files by scrolling up and down in the window on the right hand side.\n2. Marksman LSP Marksman is a markdown language server which helps me format my documents and to create links between notes.\nWhen I start to make a link it will search through my entire Zettelkasten and look for matches based on the filename or markdown headers. Here it shows me suggestions related to markdown.\n3. Pandoc According to Wikipedia, \u0026ldquo;Pandoc is a free-software document converter, widely used as a writing tool (especially by scholars) and as a basis for publishing workflows.\u0026rdquo;\nI use the Pandoc plugin for vim to export my notes to different formats when I need to.\nHowever, the thing I like most about it is that it renders my notes very nicely as I\u0026rsquo;m writing them. I really like how headings look, and that it will make text bold while editing, which did not work when I used nvim-markdown.\nBash To facilitate my workflow I wrote a few bash scripts.\nWhen I want to take a note, I run the zet command from the command line. This command is a bash script I wrote which creates a file in my inbox, adds a template to it, and opens the file in Neovim. I can either provide a filename to it as an argument or it will ask me to provide one.\nI have another script named blog which I use to create notes that I intend on publishing on my website. These files are stored in a different location and use a different template. The blog script also has a pub function that will publish the script directly to my website in a few seconds after I\u0026rsquo;m done writing.\nThese scripts can be found in my dotfiles repo.\nIdentifiers and Filenames The Zettelkasten method assigns unique identifiers to notes. I generate a unique identifier from the current date and time, but I never really use these identifiers. However, since I\u0026rsquo;ve been doing this consistently from the beginning, I\u0026rsquo;ll keep generating them to make my system future proof. Who knows, maybe they will become useful someday when I suddenly find a use case for it.\nI\u0026rsquo;ve seen other people use unique identifiers in filenames as well, but this is a big no for me. I want to be able to discern what the contents might be from the filename whenever I\u0026rsquo;m navigating around my directory structure. Using the filename as the note identifier is a much better solution in my opinion. There is a small chance that you will choose a filename that already exists, but you can easily add a number to it or change a word and you have solved that problem.\nMoreover, I use the marksman LSP server to create links between my notes in Neovim. The LSP server is based on headings and filenames, so I\u0026rsquo;m really glad that I chose to use filenames such as \u0026ldquo;fundamentals-of-bicep.md\u0026rdquo; right from the beginning.\nTags and Links If you are as deep into note-taking systems as I am, you might have come across the problem of using tags versus using links for your note-taking system.\nI primarily use [[markdown links]] to create links in my notes. This is also why you will sometimes see these things in my public zettelkasten. The links between notes are rendered beatifully by Obsidian in the graph view. This is also why I\u0026rsquo;ll always try to keep my system compatible with Obsidian, even though I\u0026rsquo;ve moved away from the application almost entirely.\nHere is a graph view of my Zettelkasten:\nFor the notes in my Zettelkasten which are intended to be published on my website, I use tags in the YAML frontmatter. These are used by my static website generator, Hugo, to create tag pages on my website which I think will be useful to my readers.\nHere is an example of the YAML front matter of one of these notes:\n--- title: \u0026#34;I\u0026#39;m In Love with my Work: Lessons from a Japanese Sushi Master\u0026#34; date: 2022-10-08 tags: - Career - Personal - Article --- Learn about Zettelkasten If you\u0026rsquo;re interested in learning more about the Zettelkasten Method, I created a course that will teach you everything you need to know about the method.\nWe will build a Zettelkasten in Obsidian together.\nAnd the best part?\nThis Zettelkasten setup is fully compatible with (neo)vim.\nYou are in full control of your files, your data, and your editing experience.\nThis course is my life\u0026rsquo;s work. It\u0026rsquo;s 2000 hours worth of research and practice condensed into 2 hours.\nView the course on Udemy by clicking here\nIt\u0026rsquo;s getting 5 star reviews every day.\nAlternatively, you can get UNLIMITED access to ALL my courses on note-taking, productivity, obsidian, DevOps, Kubernetes and more by joining my Skool community.\nCheck out the community here.\nYou will get direct access to me and the community, and the opportunity to ask questions by joining one of the 3 weekly Q\u0026amp;A calls or by making a post and connecting with fellow note-takers.\nConclusion That\u0026rsquo;s about it. My system is a minimalistic set of tools utilized for maximum productivity. It\u0026rsquo;s Neovim with a few plugins and a couple of helper scripts I wrote in bash that store markdown files in a directory structure that I find meaningful. After using many different applications and solutions, I\u0026rsquo;m extremely satisfied with the system I built for myself which is very basic, free from distractions and tailored to my own needs.\nWhen I want to reflect on a certain topic or to write an article, I go through my collection of notes and link them together. Very often I find that the creation of these links will stimulate even more new connections and associations, and I end up with new ideas and more topics of study I want to explore and write about. I use my Zettelkasten as a vehicle for reflection, learning and creativity.\nI love the simplicity of my system. It enables me to capture notes very quickly while I\u0026rsquo;m working or studying with very little effort. By storing my note collection on my iCloud drive and in GitHub, they can be accessed from all of my devices across all different operating systems.\nI hope that this article may give you some inspiration to start building your own note collection. You don\u0026rsquo;t need to build your own system like I have done, any app that satisfies your needs can be used. Keeping notes on the things I encounter in life is one of the most enriching habits I\u0026rsquo;ve acquired. It will be interesting to see how large my collection grows over the years!\nLinks: 202303270703\nThe Zettelkasten Method by Mischa van den Burg\nMy note-taking \u0026amp; command-line community\nhttps://fortelabs.com/blog/basboverview/\nhttps://fortelabs.com/blog/para/\nhttps://rwx.gg/lang/md/\nhttps://github.com/mischavandenburg/dotfiles/tree/main/scripts\n","permalink":"https://mischavandenburg.com/zet/neovim-zettelkasten/","summary":"I used to make all my notes on paper, but I decided to switch to a digital note-taking system about two years ago.\nDigital note-taking provides the following advantages:\nSearchability Collected in one place Can be converted to different output formats Easier to share with others Does not take up physical space No risk of losing your notes in case of fire or other disaster I\u0026rsquo;ve gone through several iterations of my note-taking systems.","title":"My Neovim Zettelkasten: How I Take Notes in Markdown Using Vim and Bash"},{"content":"I\u0026rsquo;m working through the \u0026ldquo;greet\u0026rdquo; challenge by rwxrob. It is amazing how such a relatively simple and small challenge can lead down to so many rabbit holes.\nThe program should take input from the user and print it out. I worked through the challenge together with Rob in his video but I\u0026rsquo;m going to talk (write) myself through these functions to fully understand what\u0026rsquo;s going on.\nWe have the following function in main.go:\nfunc main() { var name string var err error if len(os.Args) \u0026gt; 1 { name = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } if name == \u0026#34;\u0026#34; { fmt.Println(\u0026#34;Hello there, what\u0026#39;s your name?\u0026#34;) name, err = internal.ReadLine(os.Stdin) if err != nil { log.Print(err) return } } greet.Hi(name) } If the number of arguments passed to the program is greater than 1, we set name to a joined string created from the provided arguments. Args[0] would be the path to the program, so we don\u0026rsquo;t want that to be included. As a result, greet mischa will pass mischa to the greet.Hi() function, as defined in greet.go, and print out Hello, mischa.\nHowever, if no arguments are passed to the greet program, discovered in case name is empty, we ask the user for input. We capture the input by os.Stdin and pass it to the ReadLine() function, which is located at internal/readline.go.\n// ReadLine takes any io.Reader and returns a trimmed string (initial // and trailing white space) or an empty string and error if any error // is encountered. func ReadLine(in io.Reader) (string, error) { out, err := bufio.NewReader(in).ReadString(\u0026#39;\\n\u0026#39;) out = strings.TrimSpace(out) return out, err } } ReadLine has a parameter in of type io.Reader, which is an interface. Next, we determine that ReadLine should return two values of type string and error. I\u0026rsquo;m saving to learn about interfaces for another day, I\u0026rsquo;m just going to work through this function now.\nWe assign the output of bufio.NewReader(in).ReadString('\\n') to two new variables named out and err using the \u0026ldquo;walrus operator\u0026rdquo; := which detects the types automatically. We can do it like this because ReadString returns (string, error).\nWe take the in argument of type io.Reader which was passed to the ReadLine function, which in this case is the io.Stdin that came from our main function, and pass it on to bufio.NewReader(in). Then we are able to read the string until the newline character \\n in the string, and trim off the whitespace from the beginning and the end of the string by calling TrimSpace on the out variable.\nThen we return the trimmed string back to our original main function, which will pass it on to the Hi() function.\nHowever, if the bufio.NewReader(in).ReadString('\\n') should return an error, it is caught by this code in the main function:\nif err != nil { log.Print(err) return } This is a standard way of handling errors in Go. If the error is anything else than nil, we will print the error and end the function with the return keyword.\nThoughts I\u0026rsquo;m really glad I took the time to talk / write myself through this program. I think I\u0026rsquo;m going to make a habit of this as I\u0026rsquo;m learning Go. It made everything much clearer when I sat down and traced the arguments from function to function and describing every step in my own words.\nI\u0026rsquo;ll post this note in the YouTube comments, maybe somebody will find it useful as well.\nLinks: This page goes a lot deeper in what stdin and stdout actually do in this context. Very interesting reading:\nhttps://stackoverflow.com/questions/12363030/read-from-initial-stdin-in-go\nThe code in my repo:\nhttps://github.com/mischavandenburg/go/tree/main/rwxrob/boost2022/challenges/greet\n[[go]]\n[[go-rwxrob]]\n[[coding]]\n[[functions]]\n","permalink":"https://mischavandenburg.com/zet/go-reading-stdinput-cmdline/","summary":"I\u0026rsquo;m working through the \u0026ldquo;greet\u0026rdquo; challenge by rwxrob. It is amazing how such a relatively simple and small challenge can lead down to so many rabbit holes.\nThe program should take input from the user and print it out. I worked through the challenge together with Rob in his video but I\u0026rsquo;m going to talk (write) myself through these functions to fully understand what\u0026rsquo;s going on.\nWe have the following function in main.","title":"Go - Reading from Standard Input Provided by User"},{"content":"Beginner Boost Week 17 and 18 Notes Link to video\nDon\u0026rsquo;t forget to set GOBIN=~/.local/bin, GOPRIVATE, CGO_ENABLED=0 Go Testing - Example Tests func ExampleFoo() { foo() // Output: // Foo } The ExampleFoo indicates the test here. It needs to match the name exactly after Example. But it is capitalized.\nIt runs that function and will compare the output to what is specified.\nIt says \u0026ldquo;see if the program generates this output in std out\u0026rdquo;.\nThese are called example tests.\nTo export a function the first letter should be a capital. Everything that has a capital as first letter is exported.\nWhen you are writing example tests you are providing readable automatic documentation to your end users.\nYou can use // Unordered Output if you don\u0026rsquo;t care about the order.\nPrintf use printf %q to see all of the characters. %q escapes all of the invisible characters. So you can see \\r or \\n for example. This is the only way to check for empty values in Example tests.\nHow to Learn Go Don\u0026rsquo;t go to the books. Stay with the spec, write your own projects, and find one or two or 10 Go projects and study the crap out of them.\nStudy the code bases. Kind is a good codebase.\nRead the Go codebase!\nhttps://youtu.be/9hEnzD-bNy4?t=8467\nRead other people\u0026rsquo;s code and see if your code looks like that.\n\u0026ldquo;Good artists copy, great artists steal.\u0026rdquo; - quote accredited to Steve Jobs but it has its origin in T.S. Eliot and even further back.\nStart with the tests! Read the tests first.\nSearching Go Documentation Always search for golang when searching on the net. Preferably text based searching.\nThen you use go doc os.Stderr - for example.\nVariable Names Long variable names are frowned upon in the Go community. They should be as short as they need to be.\nIf using in a tight scope, a single letter is fine. In a block two or three characters is more than enough. It distracts from reading the code if they have very long names.\nThe more remote the variable is, the more descriptive it should be.\nThe Art of Coding breaking everything down into small 1 task functions don\u0026rsquo;t repeat yourself See stuttering\nTop Level Libraries It\u0026rsquo;s not common to make a single utility at a top-level GitHub repo.\nIt\u0026rsquo;s very likely that you are going to want to reuse code somewhere.\nYou should start thinking of things as composition. \u0026ldquo;How is this code going to be used by other people\u0026rdquo;.\nGo is different than bash: you will use this code elsewhere.\nWhat is the function of what I\u0026rsquo;m creating? Create these as small composable blocks So you can use them later Example Based Testing https://pkg.go.dev/testing\nThe package also runs and verifies example code. Example functions may include a concluding line comment that begins with \u0026ldquo;Output:\u0026rdquo; and is compared with the standard output of the function when the tests are run. (The comparison ignores leading and trailing space.) These are examples of an example:\nfunc ExampleHello() { fmt.Println(\u0026#34;hello\u0026#34;) // Output: hello } func ExampleSalutations() { fmt.Println(\u0026#34;hello, and\u0026#34;) fmt.Println(\u0026#34;goodbye\u0026#34;) // Output: // hello, and // goodbye } Searching Information Being able to look up information quickly and taking notes about them is just as important as the coding itself.\nPackages or Libraries You can either have a command, which will be a main package, or you can have an importable library named .go. Should have any other name besides main.\nYou rarely want your top level of your module to be a package.\nConvention: cmd directory.\nThis is specifically for separate commands.\nStuttering greet.Greet() is called stuttering in go. Don\u0026rsquo;t do this.\nModules A module can be defined as \u0026ldquo;a GitHub repo with a go.mod\u0026rdquo; in it.\n\u0026ldquo;A collection of Go packages stored in a file tree with a go.mod file at its root\u0026rdquo;\nThe go.mod contains the import path of the module.\nWhat Module Does Greet Belong To? In the case of our greet command, it belongs to the greet module located at:\ngithub.com/mischavandenburg/go/rwxrob/boost2022/challenges/greet\nGreet has a main package which is part of the greet module.\nNow we will add another command with another main package to this module.\nCLI Stuff that has to do with interacting with the user on the command line should never be in the package library. The package library should be written to be solid no matter what.\nThe \u0026ldquo;name\u0026rdquo; for our greet program should be able to come to anywhere.\nAvoid Interactive Input You generally want to avoid interactive input. Prefer arguments, env variables or file inputs.\nIf you call your program from another script it will stall if it waits for interactive input.\nA good example is forgetting \u0026ldquo;-y\u0026rdquo; when running apt-get.\nAn interactive story game is a different use case than a CLI tool.\nUNIX filters are specifically designed to read input and generate an output based on the input. Here it is expected behaviour to stall if no input is given.\nRunes A rune is a single Unicode point.\nThe same people who created Go, created the Unicode standard.\nGo has the best Unicode support.\nWriting Documentation Convention is to always start with the name of the thing you are documenting. When documenting a function, always start with the function name. For example // ReadLine reads from standard input\nUse go doc --all to see a print of all the documentation in your package.\nThe same goes for packages. Always start with \u0026ldquo;Package internal does bla bla\u0026rdquo;\nLinks 202303261403\nhttps://www.youtube.com/watch?v=WMH5ENF_Xvo\n[[go]]\n[[coding]]\n[[rwxrob]]\n","permalink":"https://mischavandenburg.com/zet/go-rwxrob-boost-week-17-18/","summary":"Beginner Boost Week 17 and 18 Notes Link to video\nDon\u0026rsquo;t forget to set GOBIN=~/.local/bin, GOPRIVATE, CGO_ENABLED=0 Go Testing - Example Tests func ExampleFoo() { foo() // Output: // Foo } The ExampleFoo indicates the test here. It needs to match the name exactly after Example. But it is capitalized.\nIt runs that function and will compare the output to what is specified.\nIt says \u0026ldquo;see if the program generates this output in std out\u0026rdquo;.","title":"Go - Skillstak Beginner Boost Week 17 and 18 Notes"},{"content":"To read the news free from distractions and ads I use Newsboat as a reader for RSS feeds.\nHowever, one thing that annoyed me was that it would span across my entire screen in the terminal. When you read blogs or news pages in the browser, you\u0026rsquo;ll notice that the text is always located in a middle column of the window, so you don\u0026rsquo;t have to move your neck while reading. At least, this is the case with well designed websites that serve text content.\nVim has a plugin that achieves this and I use it extensively. It is called Zenmode. In Neovim I use a similar plugin called No Neckpain.\nTo achieve a similar configuration for Newsboat I used tmux. I wrote the following bash script:\n#!/bin/bash # nb opens a new pane and runs newsboat in it. I want to read from a centered column in my screen. tmux split-pane -h \\; resize-pane -x 130\\; send -t 2 \u0026#34;newsboat\u0026#34; Enter\\; send -t 1 \u0026#34;clear\u0026#34; Enter Note that this needs to be run from within an existing tmux window with no split panes.\nIt splits the current window in to two panes, resizes the new pane to a width of 130 pixels and sends the \u0026ldquo;newsboat\u0026rdquo; command to the new pane, and the \u0026ldquo;clear\u0026rdquo; command to the old (left) pane to keep it nice and clean.\nIn my ~/.newsboat/config file I added the following setting:\ntext-width 72\nThis will limit the text on the right hand side of the screen.\nThe end result looks like this:\nLinks: 202303260903\nhttps://github.com/folke/zen-mode.nvim\nhttps://github.com/shortcuts/no-neck-pain.nvim\n","permalink":"https://mischavandenburg.com/zet/newsboat-in-zenmode/","summary":"To read the news free from distractions and ads I use Newsboat as a reader for RSS feeds.\nHowever, one thing that annoyed me was that it would span across my entire screen in the terminal. When you read blogs or news pages in the browser, you\u0026rsquo;ll notice that the text is always located in a middle column of the window, so you don\u0026rsquo;t have to move your neck while reading.","title":"How to Run Newsboat in Zenmode"},{"content":"I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.\nCloud These are primarily cloud services. The external cloud.\n\u0026ldquo;Something as a Service\u0026rdquo;.\nAmazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape\nCloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.\nComputing Edge Computing\nHigh Performance Computing\nEncapsulates all of the technologies that are involved with containerization of work, jobs and nodes\nDeployment of compute resources as nodes\nThis is why Google\u0026rsquo;s Borg was called Borg\nComputers are drones of a larger collective\nEvery node puts all the resources into the collective.\nThe collective is all the nodes combined, and Kubernetes is the Borg that orchestrates everything. It sees available resources and allocates the work that needs to be done.\nBorg is the internal system developed at Google to run their infrastructure. You can read about it in the Site Reliability Engineering books and I highly recommend them.\nKubernetes is /proc for the cloud\nRob Muhlenstein\nMost Important Technologies Docker, Dockerfiles\nKubernetes\nHelm\nHarbor\nDifferent registries, harbor, quay\nIt is a lot of Python and POSIX shell\nGo for infrastructure application development\nKubernetes and Helm have won the game\nContainers: Size Matters Size matters (again) in the cloud The smaller your container the better, because it takes less resources and less costs DevOps DevOps is not the same as Cloud Native. It is one piece of it, a specific set of practices and actions that can be done within Cloud Native.\nHow you write software and release it CI/CD Focused on getting the software out GitLab has become the one stop shop Purpose is to write software and get it published fast GitOps Summary In summary, \u0026ldquo;cloud\u0026rdquo; stands for the services offered by cloud providers such as AWS, Azure and GCP. Cloud Native stands for all of the technology that makes these cloud services possible. DevOps is part of Cloud Native, but definitely not the same thing. DevOps is concerned with how software is written and released.\nLinks: 202303262003\nhttps://youtu.be/gyjRriOyw-k\nhttps://landscape.cncf.io/\nhttps://sre.google/books/\n[[rwxrob]]\n[[devops]]\n[[Kubernetes]]\n[[Cloud Native]]\n[[cncf]]\n","permalink":"https://mischavandenburg.com/zet/cloud-cloudnative-devops/","summary":"I found an excellent video by Rob Muhlenstein explaining the differences between Cloud, Cloud Native and DevOps. Here are the notes I wrote.\nCloud These are primarily cloud services. The external cloud.\n\u0026ldquo;Something as a Service\u0026rdquo;.\nAmazon Azure GCP Cloud Native This is Cloud Native: The CNCF Landscape\nCloud Native is the technology that makes the cloud possible, and all the technology dependent on those services.\nComputing Edge Computing\nHigh Performance Computing","title":"The Difference Between DevOps, Cloud and Cloud Native"},{"content":"It happens that I want to share my notes with friends that just simply want a pdf instead of a markdown file.\nThis morning I figured out a quick way to convert markdown to pdf on a M2 MacBook running MacOS Ventura.\nYou need the pandoc and wkhtmltopdf packages.\nbrew install pandoc wkhtmltopdf To convert:\npandoc 00-zettelkasten/Fundamentals\\ of\\ Bicep.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf This will output a pdf to my /tmp/ directory and it looks pretty good.\nTo convert all markdown files in a directory you can use a wildcard:\npandoc 00-zettelkasten/*.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf Fun fact: converting my entire zettelkasten took a few seconds and generated a document of 80 pages with a font size of 10. Pretty fun to see it in a more regular format instead of the terminal.\npandoc Switched to pandoc for my vim markdown plugin and I\u0026rsquo;m starting to like it a lot. I struggled a bit to get a working setup because of all the folding etc. But I figured it out and found out that rwxrob also hates folding haha.\nhttps://www.youtube.com/watch?v=3NiBr3j-vMw https://www.youtube.com/watch?v=Efk2M77naFU\u0026amp;list=TLPQMjQwMzIwMjPleFLT9W4o_Q\u0026amp;index=3 https://www.youtube.com/watch?v=3NiBr3j-vMw\u0026amp;list=TLPQMjQwMzIwMjPleFLT9W4o_Q\u0026amp;index=1\nsearch rwxrobs channel for markdown\n","permalink":"https://mischavandenburg.com/zet/pandoc-markdown-pdf-macos/","summary":"It happens that I want to share my notes with friends that just simply want a pdf instead of a markdown file.\nThis morning I figured out a quick way to convert markdown to pdf on a M2 MacBook running MacOS Ventura.\nYou need the pandoc and wkhtmltopdf packages.\nbrew install pandoc wkhtmltopdf To convert:\npandoc 00-zettelkasten/Fundamentals\\ of\\ Bicep.md --pdf-engine=wkhtmltopdf -o /tmp/test.pdf This will output a pdf to my /tmp/ directory and it looks pretty good.","title":"Converting markdown to PDF on MacOS"},{"content":"Yesterday I reached a significant milestone. I ran my first 5K!\nI\u0026rsquo;ve always been on the heavy side. I did a lot of weightlifting from age 18 to around 27 and I was always rather bulky, but very strong. This meant I was very good at working in the woods or helping people move house, but I\u0026rsquo;ve never been able to do any running or endurance because my knees would start protesting very quickly. However, I always admired endurance athletes and I had the desire to be able to run more than a few minutes without feeling the urge to die on the spot.\nOver the past 1,5 years I lost 20kg (~44 pounds) so I decided to give it yet another try.\nI\u0026rsquo;ve never been this lightweight so I decided to give running another try. I started small with only 10 minutes but over the course of two months I built it up gradually.\nLast week I ran 4km and it felt pretty good, so I just decided to push a little bit more and try for my goal of 5k, and it went well! It feels like quite an accomplishment to me.\nNow that I lost all this weight running actually feels good, and it is liberating to discover a whole new area of exercise that I haven\u0026rsquo;t explored yet: endurance. Looking back to my progress over a couple of months, I\u0026rsquo;m curious how far I\u0026rsquo;ll come and what my limits are.\n","permalink":"https://mischavandenburg.com/zet/ran-5km/","summary":"Yesterday I reached a significant milestone. I ran my first 5K!\nI\u0026rsquo;ve always been on the heavy side. I did a lot of weightlifting from age 18 to around 27 and I was always rather bulky, but very strong. This meant I was very good at working in the woods or helping people move house, but I\u0026rsquo;ve never been able to do any running or endurance because my knees would start protesting very quickly.","title":"I ran 5K for the first time in my life!"},{"content":"Did some website housekeeping today. Spent the entire morning on a few tasks that I intended to do for a while. I added a search page and reorganized the menu, and I added a \u0026ldquo;Start Here\u0026rdquo; page.\nI hope that the new \u0026ldquo;Start Here\u0026rdquo; page will do a better job of explaining the how and why of my website, and that the search function will help you nagivate my website better.\n","permalink":"https://mischavandenburg.com/zet/website-housekeeping/","summary":"Did some website housekeeping today. Spent the entire morning on a few tasks that I intended to do for a while. I added a search page and reorganized the menu, and I added a \u0026ldquo;Start Here\u0026rdquo; page.\nI hope that the new \u0026ldquo;Start Here\u0026rdquo; page will do a better job of explaining the how and why of my website, and that the search function will help you nagivate my website better.","title":"Made Some Website Improvements"},{"content":"I do all of my writing in markdown. I keep my diaries in my Second Brain, I constantly write notes on the topics I\u0026rsquo;m studying, and my entire blog is written in markdown.\nSome of the writing is done in Obsidian but I\u0026rsquo;m moving away from Obsidian step by step. I\u0026rsquo;ll still keep my second brain compatible with it, but I want to be able to do all of my writing in (neo)vim.\nI\u0026rsquo;ve been using nvim-markdown for a while now and I was quite happy with it, but it was bothering me that bold text was not rendered in my editor.\nPandoc had been on my radar for a while because it uses the Commonmark markdown spec and it can be used to convert to many different documentation types. It\u0026rsquo;s an interesting thought to keep my entire collection of notes and writings compatible with something that can be converted into anything.\nI noticed rwxrob uses pandoc as one of his few vim plugins.\nIt took me an hour to make it compatible with neovim and to disable the spelling and folding. I couldn\u0026rsquo;t just set the provided variables because they are still in vimscript and I haven\u0026rsquo;t figured out how to set them yet because they use a hashtag in their name #\nSo far I like it, but there are a few things that I miss from my nvim-markdown\nautomatic bulleted list nested bulleted lists with nvim-markdown these lists would continue Here I have to indent each line myself and add a bullet navigation between headers I didn\u0026rsquo;t use this much to be honest, but I haven\u0026rsquo;t figured it out in pandoc yet. Maybe it is possible? I find myself using bullete lists and nested lists a lot in my notes, but I wonder if it is better to step away from those and make more use of markdown headings.\n","permalink":"https://mischavandenburg.com/zet/trying-out-pandoc/","summary":"I do all of my writing in markdown. I keep my diaries in my Second Brain, I constantly write notes on the topics I\u0026rsquo;m studying, and my entire blog is written in markdown.\nSome of the writing is done in Obsidian but I\u0026rsquo;m moving away from Obsidian step by step. I\u0026rsquo;ll still keep my second brain compatible with it, but I want to be able to do all of my writing in (neo)vim.","title":"Trying out pandoc for vim"},{"content":"[[Neovim]]\nI find myself quoting words very often in vim when I\u0026rsquo;m writing bash code. I used to do this by simply navigating around the word and typing them, but I knew there had to be a better way.\nI found this vim command:\nciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P\n\u0026ldquo;c\u0026rdquo; deletes into register and enters insert mode. \u0026ldquo;iw\u0026rdquo; stands for \u0026ldquo;inner word\u0026rdquo; and selects the word.\nSo we delete the entire word and enter insert mode. Then we type two quotes, and we press \u0026ldquo;P\u0026rdquo; to paste the register (containing the word) before the cursor.\nVoila, the word is surrounded by quotes.\nTo make it even easier, I added this to my keymaps, and I\u0026rsquo;ll add a few more for parentheses and brackets.\nvim.keymap.set(\u0026quot;n\u0026quot;, \u0026quot;\u0026lt;leader\u0026gt;wsq\u0026quot;, 'ciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P', { desc = \u0026quot;Word Surround Quotes\u0026quot; })\nhttps://vi.stackexchange.com/questions/21113/vimscript-surround-word-under-cursor-with-quotes\n","permalink":"https://mischavandenburg.com/zet/surround-word-quotes-neovim/","summary":"[[Neovim]]\nI find myself quoting words very often in vim when I\u0026rsquo;m writing bash code. I used to do this by simply navigating around the word and typing them, but I knew there had to be a better way.\nI found this vim command:\nciw\u0026quot;\u0026quot;\u0026lt;Esc\u0026gt;P\n\u0026ldquo;c\u0026rdquo; deletes into register and enters insert mode. \u0026ldquo;iw\u0026rdquo; stands for \u0026ldquo;inner word\u0026rdquo; and selects the word.\nSo we delete the entire word and enter insert mode.","title":"How to Surround a Word with Quotes in Vim"},{"content":"A module is generally associated with a single git repo.\nYou can have a module with multiple packages, and each package would get its own subdirectory.\nYou should always name your main file main.go\ncreating a module Use the go mod init {{your path here}} command to initiate a module.\nmultiple modules I was running into some trouble with this because I want to have one big repo where I will store all my go projects.\nMy gopls LSP in Neovim would start throwing errors when I added multiple projects in my repo.\nThe fix is to create a separate directory for each project. For example:\n/go/hello/main.go\n/go/hi/main.go\nHello and Hi are each separate projects.\nNow I enter each of these directories and run go mod init hello\nI\u0026rsquo;m sure this isn\u0026rsquo;t good practice for production code, but it serves its purpose to collect all my learning code in one place.\nhttps://www.youtube.com/watch?v=9hEnzD-bNy4\n","permalink":"https://mischavandenburg.com/zet/package-module-go/","summary":"A module is generally associated with a single git repo.\nYou can have a module with multiple packages, and each package would get its own subdirectory.\nYou should always name your main file main.go\ncreating a module Use the go mod init {{your path here}} command to initiate a module.\nmultiple modules I was running into some trouble with this because I want to have one big repo where I will store all my go projects.","title":"What is the difference between a Go module and a package?"},{"content":"Today was meal-prepping day and I cut up some vegetables for the coming week. I’m on a strict caloric restriction regimen and need to meticulously track and plan all the food that I consume.\nI cut up three kinds of vegetables and wrote down how many grams of each I cut up so I could divide them by three and add them to my calorie tracking application.\nAs I took out my phone to pick up my calculator to divide each number, my inner engineer started complaining about the fact that I had to do three calculations and that it would be much better to loop over an array of these values.\nI had my laptop nearby with a terminal open and wrote this instead:\nfor i in 287 252 321; do echo \u0026#34;$i / 3\u0026#34; | bc; done 95 84 107 After turning my passion into my career, I love the fact that I’m starting to think like an engineer in all my other areas of life. I’m also happy to see my progress in command line work. I chose to go the hard way, using bash instead of zsh and doing all my editing in vim and using Linux as much as possible, and it is paying off because I don’t have to think much about these little operations anymore.\nThen again, I would be much better off training my brain to become better at doing math without the aid of paper or calculators 🤓\nIf you\u0026rsquo;re curious about what a shokunin is, check out this article I wrote: https://mischavandenburg.com/zet/articles/jiro-sushi/\n","permalink":"https://mischavandenburg.com/zet/cli-shokunin-moment/","summary":"Today was meal-prepping day and I cut up some vegetables for the coming week. I’m on a strict caloric restriction regimen and need to meticulously track and plan all the food that I consume.\nI cut up three kinds of vegetables and wrote down how many grams of each I cut up so I could divide them by three and add them to my calorie tracking application.\nAs I took out my phone to pick up my calculator to divide each number, my inner engineer started complaining about the fact that I had to do three calculations and that it would be much better to loop over an array of these values.","title":"Had a CLI Shokunin Moment Today"},{"content":"I do all my coding and note taking in the terminal using tmux and neovim. I picked up a nice trick from Rob Muhlenstein today.\nYou can use this command in a split window to keep running a Go file. It will update when you save the file.\nentr -c bash -c \u0026quot;go run main.go\u0026quot; \u0026lt;\u0026lt;\u0026lt; main.go\nEntr runs commands when files change. Here we are feeding it only one file, but you can also feed it a directory like so:\nfind src/ | entr -s 'make'\nSuper handy to see the outcome of your code changes in real time.\nTo run all the files in the directory, use the following:\nentr -c bash -c \u0026quot;go run . \u0026quot; \u0026lt; \u0026lt;(find .)\nI picked this up while going through Rob\u0026rsquo;s Beginner Boost of 2022:\nhttps://youtu.be/kwrN3jbv4sE\n","permalink":"https://mischavandenburg.com/zet/running-go-on-change/","summary":"I do all my coding and note taking in the terminal using tmux and neovim. I picked up a nice trick from Rob Muhlenstein today.\nYou can use this command in a split window to keep running a Go file. It will update when you save the file.\nentr -c bash -c \u0026quot;go run main.go\u0026quot; \u0026lt;\u0026lt;\u0026lt; main.go\nEntr runs commands when files change. Here we are feeding it only one file, but you can also feed it a directory like so:","title":"How to continuously run a Go file while coding in the terminal"},{"content":"Deploying to subscriptions and management groups To tell Bicep which scope to deploy to, use the targetScope keyword, for example, managementGroup.\nYou\u0026rsquo;re not specifying which management group exactly, this is done during deployment of the template file.\ntargetScope can be set to resourceGroup, subscription, managementGroup or tenant.\nIf it is not set, Bicep assumes resourceGroup.\ncreate a resource group targetScope = \u0026#39;subscription\u0026#39; resource resourceGroup \u0026#39;Microsoft.Resources/resourceGroups@2021-01-01\u0026#39; = { name: \u0026#39;example-resource-group\u0026#39; location: \u0026#39;westus\u0026#39; } To deploy you use az deployment group create for resource groups, but you use az deployment sub create for subscriptions, mg for management group and tenant for tenant.\ndeployment scripts deploymentScripts resources are either PowerShell or Bash scripts that run in a Docker container as part of your template deployment. The default container images have either the Azure CLI or Azure PowerShell available. These scripts run during the processing of the ARM template, so you can add custom behavior to the deployment process.\nHere is an example of a deployment script with some comments:\nresource myFirstDeploymentScript \u0026#39;Microsoft.Resources/deploymentScripts@2020-10-01\u0026#39; = { name: \u0026#39;myFirstDeploymentScript\u0026#39; location: resourceGroup().location // can be AzurePowershell or Azure CLI kind: \u0026#39;AzurePowerShell\u0026#39; // the script will be run in a container. We need to provide a Managed Identity to give the script the required permissions identity: { type: \u0026#39;UserAssigned\u0026#39; userAssignedIdentities: { \u0026#39;/subscriptions/01234567-89AB-CDEF-0123-456789ABCDEF/resourcegroups/deploymenttest/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myscriptingid\u0026#39;: {} } } properties: { azPowerShellVersion: \u0026#39;3.0\u0026#39; // in Bicep we use \u0026#39;\u0026#39;\u0026#39; to indicate a multi line string scriptContent: \u0026#39;\u0026#39;\u0026#39; $output = \u0026#39;Hello Learner!\u0026#39; Write-Output $output // the $DeploymentScriptOutputs variable is created to return output back to the Bicep template // It needs to be a hash table $DeploymentScriptOutputs = @{} $DeploymentScriptOutputs[\u0026#39;text\u0026#39;] = $output \u0026#39;\u0026#39;\u0026#39; retentionInterval: \u0026#39;P1D\u0026#39; } } output scriptResult string = myFirstDeploymentScript.properties.outputs.text You can also write deployment scripts in Bash. To create outputs from a Bash script, you need to create a JSON file in a location specified by the AZ_SCRIPTS_OUTPUT_PATH environment variable.\nTo include a script file, use the following:\nproperties: { azPowerShellVersion: \u0026#39;3.0\u0026#39; scriptContent: loadTextContent(\u0026#39;myscript.ps1\u0026#39;) retentionInterval: \u0026#39;P1D\u0026#39; } deploying a managed identity and assigning a role var userAssignedIdentityName = \u0026#39;configDeployer\u0026#39; var roleAssignmentName = guid(resourceGroup().id, \u0026#39;contributor\u0026#39;) var contributorRoleDefinitionId = resourceId(\u0026#39;Microsoft.Authorization/roleDefinitions\u0026#39;, \u0026#39;b24988ac-6180-42a0-ab88-20f7382dd24c\u0026#39;) resource userAssignedIdentity \u0026#39;Microsoft.ManagedIdentity/userAssignedIdentities@2018-11-30\u0026#39; = { name: userAssignedIdentityName location: resourceGroup().location } resource roleAssignment \u0026#39;Microsoft.Authorization/roleAssignments@2020-04-01-preview\u0026#39; = { name: roleAssignmentName properties: { roleDefinitionId: contributorRoleDefinitionId principalId: userAssignedIdentity.properties.principalId principalType: \u0026#39;ServicePrincipal\u0026#39; } } template specs When you have a lot of reusable templates, you can use Template Specs to enable your entire organization to deploy them.\nYou can convert a Bicep file to a template spec. The template spec is then deployed to Azure as a resource, and anybody with the right access and do deployments with the template spec from the portal or Azure CLI. Azure will handle the version control.\nYou will lose any comments and whitespace.\nBicep modules are intended to be combined into larger deployments. Template specs are for sets of resources with a certain configuration.\nTemplate specs can be used as a Bicep module. You use the following code to import it:\nmodule storageAccountTemplateSpec \u0026#39;ts:f0750bbe-ea75-4ae5-b24d-a92ca601da2c/sharedTemplates/StorageWithoutSAS:1.0\u0026#39; = { name: \u0026#39;storageAccountTemplateSpec\u0026#39; } ","permalink":"https://mischavandenburg.com/zet/advanced-bicep/","summary":"Deploying to subscriptions and management groups To tell Bicep which scope to deploy to, use the targetScope keyword, for example, managementGroup.\nYou\u0026rsquo;re not specifying which management group exactly, this is done during deployment of the template file.\ntargetScope can be set to resourceGroup, subscription, managementGroup or tenant.\nIf it is not set, Bicep assumes resourceGroup.\ncreate a resource group targetScope = \u0026#39;subscription\u0026#39; resource resourceGroup \u0026#39;Microsoft.Resources/resourceGroups@2021-01-01\u0026#39; = { name: \u0026#39;example-resource-group\u0026#39; location: \u0026#39;westus\u0026#39; } To deploy you use az deployment group create for resource groups, but you use az deployment sub create for subscriptions, mg for management group and tenant for tenant.","title":"Notes: Advanced Bicep"},{"content":"The past few days I\u0026rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.\nUnfortunately you can\u0026rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.\nMinikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven\u0026rsquo;t tried any of the other alternatives because I found something better.\nRancher Desktop provides everything that you need. It sets up a local VM where it will run a Kubernetes cluster using k3s. It will configure the containerd container engine for you which you can interact with using nerdctl.\nTo install:\nbrew install rancher #after installing rancher, start it up and wait for it to boot the VM. alias docker=nerdctl docker run hello-world And you\u0026rsquo;re good to go. Rancher will add the rancher-desktop to your kube context.\nTo test your Kubernetes cluster:\nk get pods k get nodes # test running a pod k run nginx --image=nginx k expose pod nginx --port=80 --type=NodePort # inspect your services and look for 80:31066/TCP under PORT(S) k get svc curl localhost:31066 Or visit localhost:31066 in your browser. Replace 31066 with the port you found listed under your services.\n","permalink":"https://mischavandenburg.com/zet/docker-kubernetes-on-mac-m2/","summary":"The past few days I\u0026rsquo;ve been trying out a few options to run Docker containers and a Kubernetes clusters on my new MacBook Pro M2.\nUnfortunately you can\u0026rsquo;t just run brew install docker and expect it to work. Additionally, Docker desktop requires that you purchase a license if you use it for work purposes.\nMinikube works fine as well, but the networking driver for qemu is not fully supported yet, and I haven\u0026rsquo;t tried any of the other alternatives because I found something better.","title":"Running Docker and Kubernetes on Mac M2"},{"content":"Today I finished the Intermediate Bicep module. Here are my notes.\nChild and Extension You can also use Bicep to refer to resources that were created outside the Bicep file itself. For example, you can refer to resources that your colleagues have created manually by using the Azure portal, or from within another Bicep template or module, even if they\u0026rsquo;re in a different resource group or subscription. By using these features of Bicep, you can unlock the ability to create powerful templates that deploy all aspects of your Azure infrastructure.\nSome resources are only deployed in context of their parent. For example:\nVirtual network subnets Microsoft.Network/virtualNetworks/subnets App Service configuration Microsoft.Web/sites/config SQL databases Microsoft.Sql/servers/databases Virtual machine extensions Microsoft.Compute/virtualMachines/extensions Storage blob containers Microsoft.Storage/storageAccounts/blobServices/containers Azure Cosmos DB containers\nIt does not make sense for a container to exist without a storage account.\nDifference between child and extension In summary, you define extensions with the scope keyword, and child resources are defined by nesting them or by using the parent keyword.\nAn extension resource is a resource that modifies another resource. For example, assigning a role to a resource.\nA child resource is a resource that exists only within the context of another resource, such as a subnet existing only within a vnet.\nNested resource resource vm \u0026#39;Microsoft.Compute/virtualMachines@2020-06-01\u0026#39; = { name: vmName location: location properties: { // ... } resource installCustomScriptExtension \u0026#39;extensions\u0026#39; = { name: \u0026#39;InstallCustomScript\u0026#39; location: location properties: { // ... } } } Here the extension resource is within the vm resource. The fully qualified domain name is Microsoft.Compute/virtualMachines/extensions, but it is not necessary because it inherits it from the parent. Therefore we only need to specify \u0026rsquo;extensions\u0026rsquo; here.\nNo API version is specified either, this is also inherited.\nYou can refer to a nested resource by using the :: operator. For example, you could create an output that will return the full resource ID of the extension:\noutput childResourceId string = vm::installCustomScriptExtension.id Parent property This is the second way to declare a child resource.\nresource vm \u0026#39;Microsoft.Compute/virtualMachines@2020-06-01\u0026#39; = { name: vmName location: location properties: { // ... } } resource installCustomScriptExtension \u0026#39;Microsoft.Compute/virtualMachines/extensions@2020-06-01\u0026#39; = { parent: vm name: \u0026#39;InstallCustomScript\u0026#39; location: location properties: { // ... } } dependsOn use dependsOn to indicate a dependency.\ndependsOn: [ vm ] Extension resources Extension resources are always attached to other Azure resources. They extend them with extra functionality. Some examples are role assignments, locks, and policy assignments.\nIt doesn\u0026rsquo;t make sense to deploy a lock by itself. It always has to be deployed to another resource, because it prevents deletion or modification of a resource.\nResources are defined almost the same way as normal resources, but you add the scope property to tell Bicep that it is attached to another resource in the template.\nresource cosmosDBAccount \u0026#39;Microsoft.DocumentDB/databaseAccounts@2020-04-01\u0026#39; = { name: cosmosDBAccountName location: location properties: { // ... } } resource lockResource \u0026#39;Microsoft.Authorization/locks@2016-09-01\u0026#39; = { scope: cosmosDBAccount name: \u0026#39;DontDelete\u0026#39; properties: { level: \u0026#39;CanNotDelete\u0026#39; notes: \u0026#39;Prevents deletion of the toy data Cosmos DB account.\u0026#39; } } Extensions have slightly different resource ID\u0026rsquo;s. They consist of the parent resource ID, the separator /providers/, and the extension resource ID.\nIf you see a resource ID that starts with a normal resource ID and then adds /providers/ and another resource type and name, it means that you\u0026rsquo;re looking at an extension resource ID.\nExisting resources Bicep files often need to refer to resources that have been already created elswewhere. They might have been created in the portal or by another Bicep file.\nHere you use the existing keyword in Bicep. You are defining a resource that already exists, and therefore you are telling Bicep that it shouldn\u0026rsquo;t try to deploy it. Think of it as a placeholder resource.\nYou can do the same for nested or child resources.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: \u0026#39;toydesigndocs\u0026#39; } Existing resources outside of the resource group and subscription resource vnet \u0026#39;Microsoft.Network/virtualNetworks@2020-11-01\u0026#39; existing = { scope: resourceGroup(\u0026#39;f0750bbe-ea75-4ae5-b24d-a92ca601da2c\u0026#39;, \u0026#39;networking-rg\u0026#39;) name: \u0026#39;toy-design-vnet\u0026#39; } You can refer to these as long as they are within your Azure AD tenant.\nAdd child and extension resources to an existing resource resource server \u0026#39;Microsoft.Sql/servers@2020-11-01-preview\u0026#39; existing = { name: serverName } resource database \u0026#39;Microsoft.Sql/servers/databases@2020-11-01-preview\u0026#39; = { parent: server name: databaseName location: location sku: { name: \u0026#39;Standard\u0026#39; tier: \u0026#39;Standard\u0026#39; } } Use the existing keyword to refer to the resource, and then you add the child by specifying the parent property.\nFinally, to deploy an extension resource to an existing resource, use the scope keyword:\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: \u0026#39;toydesigndocs\u0026#39; } resource lockResource \u0026#39;Microsoft.Authorization/locks@2016-09-01\u0026#39; = { scope: storageAccount name: \u0026#39;DontDelete\u0026#39; properties: { level: \u0026#39;CanNotDelete\u0026#39; notes: \u0026#39;Prevents deletion of the toy design documents storage account.\u0026#39; } } Referring to an existing resource\u0026rsquo;s properties Define the resource and you can refer to its properties if the prperty isn\u0026rsquo;t secure.\nresource applicationInsights \u0026#39;Microsoft.Insights/components@2018-05-01-preview\u0026#39; existing = { name: applicationInsightsName } resource functionApp \u0026#39;Microsoft.Web/sites@2020-06-01\u0026#39; = { name: functionAppName location: location kind: \u0026#39;functionapp\u0026#39; properties: { siteConfig: { appSettings: [ // ... { name: \u0026#39;APPINSIGHTS_INSTRUMENTATIONKEY\u0026#39; value: applicationInsights.properties.InstrumentationKey } ] } } } When you need to access secure data, use the listKeys() function.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: storageAccountName } resource functionApp \u0026#39;Microsoft.Web/sites@2020-06-01\u0026#39; = { name: functionAppName location: location kind: \u0026#39;functionapp\u0026#39; properties: { siteConfig: { appSettings: [ // ... { name: \u0026#39;StorageAccountKey\u0026#39; value: storageAccount.listKeys().keys[0].value } ] } } } The VScode extension will show hints to help you understand the data this function returns.\nYou need to have sufficient permissions to use the listKeys function.\nChild and extension In this example we are attaching an extension to a child resource.\nresource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2019-06-01\u0026#39; existing = { name: storageAccountName resource blobService \u0026#39;blobServices\u0026#39; existing = { name: \u0026#39;default\u0026#39; } } /* Note: Here we are attaching to blobServices which itself is a child resource. So we are attaching an extension to a child resource. */ resource storageAccountBlobDiagnostics \u0026#39;Microsoft.Insights/diagnosticSettings@2017-05-01-preview\u0026#39; = { scope: storageAccount::blobService name: storageAccountBlobDiagnosticSettingsName properties: { workspaceId: logAnalyticsWorkspace.id logs: [ { category: \u0026#39;StorageRead\u0026#39; enabled: true } { category: \u0026#39;StorageWrite\u0026#39; enabled: true } { category: \u0026#39;StorageDelete\u0026#39; enabled: true } ] } } Structuring Bicep for Collaboration configuration maps Using many different parameters can be confusing to the user of the template you\u0026rsquo;re writing. One way of solving this is by creating a config map:\n@allowed([ \u0026#39;Production\u0026#39; \u0026#39;Test\u0026#39; ]) param environmentType string = \u0026#39;Test\u0026#39; var environmentConfigurationMap = { Production: { appServicePlan: { sku: { name: \u0026#39;P2V3\u0026#39; capacity: 3 } } storageAccount: { sku: { name: \u0026#39;ZRS\u0026#39; } } } Test: { appServicePlan: { sku: { name: \u0026#39;S2\u0026#39; capacity: 1 } } storageAccount: { sku: { name: \u0026#39;LRS\u0026#39; } } } } resource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2020-06-01\u0026#39; = { name: appServicePlanName location: location sku: environmentConfigurationMap[environmentType].appServicePlan.sku } Here we are taking a parameter for the environment, but create an object that contains the settings for that particular environment. Note that we are accessing it like we\u0026rsquo;d access a dictionary in python: sku: environmentConfigurationMap[environmentType].appServicePlan.sku\nNaming In Bicep, you ordinarily use camelCase capitalization style for the names of parameters, variables, and resource symbolic names.\nResource names cannot be renamed after they\u0026rsquo;re deployed in Azure.\nComments use // for single line comments, and /* */ for multi-line comments.\nWhen adding comments to JSON files, you might have to save the file as jsonc to let the code editor know that comments are allowed.\n","permalink":"https://mischavandenburg.com/zet/intermediate-bicep/","summary":"Today I finished the Intermediate Bicep module. Here are my notes.\nChild and Extension You can also use Bicep to refer to resources that were created outside the Bicep file itself. For example, you can refer to resources that your colleagues have created manually by using the Azure portal, or from within another Bicep template or module, even if they\u0026rsquo;re in a different resource group or subscription. By using these features of Bicep, you can unlock the ability to create powerful templates that deploy all aspects of your Azure infrastructure.","title":"Notes: Intermediate Bicep"},{"content":"I\u0026rsquo;ll be working with Bicep during my next contract, so I\u0026rsquo;m working through the Bicep modules on Microsoft Learn to prepare. I must say that these modules are particularly helpful. They are well structured and they provide you with free sandbox environments to practice deploying the templates you create.\nWhy Bicep? Resources in Azure are deployed by the Azure Resource Manager (ARM). These resources are JSON objects under the covers, and ARM templates are a way to generate these JSON objects. However, JSON is not really meant to be edited by humans, and the ARM templates are not very suitable for editing either. Thus, Bicep was developed to allow for a better editing experience and better readability and reusability.\nBicep templates are transpiled into JSON objects, which are sent to the Azure API to create resources with the Azure Resource Manager.\nFundamentals of Bicep Notes A parameter lets you bring in values from outside the template file. For example, if someone is manually deploying the template by using the Azure CLI or Azure PowerShell, they\u0026rsquo;ll be asked to provide values for each parameter. They can also create a parameter file, which lists all of the parameters and values they want to use for the deployment. If the template is deployed from an automated process like a deployment pipeline, the pipeline can provide the parameter values.\nA variable is defined and set within the template. Variables let you store important information in one place and refer to it throughout the template without having to copy and paste it.\ngenerating unique names Bicep has another function called uniqueString() that comes in handy when you\u0026rsquo;re creating resource names. When you use this function, you need to provide a seed value, which should be different across different deployments but consistent across all deployments of the same resources.\nparam storageAccountName string = uniqueString(resourceGroup().id) Every time you deploy the same resources, they\u0026rsquo;ll go into the same resource group. The uniqueString() function will return the same value every time. If you deploy into two different resource groups in the Azure subscription, the resourceGroup().id value will be different, because the resource group names will be different. The uniqueString() function will give different values for each set of resources. If you deploy into two different Azure subscriptions, even if you use the same resource group name, the resourceGroup().id value will be different because the Azure subscription ID will be different. The uniqueString() function will again give different values for each set of resources. combining strings Can use string interpolation to generate a unique string with a recognizable hardcoded part:\nparam storageAccountName string = 'toylaunch${uniqueString(resourceGroup().id)}'\nThis can also be handy for generating correct names. For example, storage accounts may not begin with a number.\nparameter decorators allowed parameters @allowed([ \u0026#39;nonprod\u0026#39; \u0026#39;prod\u0026#39; ]) param environmentType string The template cannot be deployed unless the nonprod or prod values are provided.\n@allowed is a parameter decorator: it gives Bicep information on what the parameter\u0026rsquo;s value needs to be.\nYou can also specify the allowed length of the parameter by using the following decorators:\n@minLength(5) @maxLength(24) param storageAccountName string You can apply multiple decorators to a parameter by putting each on a separate line.\nThese min and maxLength decorators can also be used to limit the length of an array.\nTo limit int values:\n@minValue(1) @maxValue(10) param appServicePlanInstanceCount int Finally, you can add descriptions to your parameters with the @description decorator:\n@description(\u0026#39;The locations into which this Cosmos DB account should be configured. This parameter needs to be a list of objects, each of which has a locationName property.\u0026#39;) param cosmosDBAccountLocations array if statements var storageAccountSkuName = (environmentType == \u0026#39;prod\u0026#39;) ? \u0026#39;Standard_GRS\u0026#39; : \u0026#39;Standard_LRS\u0026#39; var appServicePlanSkuName = (environmentType == \u0026#39;prod\u0026#39;) ? \u0026#39;P2V3\u0026#39; : \u0026#39;F1\u0026#39; Let\u0026rsquo;s unpack this:\n? is a ternary operator and evaluates an if/then statement. The value after ? is used if the expression is true. If it\u0026rsquo;s false, the value after : is used.\nSo here, if the environmentType is prod, the SKU is set to Standard_GRS\nObjects in Bicep You can use objects within resource definitions, within variables, or within expressions in your Bicep file.\nObjects are the same as dictionaries in python:\nparam appServicePlanSku object = { name: \u0026#39;F1\u0026#39; tier: \u0026#39;Free\u0026#39; capacity: 1 } These are called \u0026ldquo;properties\u0026rdquo; of type string and int. Note that they are line separated, not comma separated like in python.\nWhen referencing the parameter in the template, you can use dot notation to access the object properties:\nresource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2022-03-01\u0026#39; = { name: appServicePlanName location: location sku: { name: appServicePlanSku.name tier: appServicePlanSku.tier capacity: appServicePlanSku.capacity } } [!important] Keep in mind that you don\u0026rsquo;t specify the type of each property within an object. However, when you use a property\u0026rsquo;s value, its type must match what\u0026rsquo;s expected. In the previous example, both the name and the tier of the App Service plan SKU must be strings.\nExample: tags param resourceTags object = { EnvironmentName: \u0026#39;Test\u0026#39; CostCenter: \u0026#39;1000100\u0026#39; Team: \u0026#39;Human Resources\u0026#39; } resource appServicePlan \u0026#39;Microsoft.Web/serverfarms@2022-03-01\u0026#39; = { name: appServicePlanName location: location tags: resourceTags sku: { name: \u0026#39;S1\u0026#39; } } resource appServiceApp \u0026#39;Microsoft.Web/sites@\u0026#39; = { name: appServiceAppName location: location tags: resourceTags kind: \u0026#39;app\u0026#39; properties: { serverFarmId: appServicePlan.id } } Here we take the tags for all the resources of the template as parameters. But we easily reuse all the tags for each resource by referencing the entire object.\nArrays Arrays are not typed in Bicep. You cannot specify that it must contain strings.\nExample:\nparam cosmosDBAccountLocations array = [ { locationName: \u0026#39;australiaeast\u0026#39; } { locationName: \u0026#39;southcentralus\u0026#39; } { locationName: \u0026#39;westeurope\u0026#39; } ] This is an array of objects, which have an locationName property each.\nAnd you would access it by:\nresource account \u0026#39;Microsoft.DocumentDB/databaseAccounts@2022-08-15\u0026#39; = { name: accountName location: location properties: { locations: cosmosDBAccountLocations } } Specifying parameter values When deploying a template file there are three options:\ndefault values command line parameter file Parameter file This is a json file. To deploy a template with a paramter file, use:\naz deployment group create \\ --template-file main.bicep \\ --parameters main.parameters.json priority The order of priority is this, from high to low priority:\nParameters specified on the command line Parameter file Default values in template Securing parameters It is best to use Managed Identities for Azure, but if you need to supply secret values to a deployment, use the @secure() decorator. These values aren\u0026rsquo;t available in the deployment logs, and they won\u0026rsquo;t be displayed on the screen when entered in the terminal.\nLoops Defined with the for keyword. Usually you iterate over an array to create multiple instances of a resource.\nCopy loops param storageAccountNames array = [ \u0026#39;saauditus\u0026#39; \u0026#39;saauditeurope\u0026#39; \u0026#39;saauditapac\u0026#39; ] resource storageAccountResources \u0026#39;Microsoft.Storage/storageAccounts@2021-09-01\u0026#39; = [for storageAccountName in storageAccountNames: { name: storageAccountName location: resourceGroup().location kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } }] Notice that bicep requires \u0026ldquo;[\u0026rdquo; before the for, and a closing bracket.\ncount loops resource storageAccountResources \u0026#39;Microsoft.Storage/storageAccounts@2021-09-01\u0026#39; = [for i in range(1,4): { name: \u0026#39;sa${i}\u0026#39; location: resourceGroup().location kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } }] The range function takes two arguments. The first one specifies the starting value, and the second tells Bicep the number of values you want.\nIf you use range(3,4), you will get 3, 4, 5 and 6.\naccessing the index param locations array = [ \u0026#39;westeurope\u0026#39; \u0026#39;eastus2\u0026#39; \u0026#39;eastasia\u0026#39; ] resource sqlServers \u0026#39;Microsoft.Sql/servers@2021-11-01-preview\u0026#39; = [for (location, i) in locations: { name: \u0026#39;sqlserver-${i+1}\u0026#39; location: location properties: { administratorLogin: administratorLogin administratorLoginPassword: administratorLoginPassword } }] The first value is zero, so you can add 1 to i if you want your names to be sqlserver-1, sqlserver-2 etc.\ni is used here, but you can use any value you want.\nFiltering with loops param sqlServerDetails array = [ { name: \u0026#39;sqlserver-we\u0026#39; location: \u0026#39;westeurope\u0026#39; environmentName: \u0026#39;Production\u0026#39; } { name: \u0026#39;sqlserver-eus2\u0026#39; location: \u0026#39;eastus2\u0026#39; environmentName: \u0026#39;Development\u0026#39; } { name: \u0026#39;sqlserver-eas\u0026#39; location: \u0026#39;eastasia\u0026#39; environmentName: \u0026#39;Production\u0026#39; } ] resource sqlServers \u0026#39;Microsoft.Sql/servers@2021-11-01-preview\u0026#39; = [for sqlServer in sqlServerDetails: if (sqlServer.environmentName == \u0026#39;Production\u0026#39;) { name: sqlServer.name location: sqlServer.location properties: { administratorLogin: administratorLogin administratorLoginPassword: administratorLoginPassword } tags: { environment: sqlServer.environmentName } }] This will deploy -we and -eas, but not -eus2, because the environmentName does not match Production.\nControlling loop execution By default all the iterations of a loop are executed simultaneously. However, you don\u0026rsquo;t always want this to be happening.\nTo control the amount you can use the @batchSize decorator.\n@batchSize(2) resource appServiceApp \u0026#39;Microsoft.Web/sites@2021-03-01\u0026#39; = [for i in range(1,3): { name: \u0026#39;app${i}\u0026#39; // ... }] Here bicep will wait for the first two to be fully completed before it moves to the next.\nTo loop sequentially, meaning one at a time in order, use @batchSize(1)\nVariable loops You can use loops to create arrays that you can use in the Bicep template.\nvar items = [for i in range(1, 5): \u0026#39;item${i}\u0026#39;] This produces an array containing the values item1, item2 up to 5 stored in the items variable.\nReminds me of list comprehensions in python.\nHere is an example:\nparam addressPrefix string = \u0026#39;10.10.0.0/16\u0026#39; param subnets array = [ { name: \u0026#39;frontend\u0026#39; ipAddressRange: \u0026#39;10.10.0.0/24\u0026#39; } { name: \u0026#39;backend\u0026#39; ipAddressRange: \u0026#39;10.10.1.0/24\u0026#39; } ] var subnetsProperty = [for subnet in subnets: { name: subnet.name properties: { addressPrefix: subnet.ipAddressRange } }] resource virtualNetwork \u0026#39;Microsoft.Network/virtualNetworks@2021-08-01\u0026#39; = { name: \u0026#39;teddybear\u0026#39; location: resourceGroup().location properties:{ addressSpace:{ addressPrefixes:[ addressPrefix ] } subnets: subnetsProperty } } The content of the subnetsProperty array would look like this:\n[ { name: \u0026#39;frontend\u0026#39;, properties: { addressPrefix: \u0026#39;10.10.0.0/24\u0026#39; } }, { name: \u0026#39;backend\u0026#39;, properties: { addressPrefix: \u0026#39;10.10.1.0/24\u0026#39; } } ] Output loops To output the contents of the array:\nvar items = [ \u0026#39;item1\u0026#39; \u0026#39;item2\u0026#39; \u0026#39;item3\u0026#39; \u0026#39;item4\u0026#39; \u0026#39;item5\u0026#39; ] output outputItems array = [for i in range(0, length(items)): items[i]] Modules You can create modules in Bicep so the code becomes reusable. You can share the modules with other teams and use them for different outcomes.\nGenerally, it\u0026rsquo;s not a good practice to create a module for every resource that you deploy. A good Bicep module typically defines multiple related resources. However, if you have a particularly complex resource with a lot of configuration, it might make sense to create a single module to encapsulate the complexity. This approach keeps your main templates simple and uncluttered.\nSo for example it would make sense to write a networking module and a database module that handles these resources.\nModules can be nested, but it can quickly become very complex.\nTo call a module in a template:\nmodule appModule \u0026#39;modules/app.bicep\u0026#39; = { name: \u0026#39;myApp\u0026#39; params: { location: location appServiceAppName: appServiceAppName environmentType: environmentType } } The modules are stored in the modules folder in your root directory.\nParameters Modules will take parameters, but it is good practice to leave out default values for parameters in modules. In templates it\u0026rsquo;s good practice to add defaults wherever you can. Therefore it is best to leave them out in modules because templates usually have their own default values. This can get confusing if you have similar default values in the templates and modules.\nModule dependency Bicep will figure out automatically if there is a dependency between modules. For example:\n@description(\u0026#39;Username for the virtual machine.\u0026#39;) param adminUsername string @description(\u0026#39;Password for the virtual machine.\u0026#39;) @minLength(12) @secure() param adminPassword string module virtualNetwork \u0026#39;modules/vnet.bicep\u0026#39; = { name: \u0026#39;virtual-network\u0026#39; } module virtualMachine \u0026#39;modules/vm.bicep\u0026#39; = { name: \u0026#39;virtual-machine\u0026#39; params: { adminUsername: adminUsername adminPassword: adminPassword subnetResourceId: virtualNetwork.outputs.subnetResourceId } } Here the virtualMachine module takes the subnetResourceId from the virtualNetwork module outputs.\nBecause it is defined like this, Bicep will wait with deploying the virtualMachine modul until the virtualNetwork module is finished, and pass in the required parameter.\nIt is important to note that this means that it will wait until the virtualNetwork module is completely finished. If it takes a long time to deploy the previous module, all the subsequent modules will have to wait until it\u0026rsquo;s finished.\n","permalink":"https://mischavandenburg.com/zet/articles/fundamentals-of-bicep/","summary":"I\u0026rsquo;ll be working with Bicep during my next contract, so I\u0026rsquo;m working through the Bicep modules on Microsoft Learn to prepare. I must say that these modules are particularly helpful. They are well structured and they provide you with free sandbox environments to practice deploying the templates you create.\nWhy Bicep? Resources in Azure are deployed by the Azure Resource Manager (ARM). These resources are JSON objects under the covers, and ARM templates are a way to generate these JSON objects.","title":"Notes: Fundamentals of Bicep"},{"content":"The CTO of my new company recommended the Udemy course \u0026ldquo;Go: The Complete Developer\u0026rsquo;s Guide (Golang)\u0026rdquo;. I started today and here are some notes I made.\nHello World in Go We start by writing a Hello World and studying all the elements.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } How do we run code? go run main.go runs the program go build main.go compiles it to an executable\nWhat does package main mean? package main\nA package is a collection of common source code files.\nOne app is a package. If you have multiple files in a folder, such as helper.go or support.go, they should have package main to indicate that they belong to package main.\n*Why do we call it main?\nThere are two types of packages.\nexecutable generates file that can be run reusable used as \u0026ldquo;helpers\u0026rdquo; reusable logic When you call the package main, you are telling the compiler it needs to be compiled as an executable. If it has a different name, it won\u0026rsquo;t generate an executable. Main is sacred.\nAny other name is a reusable or dependency type package (helper code).\nAnother important point is that whenever you create an executable package, it must always have a func called \u0026lsquo;main\u0026rsquo;.\nWhat does import fmt mean? The import statement is used to give our package access to code written in another package. You are saying \u0026ldquo;give access to all code in fmt\u0026rdquo;. Fmt is a standard library package included in Go. Short for format. Used to print out information to the terminal.\nOther packages included in the standard library of go are debug, math, encoding, crypto, io.\ngolang.org/pkg for documentation on standard library packages for Go.\nA lot of learning go is learning the standard packages and how they work.\nOrganizing the main.go file It is the same for every go file, just like the code example at the top of the page. Package main, import fmt, and func main.\nvariable declarations var card string = \u0026quot;Ace of Spades\u0026quot;\nvar: we are about to create a new variable\ncard: name\nstring = telling the go compiler that only strings will be assigned to this variable\nAlternatively: card := \u0026quot;Ace of Spades\u0026quot;\nHere you are relying on the compiler to figure out what type it is.\nCompiler will infer the type.\nWe only use this := assignment for new variables\nIf you want to assign a value to a variable after it is declared, you just do card = \u0026quot;Five of Diamonds\u0026quot;\nGo types Go is a statically typed language.\nJavascript, python are dynamically typed language. We don\u0026rsquo;t care what value is assigned to a variable.\nYou always a assign a type to a variable in Go.\nBasic go types:\nbool string into float64 : a number with a decimal after it. ","permalink":"https://mischavandenburg.com/zet/go-day-1/","summary":"The CTO of my new company recommended the Udemy course \u0026ldquo;Go: The Complete Developer\u0026rsquo;s Guide (Golang)\u0026rdquo;. I started today and here are some notes I made.\nHello World in Go We start by writing a Hello World and studying all the elements.\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } How do we run code? go run main.go runs the program go build main.go compiles it to an executable","title":"Learning Go Day 1: Notes"},{"content":"For month\u0026rsquo;s I considered to learn Go, and today I finally started.\nI doubted for a long time, because I figured it would be better to dive deeper into Python. However, when I reflected on it, I realized I\u0026rsquo;m able to do the things I want to do in Python. I can create scripts to manipulate data, and I can string different tools and libraries together if I need to. So how deep do I actually need to go as a DevOps Engineer?\nThe breakthrough came when I discovered this website: Good First Issue. It\u0026rsquo;s a collection of issues in open source projects which are suitable for people who are starting out. Scrolling through the available projects for Python and Go, I noticed I got a lot more energy from the things I saw in the Go list: Grafana, ArgoCD, Kubernetes. I probably have a long way to go before I can make any meaningful contributions, but my goal is to be able to contribute to open source in the future.\nAdditionally, I\u0026rsquo;m going to focus my career in the direction of Kubernetes and CNCF, where Go is mostly used.\nI have a week off before I start my new job so I\u0026rsquo;m going to spend a few hours a day to get a good start, and when I\u0026rsquo;m working full time again, my plan is to spend at least one pomodoro (25 minutes) a day on Go. Small increments will add up to something big over time.\nI created a new GitHub repo where I\u0026rsquo;ll be uploading all my practice code and projects.\n","permalink":"https://mischavandenburg.com/zet/start-learning-go/","summary":"For month\u0026rsquo;s I considered to learn Go, and today I finally started.\nI doubted for a long time, because I figured it would be better to dive deeper into Python. However, when I reflected on it, I realized I\u0026rsquo;m able to do the things I want to do in Python. I can create scripts to manipulate data, and I can string different tools and libraries together if I need to. So how deep do I actually need to go as a DevOps Engineer?","title":"I started to learn Go!"},{"content":"I\u0026rsquo;m a little obsessed with controlling everything with my keyboard. That\u0026rsquo;s why I loved AwesomeWM so much on my Arch Linux setup, I hardly used my mouse anymore.\nOne thing I loved about my setup was the ability to control my music from the keyboard from anywhere. This is a feature I picked up from the awesome-copycats theme for AwesomeWM. This was one of the first things I missed when I made my switch to MacOS.\nI started using Apple Music as my music app but it does not have any global hotkeys, and it makes you use a widget with the mouse.\nskhd I solved the problem using skhd. This is a free hotkey daemon for MacOS. To install:\nbrew install koekeishiya/formulae/skhd brew services start skhd Then I added the following to my skhd config file:\n# Control apple music globally # Based on Aesome-copycats theme for AesomeWM ctrl + cmd + fn - up : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to playpause\u0026#39; ctrl + cmd + fn - left : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to back track\u0026#39; ctrl + cmd + fn - right : osascript -e \u0026#39;tell app \u0026#34;Music\u0026#34; to play next track\u0026#39; links https://github.com/koekeishiya/skhd\n","permalink":"https://mischavandenburg.com/zet/apple-music-hotkeys/","summary":"I\u0026rsquo;m a little obsessed with controlling everything with my keyboard. That\u0026rsquo;s why I loved AwesomeWM so much on my Arch Linux setup, I hardly used my mouse anymore.\nOne thing I loved about my setup was the ability to control my music from the keyboard from anywhere. This is a feature I picked up from the awesome-copycats theme for AwesomeWM. This was one of the first things I missed when I made my switch to MacOS.","title":"Controlling Apple Music with hotkeys from anywhere on MacOS"},{"content":"My new employer kindly sponsored me with a new 16-inch MacBook Pro M2. I chose 32GB Ram and the base model with 12 CPU and 19 GPU cores and 1TB of hard disk. These specifications will suit my needs well.\nI\u0026rsquo;ve been daily driving Linux for a year now, and it has been such an incredible journey. Building my own Arch Linux system taught me much about Linux and what it means to maintain a system yourself. I will undoubtedly miss my custom AwesomeWM configuration and the freedom that Arch provides.\nTechnically I could have chosen a Windows laptop and installed Linux on it. But I went for a Mac for the following reasons:\nCompatibility. I\u0026rsquo;ll be doing contracts for various customers, so I\u0026rsquo;ll likely run into environments where I\u0026rsquo;ll need to use some software, such as a VPN that is not supported on Linux. MacOS will always be compatible everywhere. Portability. I\u0026rsquo;ll probably be going to the office multiple times a week, and using a MacBook as a daily driver will allow me to take my full setup with me everywhere. I also plan to spend some time abroad, and using a MacBook as a daily driver will allow me to work remotely for extended periods. Stability. Even though my Linux systems never really broke on me, there is always a possibility. I don\u0026rsquo;t have to worry about that with MacOS. Even though I\u0026rsquo;ll be switching to my MacBook as my primary workstation, I will continue using Linux for my hobby projects and home lab. So there will be plenty of opportunity to keep growing in that area in the future.\n","permalink":"https://mischavandenburg.com/zet/move-to-macos/","summary":"My new employer kindly sponsored me with a new 16-inch MacBook Pro M2. I chose 32GB Ram and the base model with 12 CPU and 19 GPU cores and 1TB of hard disk. These specifications will suit my needs well.\nI\u0026rsquo;ve been daily driving Linux for a year now, and it has been such an incredible journey. Building my own Arch Linux system taught me much about Linux and what it means to maintain a system yourself.","title":"New Laptop: Moving to MacOS"},{"content":"Last week I accepted a job offer! I\u0026rsquo;ve been hired as a DevOps Engineer by Fullstaq.\nFullstaq specialises in Kubernetes and Cloud Native, so I\u0026rsquo;m really happy I could join their team. I\u0026rsquo;ll be doing projects for a variety of clients, and I think it will be a great place to learn and gain more experience in the Kubernetes and CNCF space.\nFullstaq has been delightfully welcoming and great to work with so far. The future looks bright!\n","permalink":"https://mischavandenburg.com/zet/new-job-2023/","summary":"Last week I accepted a job offer! I\u0026rsquo;ve been hired as a DevOps Engineer by Fullstaq.\nFullstaq specialises in Kubernetes and Cloud Native, so I\u0026rsquo;m really happy I could join their team. I\u0026rsquo;ll be doing projects for a variety of clients, and I think it will be a great place to learn and gain more experience in the Kubernetes and CNCF space.\nFullstaq has been delightfully welcoming and great to work with so far.","title":"New job!"},{"content":"This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.\nInstall argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.\ncurl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer\nkubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; Retrieve your passsword\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Find out which port argocd-server is running on\nk get svc -A Look for the argocd-server and see where port 80 is mapped to. In my case, it is 80:31372.\nOpen this port in your network security group for your VM, and you should be able to log in on ArgoCD in the browser by entering the VM ip followed by the port:\nhttp://143.44.179.11:31372\nLinks https://argo-cd.readthedocs.io/en/stable/getting_started/\n","permalink":"https://mischavandenburg.com/zet/articles/lab-vm-install-argocd/","summary":"This guide uses the official getting started guide with a few modifications. This installation is only for lab purposes. Running ArgoCD in a production environment requires more configuration.\nInstall argocd and argocd cli kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml My VM is running on arm architecture, so I need these commands to install the argocd cli on ubuntu.\ncurl -sSL -o argocd-linux-arm64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-arm64 sudo install -m 555 argocd-linux-arm64 /usr/local/bin/argocd rm argocd-linux-arm64 Change the service type to LoadBalancer","title":"Lab VM project - Install ArgoCD to your Kubernetes cluster"},{"content":"You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.\nHere are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.\nInstallation sudo apt-get update sudo apt install apt-transport-https curl Install containerd\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install containerd.io Remove the default containerd configuration, because it creates errors when running kubeadm init.\nsudo rm -f /etc/containerd/config.toml sudo systemctl status containerd.service Install Kubernetes\nsudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt install kubeadm kubelet kubectl kubernetes-cni Avoid the error \u0026ldquo;/proc/sys/net/bridge/bridge-nf-call-iptables does not exist\u0026rdquo; on kubeinit (reference https://github.com/kubernetes/kubeadm/issues/1062).\nsudo modprobe br_netfilter sudo echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward Start the cluster Initialize the Kubernetes cluster for use with Flannel\nsudo kubeadm init --pod-network-cidr=10.244.0.0/16 Copy to config as kubadm command says\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Usually you wouldn\u0026rsquo;t run pods on your control-plane node. However, since we are running a lab environment on a single VM, it\u0026rsquo;s ok. To be able to schedule pods on the control-plane node, we need to remove the NoSchedule taint:\nkubectl taint node instance-20230205-0909 node-role.kubernetes.io/control-plane:NoSchedule- Add a Container Networking Interface Install Flannel to the cluster (reference https://github.com/flannel-io/flannel)\nkubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml Configure the server firewall We use Uncomplicated Firewall. Run these commands:\nsudo ufw allow 22 sudo ufw allow 6443/tcp sudo ufw allow 2379:2380/tcp sudo ufw allow 10250/tcp sudo ufw allow 10259/tcp sudo ufw allow 10257/tcp sudo ufw enable sudo ufw status Set up bashrc Next, edit your bashrc with vim ~/.bashrc and add these lines:\nsource \u0026lt;(kubectl completion bash) alias k=kubectl complete -o default -F __start_kubectl k Then run source ~/.bashrc\nThis configures autocompletion for kubectl, and sets up \u0026ldquo;k\u0026rdquo; as an alias for kubectl.\nLet\u0026rsquo;s run a pod! To see all pods running on your cluster:\nk get pods -A\nNow let\u0026rsquo;s run a simple nginx pod and expose it:\nk run nginx --image=nginx k expose pod nginx --port=80 --type=NodePort To find out which port it\u0026rsquo;s running on, run k get service. In the PORT(S) column, there will be an nginx service exposing port 80 to a random port on the node in the range of 30000-32767.\nIn my case, it says \u0026ldquo;80:31878/TCP\u0026rdquo;\nTo see if we can reach the container, run:\ncurl localhost:31878\nIf everything went well, you will get back the HTML of the default index page served by NGINX:\nubuntu@instance-20230205-0909:~$ curl localhost:31878 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; To reach the pod from the browser, open your port in the security group configured for the subnet of your VM.\nGood luck with your new lab environment!\nLinks https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\nhttps://kubernetes.io/docs/concepts/services-networking/service/\nhttps://github.com/flannel-io/flannel\n","permalink":"https://mischavandenburg.com/zet/articles/simple-cluster-on-ubuntu-vm/","summary":"You can get a free 24GB ram VM from Oracle. What better place for your own Kubernetes lab that is always available? See this article to create your VM.\nHere are the steps I took to install a single node kubernetes cluster on the Ubuntu VM.\nInstallation sudo apt-get update sudo apt install apt-transport-https curl Install containerd\nsudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.","title":"Setting up a Kubernetes cluster on an Ubuntu 20.04 VM with containerd and flannel"},{"content":"A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.\nYou can host a 4CPU 24GB VM for free!\nThis is perfect for a lab environment.\nI spent my evening creating the VM and setting up a kubernetes cluster from scratch.\nUse this video to claim your free vm:\nhttps://www.youtube.com/watch?v=NKc3k7xceT8\n","permalink":"https://mischavandenburg.com/zet/free-oracle-vm/","summary":"A few weeks ago someone gave me a tip. Oracle actually has a really good free tier offering.\nYou can host a 4CPU 24GB VM for free!\nThis is perfect for a lab environment.\nI spent my evening creating the VM and setting up a kubernetes cluster from scratch.\nUse this video to claim your free vm:\nhttps://www.youtube.com/watch?v=NKc3k7xceT8","title":"Get a free 4 CPU 24GB Ram VM on from Oracle"},{"content":"Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID\u0026rsquo;s in each of them.\nHere are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.\nList out the disks with lsblk:\n(ins)[mischa@arch-beast ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:0 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:2 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:3 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:1 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part I will be adding /dev/sde to my system. As you see, I already created a partition on it, named sde1. The mountpoint for the disk will be /data-hdd3.\nIf you still need to add your partition, use sudo gdisk /dev/sde to write a new table and partition.\nencryption First I create the mount point I\u0026rsquo;ll use and set the appropriate permisssions:\nsudo mkdir /data-hdd3 sudo chown mischa:mischa /data-hdd3 Now we create a LUKS header and an encrypted filesystem on the disk. Note that I\u0026rsquo;m using the notation convention from the Arch Wiki where the \u0026ldquo;#\u0026rdquo; indicates that the command should be run as root.\n# cryptsetup -y -v luksFormat /dev/sde1 # cryptsetup open /dev/sde1 data-hdd3 # mkfs.ext4 /dev/mapper/data-hdd3 # mount /dev/mapper/data-hdd3 /data-hdd3 Verify that it worked and the new encrypted partition is mounted:\narch-beast# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:0 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:2 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:3 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:1 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part └─data-hdd3 254:4 0 931.5G 0 crypt /data-hdd3 Auto mounting at boot We\u0026rsquo;ll need to add this disk to the kerenel parameters, /etc/crypttab and /etc/fstab. I haven\u0026rsquo;t gotten round to switching to systemd boot yet, but I will do so very soon.\nOpen tmux and split the pane. In the bottom pane, run lsblk -f to have all the UUIDs listed. Then open the grub configuration file with sudoedit /etc/default/grub\nYou can discern which uuid to add from the listed examples. For my new disk, I needed to add the following:\nrd.luks.name=3169af6c-a129-448e-b451-d7767866f607 data-hdd3=/dev/mapper/data-hdd3\nThen run sudo grub-mkconfig -o /boot/grub/grub.cfg to update grub with the new settings. Adjust the path if you use a different path for your boot partition!\nNext, we add it to /etc/crypttab\nTo mount the new encrypted partition at boot, we add it to /etc/fstab.\nNote that this time we need to use the UUID of the partition located at /dev/mapper/data-hdd3\nUse sudo findmnt --verify to check if there is antyhing wrong with the file.\nNow you should be able to reboot and your new encrypted disk should be mounted automatically.\n(ins)[mischa@arch-beast ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS sda 8:0 0 223.6G 0 disk └─sda1 8:1 0 223.6G 0 part └─games 254:1 0 223.6G 0 crypt /games sdb 8:16 0 931.5G 0 disk └─sdb1 8:17 0 931.5G 0 part └─data-hdd 254:0 0 931.5G 0 crypt /data-hdd sdc 8:32 0 931.5G 0 disk └─sdc1 8:33 0 931.5G 0 part └─data-hdd2 254:4 0 931.5G 0 crypt /data-hdd2 sdd 8:48 0 465.8G 0 disk ├─sdd1 8:49 0 300M 0 part /boot └─sdd2 8:50 0 465.5G 0 part └─root 254:2 0 465.5G 0 crypt / sde 8:64 0 931.5G 0 disk └─sde1 8:65 0 931.5G 0 part └─data-hdd3 254:3 0 931.5G 0 crypt /data-hdd3 links https://wiki.archlinux.org/title/Dm-crypt/Encrypting_an_entire_system#LUKS_on_a_partition\nhttps://wiki.archlinux.org/title/GRUB\n","permalink":"https://mischavandenburg.com/zet/articles/new-luks-encrypted-disk/","summary":"Today I added a harddisk I had lying around because I needed some more space. On my Arch Linux system I have all my drives encrypted like a good boy. It can be a bit tricky when you are adding them because you need to configure a few different files and add different UUID\u0026rsquo;s in each of them.\nHere are the steps I follow to add a new disk. Note that this how to assumes that you already have set up your system with dm-crypt.","title":"Setting up a new LUKS encrypted disk with dm-crypt in Arch Linux"},{"content":"For the past few months I\u0026rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.\nMy Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.\nI wanted to automate this for a long time now, but I never got round to it. Today I made the first steps, but it is still in progress.\nNaturally, I could use a tool like Timeshift or something similar to schedule my backups. However, I want to do it myself using rsync because I want to fully understand what I am backing up, when, and where. Rsync is also used in our environment at work, so I assume it is more common in enterprise and production environments.\nfull system backup Before I was making a full system backup every Friday using this command:\nsudo rsync -aAXH --info=stats1,progress2 --exclude={\u0026#34;/dev/*\u0026#34;,\u0026#34;/proc/*\u0026#34;,\u0026#34;/sys/*\u0026#34;,\u0026#34;/tmp/*\u0026#34;,\u0026#34;/run/*\u0026#34;,\u0026#34;/mnt/*\u0026#34;,\u0026#34;/media/*\u0026#34;,\u0026#34;/lost+found\u0026#34;,\u0026#34;/home/*/.cache/*\u0026#34;,\u0026#34;/data-hdd/\u0026#34;,\u0026#34;/games/\u0026#34;,\u0026#34;/var/lib/docker/*\u0026#34;,\u0026#34;/home/mischa/music/*\u0026#34;,\u0026#34;/swapfile\u0026#34;, \u0026#34;/data-hdd2/\u0026#34;, \u0026#34;/data-hdd3/\u0026#34;} / /data-hdd/backups/arch-beast/01-01-23 This command creates a full backup of my entire root filesystem, and it should be possible to restore my entire system by just reversing the target and destination in the end.\nHowever, as I was coming up with my new strategy, I thought this was overkill.\nslimming down All I really need to back up is my home directory and it would be nice to have my /etc directory backed up as well.\nSo I wrote a simple shell script to do this:\n#!/bin/bash BACKUPS_DESTINATION=\u0026#34;/data-hdd/backups/arch-beast\u0026#34; # format: # rsync -a --delete --quiet /path/to/backup /location/of/backup # stop the script if an error occurs set -e rsync -a --delete --quiet --exclude=\u0026#34;{\u0026#34;/home/*/.cache/*\u0026#34;}\u0026#34; /home/mischa $BACKUPS_DESTINATION/home rsync -a --delete --quiet /etc $BACKUPS_DESTINATION echo \u0026#34;Made backups on: $(date)\u0026#34; \u0026gt;\u0026gt; /var/log/backup.log -a flag from man page:\n\u0026ldquo;This is equivalent to -rlptgoD. It is a quick way of saying you want recursion and want to preserve almost everything.\u0026rdquo;\n\u0026ndash;delete: means files deleted on the source are to be deleted on the backup as well\nautomation I have a few scripts running in cronjobs on my system. I have a goal of putting them all in systemd timers, but I haven\u0026rsquo;t gotten round to it yet. For now, I will just add my backup scripts to my existing cronjobs setup.\nTo make my backups every day, I added this to my crontab:\n0 12 * * * /bin/bash /home/mischa/git/lab/bash/backup\nEvery day it will make a backup to the same directory and update the changed files, or delete the files I deleted from my system.\nI also wanted to have a weekly backup happening on Monday.\nI will make a more elaborate script to make a weekly directory, and rotate it with a new directory every week. But for now, I just chose a quick solution by creating a weekly version of my script and running it every Monday.\nThe only difference is the path:\nBACKUPS_DESTINATION=\u0026quot;/data-hdd/backups/arch-beast/weekly\u0026quot;\nIn the crontab:\n0 10 * * 1 /bin/bash /home/mischa/git/lab/bash/backup-weekly\nto do set up weekly backup in the same script create error handling and improve logging set up in systemd timers instead of crontab ","permalink":"https://mischavandenburg.com/zet/arch-backup-setup-1/","summary":"For the past few months I\u0026rsquo;ve been stuyding every hour of free time that I had. Now that I reached my certification goals for now, I finally had some time to do a chore I had been meaning to do for a long time.\nMy Arch Linux system is fully encrypted, and I make backups. But I was still doing it a bit haphazardly, usually every Friday.\nI wanted to automate this for a long time now, but I never got round to it.","title":"Setting up automated backups on my Arch Linux system with rsync and bash"},{"content":"I\u0026rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I\u0026rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.\nI started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.\nThe exam itself was difficult, but the AZ-104 was harder. The AZ-104 exam was more challenging because the questions were complicated and required you to simultaneously balance many different factors in the mind. The AZ-400 was difficult because the answer alternatives that are provided are incredibly similar to each other, and they make you very insecure about what the right choice might be. As a result, I changed my answers many times.\nI will do another study guide for this certification soon and publish my notes and Anki deck too. Now it\u0026rsquo;s time to celebrate and relax a little.\n","permalink":"https://mischavandenburg.com/zet/passed-az-400/","summary":"I\u0026rsquo;m typing this 30 minutes after I passed my AZ-400 exam. I\u0026rsquo;m sitting in a lovely cafe on Leidseplein in Amsterdam and feel relieved. Another significant certification bites the dust. This one took about 70 hours of study.\nI started preparing immediately after passing my AZ-104 exam, which was a good move. The AZ-400 requires you to know many details about Azure services and how to access them. For example, Shared Access Signatures are only used for accessing storage accounts, but they came up quite often as alternative answers to the questions.","title":"I passed the AZ-400 DevOps Expert today"},{"content":"The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.\nOne way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.\nFor example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.\nPersonal Access Tokens are an alternative to passwords but should be treated in exactly the same way.\nhttps://learn.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops\u0026amp;tabs=Windows\n","permalink":"https://mischavandenburg.com/zet/azure-personal-access-tokens/","summary":"The PAT (Personal Access Token) often comes up during practice tests for the AZ-400.\nOne way to remember when to use a PAT is that these are only for authenticating into Azure DevOps, never to external services.\nFor example, you might get a question on connecting your Azure DevOps project with a GitHub account from Azure DevOps, and PAT will show up as one of the alternative answers. By remembering that PATs are only for authenticating into ADO, you can elminate this alternative, and make your choice easier.","title":"Azure DevOps Personal Access Tokens are always for authenticating into ADO"},{"content":"I did a lot of studying last year, and I achieved a few tough certifications. I\u0026rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.\nOne technique I\u0026rsquo;ve become very fond of is the Pomodoro Technique. I don\u0026rsquo;t have any problems focusing for long periods, but I still decided to try it. I use the standard 25-minute study with a 5-minute break routine, and after four cycles, I take a 30-minute break.\nThe Pomodoro technique has been a way to force myself to take breaks, which I wasn\u0026rsquo;t used to. I used to chip away at a specific task for hours. However, I discovered that when I take a break, walk around for five minutes, and apply myself to the task again, my mind is in a fresh state and much more receptive to the information. Perhaps the time I spend studying after a break is actually more productive because the mind had a little rest.\nThe technique also pushed my limits a bit more. I study more hours a day, considering that I also work full time. There is this moment where I want to quit studying, but I ask myself, \u0026ldquo;do I have another Pomodoro in me?\u0026rdquo;\nNow that I\u0026rsquo;ve gotten used to breaking things up into 25-minute chunks of time, I started using the Pomodoro technique for other areas in life as well, such as blog writing or coding projects.\nYou can use any tool you like to start using the Pomodoro technique and pick any break schedule that suits you. I\u0026rsquo;ll link some resources below. All you need is some sort of timer. You can use a timer on your computer or a physical timer. I use the Forest app on my iPhone because it integrates with the iOs \u0026ldquo;do not disturb\u0026rdquo; and \u0026ldquo;focus\u0026rdquo; modes, so I don\u0026rsquo;t get any distracting notifications when I\u0026rsquo;m on a Pomodoro.\nhttps://science.nichd.nih.gov/confluence/display/newsletter/2020/05/07/The+Pomodoro+Technique%3A+An+Effective+Time+Management+Tool\nhttps://www.youtube.com/watch?v=5WRO79zuJ4U\n","permalink":"https://mischavandenburg.com/zet/pomodoro/","summary":"I did a lot of studying last year, and I achieved a few tough certifications. I\u0026rsquo;ve always been good at studying and never struggled with getting decent grades in university. As a result, I never felt the need to use particular techniques to pass my tests. However, now that I need to do my studies combined with a full-time job, I did some optimization and looked into study techniques.\nOne technique I\u0026rsquo;ve become very fond of is the Pomodoro Technique.","title":"The Pomodoro technique has won me over"},{"content":"Last modified: 2023-01-10\nIn this evening\u0026rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:\n!#/bin/bash echo -e ${PATH//:/\\\\n} I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I\u0026rsquo;m used to the sed / vim syntax: s/foo/bar\nAfter some research I learned that \u0026lsquo;//\u0026rsquo; is a global search and replace syntax of several text processing programs. It is known as parameter expansion in bash.\nExample:\nfoo=\u0026#34;1234567890\u0026#34; echo \u0026#34;${foo//[0-9]/x}\u0026#34; This replaces all the digits in the $foo variable with \u0026lsquo;x\u0026rsquo;, so the output would be xxxxxxxxxx\nTo do this with sed, you would do:\necho \u0026#34;$foo\u0026#34; | sed \u0026#39;s/[0-9]/x/g\u0026#39; For more info:\nman bash\n/parameter expansion\n","permalink":"https://mischavandenburg.com/zet/slash-syntax-replace/","summary":"Last modified: 2023-01-10\nIn this evening\u0026rsquo;s studies I came across this bash script in a tutorial by Rob Muhlenstein:\n!#/bin/bash echo -e ${PATH//:/\\\\n} I could not make heads or tails of all these slashes and curly braces, since the output clearly indicated that search and replacement was being performed. I\u0026rsquo;m used to the sed / vim syntax: s/foo/bar\nAfter some research I learned that \u0026lsquo;//\u0026rsquo; is a global search and replace syntax of several text processing programs.","title":"Using parameter expansion as search and replace"},{"content":"My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.\nI set up this one for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!\nIt was very easy to set up. If you don\u0026rsquo;t have your own blog, you could configure it with a different RSS feed. Hacker News for example.\nhttps://github.com/gautamkrishnar/blog-post-workflow\nhttps://github.com/abhisheknaiidu/awesome-github-profile-readme\n","permalink":"https://mischavandenburg.com/zet/adding-posts-github-readme/","summary":"My friend gave me a nice tip for customizing the readme on my personal GitHub page. I discovered there is a whole world of plugins and customizations out there.\nI set up this one for my GitHub homepage. It uses a workflow to update the readme in my personal GitHub repo with the most recent posts from this blog, based on the RSS feed. Neat!\nIt was very easy to set up.","title":"Automatically adding my recent blog posts to my GitHub Readme"},{"content":"I\u0026rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.\nRecently I\u0026rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.\nAdvantages of using bash:\nthe default Linux shell available on any Linux system full documentation available anywhere at all times with man bash free software less dependent on external plugins and configurations more portable practice by working on the command line The fact that working on the commandline is already coding convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.\nI want to improve my bash scripting, and working in the bash shell will improve that just by virtue of doing my daily tasks on the command line.\nAlso I noticed I\u0026rsquo;ve gotten used to zsh\u0026rsquo;s excellent autocompletion and menu navigation. When I log in to servers at work, there is always this little moment of \u0026ldquo;oh, I don\u0026rsquo;t have that here\u0026rdquo;. I want to get better at bash so I\u0026rsquo;m not dependent on these external crutches anymore.\nAlso, I\u0026rsquo;m going to port my zsh configuration to bash. My current zsh configuration loads a bunch of plugins, and it is more of a hassle to get set up on a new system.\nI want to be able to pull my dotfiles repo and do very few steps to configure my environment.\nBut I\u0026rsquo;m going to miss that good-looking prompt with all the lovely icons!\n","permalink":"https://mischavandenburg.com/zet/back-to-bashics/","summary":"I\u0026rsquo;ve used zsh for nearly two years now. I have a custom setup with autocompletion and a good looking prompt.\nRecently I\u0026rsquo;ve been diving deeper into bash scripting, following tutorials by rwxrob. He emphasizes all the time that it is much better to stick to bash instead of zsh.\nAdvantages of using bash:\nthe default Linux shell available on any Linux system full documentation available anywhere at all times with man bash free software less dependent on external plugins and configurations more portable practice by working on the command line The fact that working on the commandline is already coding convinced me to leave my beloved customized prompt behind (for now) and go back to the basics.","title":"Back to Bas(h)ics: leaving zsh for now"},{"content":"I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.\nNaturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don\u0026rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.\nI searched for a long time but I could not find a solution. In the end I just ran shell commands from the pipeline to run the container on on the server.\nsteps: - script: | sudo docker stop svelte-test sudo docker rm svelte-test sudo docker run --name svelte-test -p 8080:80 -d mischavandenburg/svelte-test:$(Build.BuildId) You can find the full pipeline code, the app and Dockerfile in my lab repo:\nhttps://github.com/mischavandenburg/lab/tree/main/azure-pipelines/docker-to-azure-vm\n","permalink":"https://mischavandenburg.com/zet/docker-to-azure-vm/","summary":"I wanted to build an application from a Dockerfile and deploy it to a VM. I used a default Svelte setup as an example app.\nNaturally, Azure prefers that you deploy containers to services such as Azure Container Instances or App Services, so they don\u0026rsquo;t provide modules for the pipelines to deploy to docker servers as far as I could tell.\nI searched for a long time but I could not find a solution.","title":"How to build and deploy a Docker container to an Azure VM using Azure Pipelines"},{"content":"To reach a VM from Azure Pipelines, you need to set up an environment.\nCreate your Linux VM in Azure.\nIn Azure DevOps, click envirnoments, new, and select the Virtual Machine option.\nA command is generated for you. SSH into your VM and run the command.\nNow the VM should show up under environments in Azure DevOps.\nSet up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.\ntrigger: - main pool: vmImage: ubuntu-latest jobs: - deployment: VMDeploy displayName: Deploy to VM environment: name: dev resourceType: VirtualMachine strategy: runOnce: deploy: steps: - script: echo \u0026#34;Hello world\u0026#34; You can see it when the deploy runs on the VM:\n","permalink":"https://mischavandenburg.com/zet/azure-pipelines-deploy-vm/","summary":"To reach a VM from Azure Pipelines, you need to set up an environment.\nCreate your Linux VM in Azure.\nIn Azure DevOps, click envirnoments, new, and select the Virtual Machine option.\nA command is generated for you. SSH into your VM and run the command.\nNow the VM should show up under environments in Azure DevOps.\nSet up a repo with an azure-pipelines.yml with these contents to test. under environment, set the same name as you did in Azure DevOps for your environment.","title":"How to deploy to a Linux VM in Azure with Azure Pipelines"},{"content":"For a project I\u0026rsquo;m setting up my environment with Terraform.\nI used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.\nI also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.ssh/id_rsa.pub\nTo run:\nterraform init terraform plan terraform apply The scripts prints the public IP of the newly created VM. You should be able to SSH to it:\nssh azureuser@the_printed_ip_address\nYou can find the code in my \u0026ldquo;lab\u0026rdquo; repo on GitHub.\nhttps://github.com/mischavandenburg/lab/tree/main/terraform/azure-simple-linux-vm\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/linux/quick-create-terraform\nhttps://learn.microsoft.com/en-us/azure/developer/terraform/authenticate-to-azure?source=recommendations\u0026amp;tabs=bash#authenticate-to-azure-via-a-microsoft-account\n","permalink":"https://mischavandenburg.com/zet/terraform-linux-vm/","summary":"For a project I\u0026rsquo;m setting up my environment with Terraform.\nI used this tutorial, but modified the code to make it simpler and easier to understand for beginners. The original uses a random module to generate random names, and generates a new SSH key. Also, this tutorial uses expensive VM tiers and Premium storage, which are not necessary when you are learning.\nI also thought the SSH configuration was overcomplicated. My version just takes an SSH keypair stored at ~/.","title":"Deploying a Linux VM to Azure with Terraform"},{"content":"I use the Obsidian app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.\nTelescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.\nTo pass the -L flag, and some other flags, I added the following to my telescope config file:\n-- Custom ripgrep configuration: local telescope = require(\u0026#34;telescope\u0026#34;) local telescopeConfig = require(\u0026#34;telescope.config\u0026#34;) -- Clone the default Telescope configuration local vimgrep_arguments = { unpack(telescopeConfig.values.vimgrep_arguments) } -- I want to search in hidden/dot files. table.insert(vimgrep_arguments, \u0026#34;--hidden\u0026#34;) -- I don\u0026#39;t want to search in the `.git` directory. table.insert(vimgrep_arguments, \u0026#34;--glob\u0026#34;) table.insert(vimgrep_arguments, \u0026#34;!**/.git/*\u0026#34;) -- I want to follow symbolic links table.insert(vimgrep_arguments, \u0026#34;-L\u0026#34;) telescope.setup({ defaults = { -- `hidden = true` is not supported in text grep commands. vimgrep_arguments = vimgrep_arguments, }, pickers = { find_files = { -- `hidden = true` will still show the inside of `.git/` as it\u0026#39;s not `.gitignore`d. find_command = { \u0026#34;rg\u0026#34;, \u0026#34;--files\u0026#34;, \u0026#34;--hidden\u0026#34;, \u0026#34;--glob\u0026#34;, \u0026#34;!**/.git/*\u0026#34;, \u0026#34;-L\u0026#34; }, }, }, }) Based on the configuration examples found on the project\u0026rsquo;s GitHub page.\nhttps://github.com/nvim-telescope/telescope.nvim\n","permalink":"https://mischavandenburg.com/zet/neovim-telescope-follow-symlinks/","summary":"I use the Obsidian app, but I mostly write and search my notes with neovim. I added my zet directory from this blog repo into the Obsidian vault as a symbolic link, but I soon discovered that these files were not being searched.\nTelescope.nvim uses ripgrep (rg) to do the live grepping in its search, and ripgrep does not follow symbolic links by default. You need to pass the -L flag to it.","title":"How to follow symbolic links while searching with Telescope in neovim"},{"content":"When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won\u0026rsquo;t attach.\nYou can fix this by setting the correct file type for the current buffer:\n:set ft=yaml.ansible\nYou could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day. Not all yaml files are to be used with Ansible.\nThere is logic for the Ansible language server to figure out if you are working on Ansible yaml files based on the directory structure you\u0026rsquo;re working in.\nSo setting the filetype when I needed works well for me.\nhttps://www.reddit.com/r/neovim/comments/tbd7g0/lsp_ansiblels_wont_attach_anymore/\n","permalink":"https://mischavandenburg.com/zet/ansible-lsp-fix/","summary":"When you have an Ansible language server installed, you might find that your yaml LSP will attach to your current buffer, but the ansible language server won\u0026rsquo;t attach.\nYou can fix this by setting the correct file type for the current buffer:\n:set ft=yaml.ansible\nYou could also adjust the Ansible LSP so it attaches to all yaml files. However, this does not work out for me, because I edit different yaml files for different purposes every day.","title":"Attaching the Ansible Language Server to yaml files in neovim (LSP)"},{"content":"Make sure to have pip installed.\nRun pip install python-openstackclient\nPip will install a binary called \u0026ldquo;openstack\u0026rdquo; in ~/.local/bin\nIf the openstack command is not available in your session, you might need to add it to your PATH:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.zshrc\nhttps://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html\n","permalink":"https://mischavandenburg.com/zet/install-openstack-cli/","summary":"Make sure to have pip installed.\nRun pip install python-openstackclient\nPip will install a binary called \u0026ldquo;openstack\u0026rdquo; in ~/.local/bin\nIf the openstack command is not available in your session, you might need to add it to your PATH:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.","title":"How to install the Openstack CLI on Linux"},{"content":" Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it. Source the RC file to make the environment variables avaialable to your current session: source ~/my_openstack.sh Find the instance ID of your VM from the portal. Run openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd You will be prompted for the new password which will be set on the virtual machine. ","permalink":"https://mischavandenburg.com/zet/openstack-root-password/","summary":" Download the Openstack RC file from the Openstack portal. Click your username in the top right corner to find it. Source the RC file to make the environment variables avaialable to your current session: source ~/my_openstack.sh Find the instance ID of your VM from the portal. Run openstack server set --root-password be3xxxx5-8348-418b-xxxb-c4xxxx575cd You will be prompted for the new password which will be set on the virtual machine. ","title":"How to Reset a VM Root Password using the Openstack CLI"},{"content":"When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.\nIf this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.\nYou can find the location of your binaries by running pip show package_name\nUsually the binaries will be located in ~/.local/bin on a UNIX based system.\nTo add this to your path, run:\nexport PATH=\u0026quot;$HOME/.local/bin:$PATH\u0026quot;\nAdd this to your ~/.zshrc or ~/.bashrc to make sure this happens for each shell session.\nDon\u0026rsquo;t forget to source your updated ~/.zshrc if you chose to add it:\nsource ~/.zshrc\nhttps://stackoverflow.com/questions/29980798/where-does-pip-install-its-packages\n","permalink":"https://mischavandenburg.com/zet/run-installed-pip-packages/","summary":"When you install a pip package which is meant to be run from the command line as a command, you might find that it is not available to you after installation.\nIf this happens, it might be that the path is missing from your PATH variable. Therefore, the shell does not source these binaries when initiated, and does not know that these executables exist.\nYou can find the location of your binaries by running pip show package_name","title":"How to run installed pip packages as binaries"},{"content":"Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.\nWhen you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.\nI think this is such an interesting point. I\u0026rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line. However, when I was writing bash scripts, I did consider myself to be coding. There is literally no difference. A bash script is just a string of commands that you would enter manually anyway.\nhttps://rwx.gg/\n","permalink":"https://mischavandenburg.com/zet/bash-cmdline-is-coding/","summary":"Rob Muhlenstein makes an interesting point that using bash on the command line is already coding. When you are running commands in the terminal, you are coding one line at a time.\nWhen you put these commands in a file you have a bash script. Therefore, he argues that bash should be your first language.\nI think this is such an interesting point. I\u0026rsquo;ve been using Linux and working on the command line for years but it never dawned on me that I, in fact, was coding while working on the command line.","title":"Working on the command line is already coding"},{"content":"Sed, it\u0026rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.\nToday I needed to update my /articles/ links to /zet/articles/ links because I\u0026rsquo;m restructuring my website. Here is the sed expression that is executed for every markdown file that is found by fd:\nsed -i \u0026#39;s/\\/articles\\//\\/zet\\/articles\\//g\u0026#39; $(fd .md) The result:\ndiff --git a/content/zet/move-to-zet.md b/content/zet/move-to-zet.md index 1e37283..3b817e3 100644 --- a/content/zet/move-to-zet.md +++ b/content/zet/move-to-zet.md @@ -6,7 +6,7 @@ tags: --- -I\u0026#39;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my [Obsidian](/articles/obsidian-introduction/) vault. +I\u0026#39;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my [Obsidian](/zet/articles/obsidian-introduction/) vault. These sites are super useful to help you formulate your expressions:\nhttps://sed.js.org/\nhttps://regex101.com/\n","permalink":"https://mischavandenburg.com/zet/fall-in-love-with-sed/","summary":"Sed, it\u0026rsquo;s so powerful. I remember I struggled with finding practical uses for it when I did my LPIC-1 certification. But now I find myself using it several times a week. It is so powerful to edit multiple files at a time. I use it for work, but also for making changes to my entire second brain in Obsidian with one command.\nToday I needed to update my /articles/ links to /zet/articles/ links because I\u0026rsquo;m restructuring my website.","title":"Fall in love with sed."},{"content":"Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.\nSampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.\nFor small and medium sized applications sampling is generally not necessary.\nAdvantages of sampling:\nThrottling data when the application suddenly sends a high volume of telemetry in a short time This saves costs! Keeping a pricing tier quota Reduce network traffic from telemetry collection Three different kinds of sampling:\nadaptive sampling automatically adjusts volume of telemetry from ASP.NET or Azure Functions only for these two fixed-rate sampling rate is set by the administrator use when you have a clear idea of the appropriate sampling percentage reduces volume from ASP.NET or ASP.NET Core server Java server Python applications User browsers ingestion sampling used when monthly quota is often met reduces amount of processed and retained traffic by Application Insights less processing = less cost doesn\u0026rsquo;t reduce telemetry traffic sent from the app happens at Applications Insight service endpoint disabled if SDK samples telemetry can set sampling rate without redeploying the app only applies when no other sampling is in effect supports all Application Insights SDK\u0026rsquo;s ","permalink":"https://mischavandenburg.com/zet/application-insights-sampling/","summary":"Telemetry is the collection of measurements or other data at remote points, and transmitting that data to a receiver for monitoring.\nSampling is used to reduce telemetry traffic and costs for storage and data in Application Insights.\nFor small and medium sized applications sampling is generally not necessary.\nAdvantages of sampling:\nThrottling data when the application suddenly sends a high volume of telemetry in a short time This saves costs! Keeping a pricing tier quota Reduce network traffic from telemetry collection Three different kinds of sampling:","title":"Application Insights: Telemetry Sampling"},{"content":"This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.\nContinuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.\nYou can set up several of these checks. If all these checks pass, the pipeline can proceed.\n","permalink":"https://mischavandenburg.com/zet/pipelines-continuous-monitoring/","summary":"This term can be confusing. Initially I thought it meant monitoring of the pipelines themselves. However, in the context of Azure Release Pipelines, continuous monitoring refers to something else.\nContinuous monitoring leverages metrics from other services such as Application Insights. You can set up release gates based on these metrics. For example, you can set up a release gate to roll back the deployment if an alert is being fired for high CPU usage in the application.","title":"Pipelines: Continuous Monitoring"},{"content":"Debugging is done using call stacks in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.\nUsing microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.\nDistributed tracing is an implementation of the call stack in the cloud. It is usually implemented by adding an agent, SDK, or library to the service. In Azure you can enable distributed tracing via Application Insights through auto-instrumentation or SDKs.\nUnified cross-component transaction diagnostics\n","permalink":"https://mischavandenburg.com/zet/distributed-tracing/","summary":"Debugging is done using call stacks in monolithic applications. Nowadays it is more common to deploy an application using a microservices architecture. Microservices make it easier to update certain parts of the application, and allow for more frequent deployments.\nUsing microservices does have a disadvantage: you cannot use the local call stack for debugging, because calls are sent to different microservices.\nDistributed tracing is an implementation of the call stack in the cloud.","title":"Distributed Tracing"},{"content":"A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.\nSDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.\n","permalink":"https://mischavandenburg.com/zet/sdk/","summary":"A software development kit (SDK) is a set of tools provided by the manufacturer of (usually) a hardware platform, operating system (OS), or programming language.\nSDKs contain all the tools you need to get started. They typically contain a compiler, a debugger and an API. But they can also contain documentation and testing tools.","title":"What is a SDK?"},{"content":"All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).\nYou are free to: Share — copy and redistribute the material in any medium or format\nUnder the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\nNonCommercial — You may not use the material for commercial purposes.\nNoDerivatives — If you remix, transform, or build upon the material, you may not distribute the modified material.\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\n","permalink":"https://mischavandenburg.com/zet/articles/copyright-license/","summary":"All of the content on this blog is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0).\nYou are free to: Share — copy and redistribute the material in any medium or format\nUnder the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.","title":"Copyright and License"},{"content":" As I alluded to in my article about Obsidian, I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.\nPreviously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.\nI discovered Hugo and I fell in love with it immediately.\nHugo is a static site generator based on markdown files. My entire blog is written in markdown files which are stored in a GitHub repo.. I write my blog posts in vim and when I\u0026rsquo;m done I use Hugo to generate the updated website.\nThe result is what you see in the \u0026ldquo;public\u0026rdquo; directory in the GitHub repo. This public directory is pushed to a different repo which is hooked up to my hosting provider. My hosting provider uses Plesk, and with Plesk I have the option to connect the GitHub repo to the web server with a webhook. When I push to my hosting repo, the contents are gathered by the webserver and served as public web content.\nMy complete writing and publishing workflow looks like this:\nCreate a new markdown file Write the note or article Save the file and run the \u0026ldquo;hugo\u0026rdquo; command to regenerate the website Run the \u0026ldquo;publish\u0026rdquo; script. This is a custom script I wrote that takes the contents of the \u0026ldquo;public\u0026rdquo; directory to my hosting repo Push the newly generated website to the hosting repo And we\u0026rsquo;re live! 🚀 🎉 It is such a smooth and convenient process. I can literally have a new note published to the interet within a few minutes, and it is all done from the command line using my favorite tools.\nBlog GitHub repo\n","permalink":"https://mischavandenburg.com/zet/articles/how-this-blog-is-created/","summary":"As I alluded to in my article about Obsidian, I am very fond of editing my text in neovim. Naturally, if you want to edit in neovim, you need to have your text as local files. I keep all of my personal notes in markdown.\nPreviously I was using WordPress, but the editing and writing experience became torture which I could not endure any longer. I looked for a different solution that would allow me to edit my files locally instead of in the browser.","title":"How This Blog is Created, Written and Hosted"},{"content":"When you call a function, the system sets aside space in memory for the function to do its work. Those chunks are called \u0026ldquo;stack frames\u0026rdquo; or \u0026ldquo;function frames.\u0026rdquo;\nThese frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.\nThe function that is actually doing something at the moment is on top of the stack and is known as the \u0026ldquo;active frame.\u0026rdquo;\nWhen the function finishes its work, the frame is popped off of the stack. The frame in second place becomes the active frame. It had been paused in the meantime, and now it is active again, because it is on top.\nFunctions that are not on top, are not running.\nThis video explains it well.\n","permalink":"https://mischavandenburg.com/zet/call-stacks/","summary":"When you call a function, the system sets aside space in memory for the function to do its work. Those chunks are called \u0026ldquo;stack frames\u0026rdquo; or \u0026ldquo;function frames.\u0026rdquo;\nThese frames are arranged in a stack. The frame for the most recently called function is always at the top of the stack. When a new function is called, it becomes the active frame, and it is on top of the stack.","title":"Call Stacks"},{"content":"I\u0026rsquo;m starting a project with a friend. Developing an application. We make a good team, he\u0026rsquo;s great at coding and knows the backend too.\nHe\u0026rsquo;ll do the development, I\u0026rsquo;m in charge of hosting. We\u0026rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I\u0026rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.\nEven though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application. I\u0026rsquo;ll set up a full CI/CD pipeline with testing in a secure manner. Credentials stored in an Azure key vault and images pushed to a private registry.\nThis is going to be fun!\n","permalink":"https://mischavandenburg.com/zet/starting-a-project/","summary":"I\u0026rsquo;m starting a project with a friend. Developing an application. We make a good team, he\u0026rsquo;s great at coding and knows the backend too.\nHe\u0026rsquo;ll do the development, I\u0026rsquo;m in charge of hosting. We\u0026rsquo;re setting everything up in Azure DevOps, so it is a great way to practice my Azure skills and apply the things I\u0026rsquo;ve learned in my recently obtained AZ-104 Azure Administrator certification.\nEven though it is a small scale hobby project, I still plan to approach it as if it was an enterprise production application.","title":"Starting a Project"},{"content":"I\u0026rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my Obsidian vault.\nThe concept of \u0026ldquo;atomic notes\u0026rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.\nUp until this point I\u0026rsquo;ve been publishing full articles on my blog. I came across Rob Muhlestein yesterday and I was very inspired by his setup and public zettelkasten. I think I\u0026rsquo;ll move to a similar approach. Still planning to write and publish full articles as well, but also including atomic notes and personal status updates.\n","permalink":"https://mischavandenburg.com/zet/move-to-zet/","summary":"I\u0026rsquo;ve transitioned my note taking system towards a Zettelkasten system. I still use directories for folders and make copious links, but more often than not I put them in the larger generic 00-zettelkasten directory in my Obsidian vault.\nThe concept of \u0026ldquo;atomic notes\u0026rdquo; is also very important in Zettelkasten methods. Notes should be small and concise.\nUp until this point I\u0026rsquo;ve been publishing full articles on my blog. I came across Rob Muhlestein yesterday and I was very inspired by his setup and public zettelkasten.","title":"Going to Publish Smaller, and More Often"},{"content":"As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.\nNot only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.\nIt can be challenging to keep everything memorized at all times. However, when I started my journey to become a DevOps engineer, I kept meticulous notes of everything I learned, and this practice has been highly beneficial so far. This blog is a result of the notes I keep every day.\nNote-taking is an essential part of the DevOps engineer\u0026rsquo;s toolkit. It allows you to stay organized, track changes, document processes, and keep track of important information. Using a powerful note-taking app like Obsidian can streamline your note-taking process and work more efficiently.\nObsidian for DevOps engineers Having a reliable and efficient system in place for managing your notes and documentation is crucial. That\u0026rsquo;s where Obsidian comes in. Obsidian is a powerful note-taking app that can help you organize and manage your notes more efficiently.\nOne of the main features of Obsidian is its use of \u0026ldquo;vaults.\u0026rdquo; A vault is a folder containing your notes as markdown text files. Your notes are stored on your machine rather than in the cloud. This gives you complete control over your data. You always have access to your local text files and can interact with them or back them up as you see fit.\nBecause your notes are stored as markdown files, you can use different tools to write or edit your notes. I mostly use neovim for editing, but I use the Obsidian application for making new links and visualization. Moreover, storing your notes as files allows you to run python scripts on your notes and customize your workflow as needed. This can be especially useful for automating tasks, streamlining your work, or making bulk updates.\nLinks and graph view In addition to its local storage capabilities, Obsidian also offers several other valuable features for DevOps engineers. For example, you can use the \u0026ldquo;graph view\u0026rdquo; to visualize your notes and see how they\u0026rsquo;re related to one another. This can be particularly useful for understanding complex systems and tracking changes over time.\nThese relations between notes are created by \u0026ldquo;[[markdown links]]\u0026rdquo;. When a note receives many links, its dot size will increase on the graph view, and in this manner, it is easy to see which notes or topics are significant in your vault and play an important role in your life.\nThis is the local graph view, showing all the linked notes to my Linux note.\nConclusion Many note-taking apps, such as Evernote, Notion, or Roam research, are available. I tried a few of these, but Obsidian was the best solution for me. Mainly because your notes are stored as markdown files on your machine and because they offer a syncing service with end-to-end encryption. Their graph view provides an interesting way to navigate your notes and discover unexpected connections.\nGive it a try. It\u0026rsquo;s free.\nObsidian website\n","permalink":"https://mischavandenburg.com/zet/articles/obsidian-introduction/","summary":"As a DevOps or Cloud engineer, you work with many different technologies daily. Therefore, a good engineer needs a solid foundation in Linux, a lot of knowledge about your cloud solution, networking, CI/CD, at least one programming language, and many other topics.\nNot only are there many of these categories, but within these categories, there are several alternatives. For example, in the category of infrastructure management, there is Ansible, Puppet, or Chef, each with its particular approach and configuration methods.","title":"Obsidian: A Note Taking App For DevOps Engineers"},{"content":"When you learn about DevOps, you will come across the concept of a container early on. This is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nVirtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating \u0026ldquo;fake computers\u0026rdquo; or \u0026ldquo;virtual computers\u0026rdquo; on a physical computer.\nOn your desk, you have a laptop or a desktop PC. This machine has hardware such as a motherboard, a hard disk, and a video card. To run programs on your computer, it needs an Operating System. Usually, Windows, macOS, or Linux are used.\nLet\u0026rsquo;s say you have a computer running Windows, but you want to run a program that can only run on Linux. One solution is to buy another laptop and put it beside your Windows laptop on your desk. So now you have two computers with two different operating systems.\nFortunately, there are other solutions. We can use virtualization to make a Virtual Machine. A virtual machine is created by software to imitate a fully functional running computer inside your current operating system. You can create a virtual machine that runs Linux on your Windows computer. Your Windows computer running the Linux virtual machine is known as the **host.\nNow you don\u0026rsquo;t need to buy another computer to run your Linux program. Instead, you can boot up your Linux virtual machine and run your program when needed. If you have a powerful computer, you could run ten or more virtual machines, each of which has its own operating system and custom environment.\nContainers Every time you create a virtual machine, the virtual machine needs a complete operating system to work. So, first, the software creates a virtual processor, virtual video card, and a virtual network interface. Then, it runs a fully functional operating system on that virtual hardware. This takes up a lot of resources.\nContainers are lightweight packages of software. They are designed to do a very specific task, and therefore they only contain the resources they need to do that task. Nothing more.\nContainers use the operating system of the physical computer to run. They have a very minimal, lightweight operating system inside them, but it only contains the elements they need to do their specific task. Therefore, containers are very easy to distribute, and you can run them very quickly.\nContainers are like newspapers Containers are like newspapers. Newspapers have a particular task: providing you with the day\u0026rsquo;s news. You cannot use newspapers to study for your mathematics exam. You use your math book to study for your math exam. If you want to be informed of the day\u0026rsquo;s news, you use a newspaper. This is what I mean by containers having a specific task.\nNext, newspapers are printed on a specific kind of paper. When you buy an expensive book, it will have a sturdy and durable cover, and the pages are made of nice thick paper that will last a long time. The pages don\u0026rsquo;t tear very quickly, and when the book gets wet, it can withstand it. This thick cover and high-quality papers are like the operating system of a virtual machine.\nNewspapers, on the other hand, are printed on very thin paper. Because they are designed to distribute the news to you effectively, newspapers do not need to be stored forever or do any other tasks. If you used thick, expensive paper for newspapers, they would become costly, and no one would buy them anymore. The paper is optimized to bring the news to you.\nIn the same way, the container only comes with the components it needs to do its specific task. Therefore, the container is optimized for its purpose. As a result, they can be distributed more quickly and do not take up a lot of resources when running.\nThere are other benefits to containers, such as improving the ability to autoscale your application, but I will expand on those in a future blog post.\nFurther study To learn more about containers, you can use the following resources:\nContainers \u0026amp; Friends from John Savill\u0026rsquo;s DevOps Masterclass\nDocker Documentation\nDocker Tutorial for Beginners\n","permalink":"https://mischavandenburg.com/zet/articles/what-are-containers/","summary":"When you learn about DevOps, you will come across the concept of a container early on. This is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nVirtualization To understand containers, we need to understand virtualization. Virtualization is the process of creating \u0026ldquo;fake computers\u0026rdquo; or \u0026ldquo;virtual computers\u0026rdquo; on a physical computer.","title":"What Are Containers?"},{"content":"I\u0026rsquo;ve been using neovim for over a year now, and I\u0026rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.\nI started with an empty vanilla vim config and slowly added the plugins as I went along. Videos by content creators such as ThePrimagen were also helpful in getting inspiration on which plugins I might like for my setup. But this might not be suitable for everyone. I was only editing yaml files and writing simple Python scripts at the time, whereas you might be looking for an IDE experience out of the box.\nRecently I discovered kickstart.nvim by neovim core maintainer TJdeVries.\nI decided to give it a try, and I was pleasantly surprised. It is a great starting setup for a beginner. It is simple and does not overwhelm you with thousands of features.\nI\u0026rsquo;ve completely rewritten my config based on kickstart.nvim, and I am delighted with the result. Especially the LSP setup is very well thought out, and it works much better than the setup I came up with on my own.\nTJ DeVries also made a video introducing kickstart.nvim and going through the initial setup.\nI highly recommend kickstart.nvim if you are interested in using neovim and are looking for a sane place to start.\nlinks kickstart.nvim\nKickstart video by TJdeVries\n","permalink":"https://mischavandenburg.com/zet/articles/kickstart-nvim/","summary":"I\u0026rsquo;ve been using neovim for over a year now, and I\u0026rsquo;m very happy that I went through the initial difficulty of learning vim. One of the best perks of using neovim is that you can customize your entire editing experience and workflow. However, it can be a daunting experience to start with an empty configuration and set up everything from scratch.\nI started with an empty vanilla vim config and slowly added the plugins as I went along.","title":"Getting Started with Neovim: kickstart.nvim"},{"content":"TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo\nIntroduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.\nThat opinion has changed since I\u0026rsquo;ve obtained a few IT certifications. These tests are hard! I\u0026rsquo;m typing this while sitting on the bus on my way home from my AZ-104 exam. I passed with an 860 score of 1000, where 700 or higher is a pass. But it was an astonishingly tough exam. Usually, I finish quickly and spend at least half an hour reviewing my answers. I had only 5 minutes to review my questions this time because I had used up all of the available time. The questions required intense concentration and were time-consuming because I needed to compare many options which were very similar to each other. There were no easy questions.\nPreparation I studied 80 hours for this exam in a month. I work full-time as a DevOps Engineer, so I study in the evenings and on weekends. I have my Azure Fundamentals and CKA, but I only work with Azure occasionally in my current role.\nHere is what I did to prepare for my exam:\nGo through all of the Microsoft Learn modules for the AZ-104 Watch the entire AZ-104 study list by John Savill Practice exams on TutorialsDojo until I could pass them with 90%+ scores Microsoft ESI practice exams Microsoft AZ-104 Exam prep videos Microsoft Learn You really need to master all of the subject matter. Only completing the Microsoft Learn modules is not enough preparation. They are more like summaries. At the end of each module, they provide links to the documentation for the subject for further study. Unfortunately, Microsoft does not go easy on you. It expects you to know obscure details of nearly every service this exam covers. Therefore, I advise going beyond the Microsoft Learn modules and studying the linked articles after each module.\nYouTube AZ-104 Study Playlist by John Savill I\u0026rsquo;m not sure if it\u0026rsquo;s better to watch this playlist first and then do the Microsoft modules or the other way around. I did the Microsoft modules first, but for my next exam (AZ-400 DevOps Expert), I\u0026rsquo;ll start with the videos and then do the Microsoft Learn modules.\nTutorialsdojo These practice exams are excellent. I used them in preparation for my fundamentals exam.\nThe best thing about them is that they provide extensive documentation and explanation of the questions. So after you finish the exam, you can study a lot with these examples.\nESI Practice Exams You\u0026rsquo;re lucky if your organization participates in Microsoft\u0026rsquo;s Enterprise Skills Initiative. The practice exams provided in the ESI environment give you a good indication of what you can expect at the exam. I first did the Tutorialsdojo exams and then moved on to the ESI exams, and I was humiliated. The ESI questions are very complex and hard to solve, and I learned a lot from these exams.\nThere are 210 questions total, and I worked through all of them, and whenever I failed a question, I did a deeper dive into the question\u0026rsquo;s theme.\nstudying I take notes in Obsidian, and I use Anki for spaced repetition. I highly recommend keeping a deck of Anki cards and continuously testing yourself. You will need to memorize a lot of details. For example, you are expected to remember that storage accounts of the FileStorage type do not support Geo Redundant Storage. You can find my Anki deck in the GitHub repo.\nlinks AZ-104 Exam page with learning modules\nJohn Savill\u0026rsquo;s AZ-104 Study playlist\nGitHub repo containing my notes and Anki deck\nMicrosoft AZ-104 Exam prep videos\n","permalink":"https://mischavandenburg.com/zet/articles/az-104-study-guide/","summary":"TLDR It took me 80 hours of studying to gain this certification. Here are my notes and Anki deck: GitHub repo\nIntroduction When I did my English degree at University, exams were usually handwritten essays that needed to be done within a 3-hour timeframe. Sometimes we had multiple-choice tests, and I would always consider them a guaranteed pass because multiple-choice was much easier in my experience.\nThat opinion has changed since I\u0026rsquo;ve obtained a few IT certifications.","title":"Study Guide: AZ-104 Azure Administrator Associate"},{"content":"This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.\nMy goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure. I also wanted to try out ArgoCD and learn more about GitOps.\nThe application is a simple web app that I wrote which displays a quote in the browser:\nGitOps and Structure GitOps is used to automate the process of provisioning infrastructure. Infrastructure as code is used to generate the same environment every time the environment is deployed.\nFor my project I have two separate GitHub repos. The first repo contains the code for a simple web app I created and the Dockerfile to generate the image. I call this my application repo. The other repo is my GitOps repo which contains the manifest files to deploy the application in Kubernetes. I decided to leverage Helm to create my manifest files. This way I can create templates and define my desired values in a values.yaml file in the repo.\nUltimately my goal was to use an Azure pipeline to build an image from my application repo and push it to Docker hub. This new image is given a new tag which needs to be stored. The first pipeline should trigger a new pipeline that makes a pull request to the GitOps repo to update the tag in my Helm chart.\nArgoCD will then scan the GitOps repo and realize that the tag has been updated, and deploy the new tag to my cluster.\nMinikube I used minikube to deploy my local Kubernetes cluster. Another option is kind (Kubernetes In Docker) but I wanted to use a VM approach this time.\nArgoCD ArgoCD is a declarative GitOps continuous delivery tool for Kubernetes. This is the solution I used to continuously scan my GitOps repo. When ArgoCD detects a change in the desired state, it will compare it with the state in my running cluster and make changes accordingly. I found a really good tutorial to run ArgoCD in minikube.\nAzure Pipelines With my cluster running on my local machine and my repos set up, I needed to use Azure Pipelines to bring it all together. Building the image and pushing it to Docker Hub wasn\u0026rsquo;t a big deal. But I had two big challenges in my desired setup: I needed to pass the new tag number to a new pipeline, and I needed to use Azure Pipelines to create a new PR to my GitOps repo.\nPassing a value from one pipeline to another Interestingly, this wasn\u0026rsquo;t as easy as it sounds, and from my internet searching it seemed that many people struggled with this. I decided to use the Variable Groups in Azure DevOps. However, after I finished writing my pipeline, I discovered I had no problems with reading the value from the Variable Groups, but it was impossible to update it using existing pipeline tasks. So I had to a bit of hacking to make it work. In the end I had to use the Azure CLI from within the pipeline to update my variable:\n- stage: update_tag jobs: - job: update_tag_variable displayName: Update Tag Variable steps: - bash: | az pipelines variable-group variable \\ update --group-id 202 \\ --org $(System.CollectionUri) \\ --project $(System.TeamProject) \\ --name tag --value $(Build.BuildId) env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) This didn\u0026rsquo;t feel like a very elegant solution, but it was the only solution I could come up with.\nI also struggled a lot with permissions. I needed to find the correct service principal to assign the administrator rights to. This post really helped to solve my problem.\nSubmitting a PR to a GitHub repo When I started writing my pipeline I thought it would be very straightforward to just submit a PR to a repo, but I quickly discovered that this is not natively supported in Azure pipelines yet. In fact, I could not find a way to submit a PR at all. I had to settle for a solution that checks out the GitOps repo and creates a new branch. This new branch updates the tag in the values.yaml with the new tag that was passed from the previous pipeline.\nvariables: - group: mischa-quote - name: passed_tag value: $[variables.tag] - name: branch_name value: \u0026#34;pipeline-$(passed_tag)\u0026#34; pool: vmImage: ubuntu-latest steps: - checkout: self persistCredentials: true clean: true - script: | git config --global user.email \u0026#34;mischa@pipeline.com\u0026#34; git config --global user.name \u0026#34;Mischa Pipeline\u0026#34; git switch -c \u0026#34;$(branch_name)\u0026#34; sed -i \u0026#34;s/tag:.*/tag: $(passed_tag)/\u0026#34; values.yaml git add . git commit -m \u0026#34;Update tag to $(passed_tag)\u0026#34; git push --set-upstream origin \u0026#34;$(branch_name)\u0026#34; This also felt a bit hacky to do with explicit shell commands, but it was the only way I could find to achieve my goal. I used sed to update the tag.\nResult The resulting deployment pipeline is as follows.\nI make a commit to my application repo, which triggers a build pipeline in Azure DevOps: This resulted in an image pushed to my Docker Hub: The pipeline created a new branch in my GitOps repo. Unfortunately, I have to make the PR myself, but as you can see, the pipeline successfully updates the values.yaml with the new tag which we also saw in Docker Hub: When I merged the pull request, ArgoCD detected the change and deployed a new pod with the new tag. Running a kubectl describe on the pod also verifies that we have the correct image: Conclusion This was a fun challenge, but I learned a lot from solving the problems I encountered and my entire Saturday flew by in an uninterrupted flow state. I had some good practice in setting up Azure pipelines, learned about Helm, and did my first implementation of GitOps. Not bad for a day\u0026rsquo;s work!\nLinks Application GitHub repo\nGitOps repo\nminikube\nkind\nArgoCD\ntutorial to run ArgoCD in minikube\n","permalink":"https://mischavandenburg.com/zet/articles/lab-argocd-azure-pipelines/","summary":"This weekend I had a lot of fun with a project. I wanted to learn more about GitOps and try out ArgoCD.\nMy goal was to be able to deploy an application from a GitHub repo to my local Kubernetes cluster running in minikube. There are many options I could have used, such as running Jenkins in my cluster. But I wanted to use Azure pipelines for practice, which complicates the deployment to my local cluster, because the cluster is not running on Azure.","title":"Lab Project: GitOps with ArgoCD, Azure Pipelines and Minikube"},{"content":"Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher\u0026rsquo;s account?\nThis is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.\nIdentity The first step in this process is identity. You need a starting point; for many of us, this can be our Google account. You signed up for this account and probably verified this with your phone number.\nThis relates to authentication. Authentication is the process of verifying identity. You\u0026rsquo;ll need to provide the correct password when you log in to your Google account. You must give a valid password to log in to your account and access the resources. Google uses your password to authenticate that it is you.\nAuthorization Then we have authorization. Authorization means granting access to particular resources. For example, let\u0026rsquo;s say you are working in the science classroom at school. In the classroom is a bookcase that everybody can use: it is not dangerous, and every student can take the books they need without asking for permission. In the back of the science classroom is a cabinet that contains chemicals. It would be very dangerous if everybody could go into the cabinet and take out the sulphuric acid. Not everybody might know how dangerous sulphuric acid is. That\u0026rsquo;s why the cabinet is locked.\nIf you need something from the chemicals cabinet, you need to ask permission from the teacher. You need to be authorized by the teacher to take out the sulphuric acid. When you make your request, the teacher may ask you questions to ensure you know what you are doing. He might even ask you for your school ID card because he has not seen you before. The teacher authenticates you by asking for your school ID, and then he authorizes you to take out the sulphuric acid.\nTokens How do we accomplish this on the internet?\nTo verify identities on the internet, we have identity providers. Google is an identity provider. Azure AD is also an identity provider. An open-source identity provider is Keycloak.\nIdentity providers use tokens to verify identity and authorize access to resources. There are two types of tokens: ID tokens and access tokens. And for each token, there is an associated protocol.\nID tokens OpenID Connect, also known as OIDC, is an open standard for authentication. Identity providers have agreed with each other that they will use this standard. When you go through an OpenID workflow, the result is an ID token, proving that the user has been authenticated.\nYour school ID card is the ID token in our science class example. When you started at your school, you went through a registration process. Your parents probably handled this. Your name was written down, and the school verified that it was you by looking at your passport and talking to your parents. The result of this process was your school ID card, which you use to borrow books from the library. The school ID card proves that you are a student of that school and that you can use the facilities at the school.\nAccess tokens These are specifically designed to allow access to a resource. For example, this resource could be a file on a server or a database.\nAccess tokens are strictly for authorization and use the OAuth 2.0 standard.\nIn our science class example, the token would be the key to the chemicals cabinet. The teacher authorizes you to access the cabinet and gives you the key to the cabinet.\nPutting it Together Now let\u0026rsquo;s put it together with an example.\nYou just created a new Facebook account and want to add all your friends. However, you have a Google account, and Facebook can use the contacts in your Google account to automatically add all of your friends.\nYour Google account can only be accessed by you, and your contacts are locked away behind a password. But it is possible to grant Facebook access to this.\nOn Facebook, you select the \u0026ldquo;import contacts from Google\u0026rdquo; function. Facebook sends you to Google, and Google will ask you to log in. Google is the teacher in our science class example. Google needs you to authenticate to prove that it is you. When this is done, Google generates an ID token using OIDC for Facebook: Google gives Facebook a school ID that it can use.\nNext, Facebook needs access to the contacts in your Google account. In our example, Facebook asks to take the sulphuric acid from the chemicals cabinet. You will see a menu that specifies what Facebook wants to do, and you need to give your permission. When you give your permission, Google generates an OAuth 2.0 token for Facebook. In other words, Google gives the key to the chemicals cabinet to Facebook, and Facebook is now authorized to take the sulphuric acid.\nWhen both of these tokens are generated, Facebook contacts Google and asks if it can take the sulphuric acid from the chemicals cabinet.\nGoogle, the teacher, asks Facebook for the school ID, and Facebook shows the ID card it received earlier. When Google is satisfied with the ID and successfully authenticates Facebook, it gives Facebook the key to the chemicals cabinet. Facebook is now authorized to take out the sulphuric acid. Facebook is now authorized to access the contacts in your Google account.\nLinks You can use these resources to learn more about this topic:\nAn Illustrated Guide to OAuth and OpenID connect\nID Tokens vs Access Tokens - Do you know the difference?\nMicrosoft Learn: ID Tokens\nMicrosoft Learn: Security Tokens\n","permalink":"https://mischavandenburg.com/zet/articles/identity/","summary":"Introduction Have you ever thought about your identity on the internet? How does LinkedIn know it is you when you log in to LinkedIn? And when you allow LinkedIn to post to your Twitter, how does LinkedIn access your account and not your kindergarten teacher\u0026rsquo;s account?\nThis is a \u0026ldquo;Mischa Explains\u0026rdquo; article where I attempt to explain a concept in my own words as simply as possible. I use the Feynman technique and pretend to explain it to a 12-year-old.","title":"Tokens and Identity on the Internet"},{"content":"An interesting podcast episode describing the system that runs Norway\u0026rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.\nIt was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.\nLink to the podcast episode:\nThe system that runs Norway\u0026rsquo;s welfare payments\n","permalink":"https://mischavandenburg.com/zet/articles/nav-podcast/","summary":"An interesting podcast episode describing the system that runs Norway\u0026rsquo;s welfare payments. It was interesting to hear that they focus on open source and that everything runs one one big kubernetes cluster.\nIt was also very inspiring to hear that they went from 4 nightly deployments a year to 1700 deployments a week.\nLink to the podcast episode:\nThe system that runs Norway\u0026rsquo;s welfare payments","title":"Podcast Tip: The system that runs Norway's welfare payments"},{"content":"I came across this little book while doing some research for my Jiro Dreams of Sushi article. While scanning the book\u0026rsquo;s description, I was intrigued by this sentence: \u0026ldquo;the happiness of always being busy.\u0026rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?\nIkigai \u0026ldquo;He who has a why to live for can bear with almost any how.\u0026rdquo;\nFriedrich Nietzsche According to Japanese culture, everybody has a purpose in life: the ikigai. \u0026ldquo;Ikigai is the reason we get up in the morning.\u0026rdquo; The book explores this concept through interesting stories and brings related notions from various areas to explain ikigai to the Western mind.\nThere are many different ikigai, and people can have several ikigai simultaneously. For example, some people have their vegetable garden as their ikigai, while others have drawing or calligraphy. Jiro has making sushi as his ikigai. Even though many people have their work as their ikigai, it does not necessarily have to be the case. Ikigai can also be of a social nature, such as a family or local community.\nRetirement When I watched Jiro Dreams of Sushi for the first time, I was surprised that Jiro still worked at his restaurant every day at age 85. This book explores that further.\nApparently, many people never really retire in Japan. The concept of retirement, as we know it in the West, does not exist in the Japanese language: there is \u0026ldquo;no word in Japanese that means retire in the sense of \u0026ldquo;leaving the workforce for good\u0026rdquo; as in English.\u0026rdquo; People continue doing what they like doing as long as their health allows. They keep following their ikigai.\nThis concept was very refreshing to me and had a profound influence on how I imagine my own future. After becoming more intentional about my career, I became interested in FIRE and the possibility of retiring early. However, as I contemplated this strategy, I discovered that I didn\u0026rsquo;t want to stop working because I really enjoy my work.\nWhy would I want to retire if I\u0026rsquo;m doing what I love? That was the whole point of switching my career to IT, and this is where Western culture can learn a lot from Japanese culture. In Japanese culture, there is more emphasis on aligning your career with your interests and continuing that passion well into old age.\nWhen to quit? The job you have worked so hard for? I never once hated my job. I fell in love with my work and gave my life to it. Even though I\u0026rsquo;m 85 years old, I don\u0026rsquo;t feel like retiring.\nJiro Longevity A large part of the book is about the residents of Okinawa: the island with the highest number of people 100 years old or older. The authors tell the story of their visit to the island to reveal the secret of the Okinawan\u0026rsquo;s long lives. It is not uncommon to see people working in the fields who are well into their eighties. The Okinawans freely share their secrets with us, and the authors do a great job translating the Japanese principles into actionable advice.\nMuch of the advice is centered around dietary habits. Not only what to eat, but also how to eat it: \u0026ldquo;Okinawans stop eating when they feel their stomachs reach 80 percent of their capacity, rather than overeating\u0026rdquo;, a practice which is named \u0026ldquo;hara haci bu.\u0026rdquo; They consume a daily average of 1800 to 1900 calories, a significant difference from the 2200 to 3300 calories consumed by an average person in the US every day.\nAnother theme that keeps returning is the crucial function of social groups for finding meaning in life, especially in old age: \u0026ldquo;It is customary in Okinawa to form close bonds within local communities. A moai is an informal group of people with common interests who look out for one another. For many, serving the community becomes part of their ikigai.\u0026rdquo;\nThis book may be a good resource if you are interested in longevity. Even though I only expected to read about the ikigai concept, it contains a surprisingly large amount of valuable advice and practical tips to live a longer and happier life.\nChock-full \u0026ldquo;Ikigai: The Japanese Secret to a Long and Happy Life\u0026rdquo; is a small book that covers an astonishing amount of subjects. It covers topics such as meditation, flow states, and exercise methods such as Tai Chi and Qi Gong. The disadvantage is that it does not deeply cover any of these topics. But I think this book can be a very useful springboard for further research and a great introduction to many different subjects that can improve your life.\nIt is a short read, but it contains a lot of wisdom, and I learned much from this book. It has changed the way I think of retirement and the way I approach my work. I am also very interested in health and longevity. Through the conversations with the centenarians of Okinawa, I received a lot of helpful tips, which I\u0026rsquo;m already applying to my daily life.\nHave I discovered the art of being busy? I believe I took this definition too literally when I started reading this book. I found that Okinawans lead active lives full of meaning but also take plenty of rest. Always being busy does not mean that you have to work yourself to death. However, it does mean that the secret to reaching a healthy old age is to keep doing what you love. There is no need to become sedentary and only sit in front of the TV when you turn 67. I think Western culture has a lot to learn in this area.\n","permalink":"https://mischavandenburg.com/zet/articles/ikigai-book/","summary":"I came across this little book while doing some research for my Jiro Dreams of Sushi article. While scanning the book\u0026rsquo;s description, I was intrigued by this sentence: \u0026ldquo;the happiness of always being busy.\u0026rdquo; Although I work hard every day, I also enjoy ticking off the last item on my to-do list and enjoying some rest. Is there more happiness in being in constant activity?\nIkigai \u0026ldquo;He who has a why to live for can bear with almost any how.","title":"Book Notes: Ikigai: The Japanese Secret to a Long and Happy Life"},{"content":" This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet. Phoenix will enable it to generate targeted marketing campaigns from the data when it\u0026rsquo;s finished.\nBut as we saw in the previous book, it is far from finished, and things go wrong all the time. The company is not doing well, the stock prices are falling, and they need an edge over the competition. Phoenix will be their edge, but they\u0026rsquo;ve been working on it for years. Eventually, management decided that Phoenix needed to be deployed in two weeks. But it is far from ready.\nThe main character in the Unicorn project is Maxine, a senior developer who temporarily transferred to a different department. She had to work on the Phoenix project against her will because of an unfortunate situation that needed a scapegoat. However, Maxine decides to make the best out of it, and she\u0026rsquo;d like to begin with one thing: to get a Phoenix build going on her laptop.\nVery quickly, she finds it impossible to run a full build of the Phoenix project due to missing files and other elements. She is appalled and makes it her mission to get the build going, but she meets another hurdle every step of the way. Missing credentials. Missing binaries and libraries. And for each of these hurdles, she must submit a ticket with a different department. Very soon, she has over 20 tickets running with long waiting times. Just to get a build going on her machine so she can work! Dozens of developers were hired to work on the Phoenix project. But when she asks them if they\u0026rsquo;ve managed to get a build going yet, Maxine is horrified to discover that they\u0026rsquo;ve tried for several months but haven\u0026rsquo;t made any progress. Maxine has made more progress in a week.\n\u0026ldquo;Everyone around here thinks features are important because they can see them in their app, on the web page, or in the API. But no one seems to realize how important the build process is. Developers cannot be productive without a great build, integration, and test process.\u0026rdquo;\nUnicorn Project After a few weeks, Maxine receives an invitation to have a drink with a group of people who are very interested in her. When she arrives at the bar, she meets the Rebellion: a group of developers, managers, and people from Operations, who are tired of the old organizational structure and want to make real changes. They think out of the box and experiment with new technologies, even though they are not authorized to do so.\nWith the Rebellion, Maxine significantly improves the build and deployment process. They recognized that Phoenix actually never was being built in its entirety. Developers were always working on parts of the application. However, after a lot of struggle, they create a build process that enables each developer to become operational on his first day.\nThis is the first step of a long series of exciting events that lead to Phoenix becoming a success. By the end of the book, they have a completely new development and testing process, and they can deploy changes to production without needing to take the entire application down. This allows them to create targeted marketing campaigns and respond to changes in the market. The first campaign was a huge success and generated the highest sales ever.\nMaxine\u0026rsquo;s struggle with the build process was an eye-opening experience for me. It gave me a very practical example of the need for DevOps principles to enable delivering value to customers. It is also something I recognize in my current organization. For example, projects can get stuck on a firewall change that needs to be approved by an external party. By implementing DevOps principles and arranging teams according to the \u0026ldquo;you build it, you run it\u0026rdquo; principle, teams can be responsible for the entire process from idea to production and therefore have a very short release cycle for their application.\nI thoroughly enjoyed the first part of the book. However, the second part was less engaging to me. It became long-winded and felt like butter spread over too much bread. The author demonstrates a high level of technical experience and knowledge through his descriptions of processes, deployments, and fictional applications. Although I understand the intention of making Parts Unlimited a believable company, I think it could have been accomplished with much less detail and words.\nThe second part has more corporate drama, such as temporarily suspended managers without any clear reason. The focus shifts from a development and operations perspective to a managerial perspective. Maybe I will reread the book in a few years and this part will make a lot more sense to me then. The same happened when I reread the Phoenix project. I could not understand some aspects of the book, which became much clearer to me when I revisited it after gaining experience in the field.\nI highly recommend this book to anyone working as a developer, DevOps Engineer, or in operations, especially if you are starting your career. The book gave me a lot of insights into \u0026ldquo;the old way of working\u0026rdquo; and a better understanding of the need for DevOps principles in the modern IT landscape. However, make sure to read the Phoenix project first.\nThe Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of Data by Gene Kim ","permalink":"https://mischavandenburg.com/zet/articles/unicorn-project/","summary":"This book is the sequel to the Phoenix project. Both books are set at Parts Unlimited, a fictitious company that supplies car parts to DIY mechanics and repair shops. Phoenix is a new system that Parts Unlimited has worked on for multiple years. It is supposed to handle order processing and communication between manufacturing, stores, and clients. Phoenix will also play a role in sales and marketing. The company has been gathering customer data for years, but it cannot use any of this data yet.","title":"Book Notes: The Unicorn Project"},{"content":"Last week I watched \u0026ldquo;Jiro Dreams of Sushi\u0026rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn\u0026rsquo;t grasp its underlying life lessons the first time around.\nEven though I couldn\u0026rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary. Over the years, I managed to grasp more of its deeper meaning. The documentary is about Jiro: one of the best sushi chefs in the world. His tiny restaurant, located in a subway station, even received a 3 Michelin star rating. So how does a man operating from a subway restaurant become the best in the world? And what can a DevOps Engineer learn from a sushi chef?\nJiro is referred to as a shokunin, a Japanese cultural phenomenon that doesn\u0026rsquo;t have a direct translation into English. A shokunin is an artisan or a craftsman who has devoted his entire life to becoming a master of his craft. For example, there are shokunin carpenters, weavers, and blacksmiths. The shokunin achieve mastery by doing the same action over and over again and trying to improve with every repetition.\n\u0026ldquo;Once you decide on your occupation, you must immerse yourself in your work. You have to fall in love with your work. Never complain about your job. You must dedicate your life to mastering your skill. That\u0026rsquo;s the secret of success and is the key to being regarded honorably.\u0026rdquo;\nJiro We get to know Jiro as a man who is devoted to his occupation. He follows the same routine every day; he even gets on the train from the same position. He says he dislikes holidays and prefers to be at work. He used to get up at 5:00 and get home after 22:00. Even at age 85, he\u0026rsquo;s still working at the restaurant every day, simply because he doesn\u0026rsquo;t want to do anything else.\nBut it\u0026rsquo;s not just about putting in long hours and working past the retirement age. It\u0026rsquo;s also about the mindset and the attitude that Jiro has adopted towards his craft:\n\u0026ldquo;I\u0026rsquo;ve seen many chefs who are self-critical, but I\u0026rsquo;ve never seen another chef who is so hard on himself. He sets the standard for self-discipline. He\u0026rsquo;s always looking ahead. He\u0026rsquo;s never satisfied with his work. He\u0026rsquo;s always trying to find ways to make the sushi better or to improve his skills. Even now, that\u0026rsquo;s what he thinks about every day.\u0026rdquo;\nMasuhiro Yamamoto, Food Critic. Finding my Craft It was hard to decide on my occupation. It took me 32 years and a lot of trial and error to find the answer. I have always envied people who knew they wanted to become a nurse since childhood.\nWhat occupation should I choose? I answered this question by answering another question: what do I like to do in my free time? The answer was clear: I tinkered with computers. I had already coded several websites and loved experimenting with game automation. I ran \u0026ldquo;bot farms\u0026rdquo; on Linux servers which I configured myself from the command line.\nI decided to make IT and tech my occupation. Fortunately for me, the job market was in my favor, and employers were much more willing to consider candidates without a formal background in IT. If you\u0026rsquo;d like to read more, I wrote about my journey into DevOps in this article.\nThe Ways of the Shokunin After making the career change, I adopted the shokunin mindset and dedicated my life to mastering my craft. In practical terms, I needed to commit to devoting my time to my profession and avoid straying from it. Making this commitment wasn\u0026rsquo;t very difficult because I chose my career based on my free-time activities. It\u0026rsquo;s more about adopting a mindset of striving to make everything relate to each other. For example, I try to select hobby projects that directly relate to what I\u0026rsquo;m working with during the day. This way, my leisure activities will strengthen my professional skills, and my professional expertise will improve my hobby projects, creating a feedback loop that will eventually lead to results.\nHowever, sticking to one thing does not necessarily come naturally to me. I have a tendency to pick up many different hobbies and get very excited about them. First, I will become extremely interested in a particular subject. Then, after approximately three months, I put it aside and become excited about something else. This isn\u0026rsquo;t necessarily a case of Shiny Object Syndrome because I tend to return to these hobbies in a cyclical pattern. As I apply myself to learning the skill with a feverish intensity, I\u0026rsquo;m able to make a lot of progress during these bouts of obsession. But to the outside world, it might seem like I\u0026rsquo;m constantly changing my mind about what I want.\nI always considered this a negative character trait, but over the past few years, I\u0026rsquo;ve learned to embrace it and guide this tendency in the right direction. Because I chose IT as my occupation and DevOps as an area of specialization, I gave myself a broad scope of interest with many sub-skills to learn. If you look at the DevOps Roadmap, you\u0026rsquo;ll see that a good DevOps Engineer must master many different skills.\nIt\u0026rsquo;s like I\u0026rsquo;ve given myself a large playground with a fence around it, full of exciting things to learn. I can go down a Python rabbit hole for a few months and improve my coding skills. Later, I find myself sucked into builing my own OS and learning more about Linux in the process. The fence around the playground isn\u0026rsquo;t locked, but I do my best to stay inside the fence. When, for example, I start getting the urge to get back into music production again, I consider that it will take up a few hours a day to get back on track. Investing these hours into something that brings me joy, but is also related to my occupation, would be a better option. Reflecting in this way keeps me focused on my goals.\nMy website is another example of this fusion between professional and free-time activities. I love to write. It\u0026rsquo;s a relaxing activity, even though it is very challenging sometimes. After I learn about a new subject, I try to write about it. This is the best way I know to verify whether I\u0026rsquo;ve really understood the topic. When I sit down to write about something, I force myself to think clearly about it and to make sure everything aligns properly in my mind. When I cannot explain the concepts in a few clear sentences, I know I don\u0026rsquo;t fully understand it yet.\n\u0026ldquo;Clear thinking becomes clear writing; one can\u0026rsquo;t exist without the other.\u0026rdquo;\nWilliam Zinsser, On Writing Well Another way my blog supports my professional development is because I prefer to write my blog posts in neovim in the markdown format. Writing and editing text often involves moving a lot of words and paragraphs around. Because I use vim to do my writing, I\u0026rsquo;m also constantly practicing the keybinds I use for writing and editing code during my day job. I often learn a new motion for my blog writing, which I use the next day at work.\nNever Finished \u0026ldquo;All I want to do is make better sushi. I do the same thing over and over, improving bit by bit. There is always a yearning to achieve more. I\u0026rsquo;ll continue to climb, trying to reach the top, but no one knows where the top is. Even at my age, after decades of work, I don\u0026rsquo;t think I have achieved perfection. But I feel ecstatic all day. I love making sushi. That\u0026rsquo;s the spirit of the shokunin.\u0026rdquo;\nJiro Jiro taught me never to be satisfied with my skills. If I desire to become a master of my craft, there will never be a point where I can lean back and think that I\u0026rsquo;ve learned enough. There\u0026rsquo;s always something to improve, which is why I chose an IT career. My thirst for learning cannot be quenched, and there\u0026rsquo;s always something to learn in this field.\nDevotion Although it might seem like a simple food documentary, do not be deceived. Jiro Dreams of Sushi is full of valuable life lessons. It\u0026rsquo;s one of the documentaries which I revisit regularly. It\u0026rsquo;s relaxing to watch, and I get hugely inspired by Jiro\u0026rsquo;s devotion to his craft and powerful teachings every time.\nJiro\u0026rsquo;s guidance has changed my life. He made me realize that if I wished to become a master of my craft, I needed to devote my life to it. I changed my lifestyle, so the things I do in my free time strengthen my professional skills. I also became more mindful of the content I consume and the things I read, trying to keep it related to my occupation. He showed me the true meaning of the phrase \u0026ldquo;my work is my hobby.\u0026rdquo;\n\u0026ldquo;Always look beyond and above yourself. Always try to improve on yourself. Always strive to elevate your craft. That\u0026rsquo;s what he taught me.\u0026rdquo;\nYoshikazu, Jiro\u0026rsquo;s oldest son. ","permalink":"https://mischavandenburg.com/zet/articles/jiro-sushi/","summary":"Last week I watched \u0026ldquo;Jiro Dreams of Sushi\u0026rdquo; again. It must be the eighth time I revisited this piece of art. I have very little interest in sushi, but there is much more to this documentary. It was first recommended to me by my good friend Anders more than five years ago, but I couldn\u0026rsquo;t grasp its underlying life lessons the first time around.\nEven though I couldn\u0026rsquo;t fully understand the message, I was intrigued by it and kept returning to the documentary.","title":"I'm In Love with my Work: Lessons from a Japanese Sushi Master"},{"content":"If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.\nThe Beginning The reason why I started to use vim is rather practical. When I was studying to become a Cloud Engineer, I had access to subscriptions on AWS and Azure to experiment with virtual machines. This was a perfect place to learn to work with Ansible. Many of the labs projects I did involved setting up a few virtual machines, and I destroyed many VMs when I made some big mistakes in the configuration.\nI was using Visual Studio Code at the time on my local machine, but I had to connect to a new virtual machine multiple times a day. It became very tiresome to set everything up with VSCode every time, or pulling the files to my local machine and copying them over again. So I just ssh\u0026rsquo;ed into the machines to edit the text files with the included editor, which happened to be vim.\nObsession in its Infancy When you first use vim, it is a rather disorienting experience. But in every tutorial, I was told it would be difficult in the beginning but much faster and more effective in the end. I found this very appealing because I like to do things the hard way and challenge myself.\nI discovered that there were people out there who did all of their text editing and coding in vim. I met programmers who refuse to use anything else and people who write entire books in vim. So there had to be something to it.\nIt also fitted very well with my intention of working on the command line as much as possible and moving away from GUI applications whenever possible. I like to move in this direction because I love the idea of controlling your entire workflow with your keyboard instead of using your mouse, and vim fits perfectly into this picture.\nWhat I like after Nine Months At this point, I\u0026rsquo;ve been using vim as my primary text editor for about nine months. In my current job, I work a lot with yaml files stored in private git repositories.\nI only work with these files from the command line, and I don\u0026rsquo;t have any other code editor installed. I use ripgrep and fzf (fuzzy file finder) to search through the files, and I use neovim to edit them. When I need to search for files from within vim, I use the awesome Telescope plugin.\nIn these months, I\u0026rsquo;ve picked up a few tricks, and I am starting to see the power of vim. The best thing I like about it is that I don\u0026rsquo;t have to leave my terminal window to do the tasks I need to do. Instead, I can search through the files I need to work with, open them, make adjustments, and commit them to the repository. Then I enter the command to run the ansible playbook, and it all happens in the same window, and I don\u0026rsquo;t have to lift my fingers from the keyboard.\nKeyboard Shortcuts Now that I am gaining more experience with vim, I\u0026rsquo;m picking up more advanced usages that significantly improve my workflow. For example, \u0026ldquo;da(\u0026rdquo; meaning \u0026ldquo;delete around parentheses\u0026rdquo; to quickly delete the text between two parentheses. Or \u0026ldquo;da\u0026lt;\u0026rdquo; to very quickly delete HTML tags. Another great feature is the visual block mode, where I can add comment tags to many lines simultaneously, for example.\nSearching and Navigation Navigating large text files has become incredibly quick since I started using vim. Of course, it takes some getting used to, but it is a lovely experience to open a file, press / to search and enter the keyword and immediately arrive at the point I need to be—no scrolling with the mouse and no need to lift my hands from the keyboard.\nI also love the ability to jump from sentence to sentence using ) or paragraphs using }.\nMultiple Files It takes a little while to get used to, but when you get into it, it is effortless to open up two files at a time if you need information from both. Often I need data from 4 or more files, and opening them quickly with keyboard commands has significantly improved my workflow speed.\nCustomization One of the things I enjoy most about vim is the ability to customize it exactly to my needs. I\u0026rsquo;m completely in charge of the plugins which are loaded into vim and which colors it uses, and this appeals a lot to me. However, it can be rather overwhelming in the beginning. To be honest, it is still overwhelming after ten months. It can be tough to get an idea of where to start, which plugins you need, and which settings you need to change.\nI just started with the base install of vim and started from there. Every time I required a particular functionality, I searched around to see if a plugin was available. Very often, someone out there had the same problem as you and created a plugin for it. For example, I recently installed a plugin for using emojis in vim 😄\nHow to Get Started The short answer is to simply start using vim for all of your text editing, whether it be coding or writing for pleasure. It is a cliche to say, but it will be hard in the beginning, but I promise you it will pay off in the end.\nThe second thing I\u0026rsquo;d recommend is to run vimtutor on a Linux machine. Do this once a day for a couple of weeks, and you\u0026rsquo;ll know how to edit text files on any Linux system for the rest of your life, which is a precious skill.\nFinally, don\u0026rsquo;t spend too much time reading about all the available plugins. Your needs will become apparent to you as you start to use vim for all of your tasks, and you can search for plugins to address those needs. This way, you start with a minimal editor, which you\u0026rsquo;ll build according to your needs.\nGood Luck! ","permalink":"https://mischavandenburg.com/zet/articles/how-started-vim/","summary":"If you are just starting your Linux journey, you might have noticed that a few camps exist in the Linux world. Just like in any other area of life, it seems that groups of human beings enjoy dividing themselves instead of living in harmony. There are camps centered around Linux distributions (I use Arch, btw) but also around text editors.\nThe Beginning The reason why I started to use vim is rather practical.","title":"How and Why I Started Using Vim"},{"content":" When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand \u0026ldquo;the old way\u0026rdquo; of doing things and the merits of implementing DevOps principles.\nI reread the book ten months later. In the meantime, I\u0026rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.\nNot surprisingly, it made a lot more sense to me this time, and I\u0026rsquo;m sure it will be even better when I reread it a few years later. In this article, I\u0026rsquo;ll share my thoughts and notes on the book.\nThe Story The main character is Bill Palmer, a mid-level IT manager in a manufacturing company called Parts Unlimited. Within a few pages, he is called into the CEO\u0026rsquo;s office, and he is promoted to the VP of Operations, putting him in charge of IT, much against his own will or desire.\nThe situation Bill enters is a humorously chaotic one. We are thrown straight into a Sev 1 incident where managers point fingers and shout at each other. We quickly get the impression that this is a dysfunctional department that only performs tasks for the manager who shouts the loudest while fighting off crippling outages.\nIt’s like the Wild West out here. We’re mostly shooting from the hip.”\nThe Phoenix Project\nThe bulk of the story revolves around how Bill, together with his team of managers, Wes, Patty, and John, manage to turn this chaos into a department that does work according to a streamlined plan in a much more predictable manner.\nTo achieve this goal, Bill is introduced to Erik, a prospective board member of the company. Erik becomes Bill\u0026rsquo;s mentor and guides Bill through the process of creating order in the chaos. Their interaction reminds me of Zen masters training their disciples by asking deep questions which don\u0026rsquo;t have an immediately apparent answer.\nMaster \u0026amp; Disciple Erik takes Bill to the manufacturing plant of Parts Unlimited and tries to impress upon Bill that manufacturing planning principles from Lean can be applied to IT work. Erik argues that an IT department could be structured like a factory production line, but Bill is not ready to accept this.\nA fundamental notion from manufacturing principles is that work should always be moving forwards along the production line, never backward. But unfortunately, this is very often the case in the \u0026ldquo;old\u0026rdquo; way of working: the development team works on an application for several months, and when they are finished with it, they throw it over the fence to the Operations people, whose job it is to deploy the application.\nOne of the developers had actually walked in a couple of minutes ago and said, “Look, it’s running on my laptop. How hard can it be?”\nThe Phoenix Project\nHowever, as we see happening time after time in the book, usually the application is incompatible with the infrastructure it is deployed to. As a result, the application needs to go back to development. According to manufacturing theory, this is a situation where work goes backward through the production line, which we must avoid at all costs.\nImplementation Erik challenges Bill to start doing ten deployments a day instead of one deployment every nine months. Understandably, this is a ridiculous notion to Bill. The last few deployments were disastrous events that required his entire department to pull all-nighters through the weekend, and still, the stores were not managing to process all orders and payments.\nHowever, Bill takes his mentor\u0026rsquo;s advice and figures out a way to do it together with his team. One of the main problems they uncovered was the inconsistent deployment and production environments.\nThe solution to this problem was to involve the operations people in the development stage right from the beginning, so the development environment matched the production environment exactly. The environments were standardized and put in code with version control, and things started progressing quickly.\nAs Wes talks, I think about Erik challenging me to think like a plant manager as opposed to a work center supervisor. I suddenly realize that he probably meant that I needed to span the departmental boundaries of Development and IT Operations. The Phoenix Project\nThis is just one of the problems addressed by melting away the fence between Development and Operations. By the end of the book, the two camps started to work together much better. They come closer to the target of 10 deployments a day, and the DevOps way of working was born.\nClosing Thoughts I think this book is a must-read for anyone considering entering the DevOps field or anyone already working with DevOps.\nAs a nerd who loves structure and organization, the theme of the story is incredibly entertaining and satisfying to me. The authors excellently capture the transition from an utterly disorganized situation to a predictable environment with happy co-workers. Actually, I\u0026rsquo;m a little embarrassed by how much joy this transition brings me.\nEspecially the second time around, it helped me better understand the underlying principles that enable the DevOps way of working in an organization. Moreover, it paints a great picture of how an organization can change for the better by embracing DevOps principles and how these changes express themselves in the improved quality and speed of software development and deployment. All of these advantages lead to delivering better value to the customer, which is the core focus of any productive and creative endeavor involving customers and end users.\nThe Phoenix Project, written by Gene Kim, Kevin Behr, George Spafford ","permalink":"https://mischavandenburg.com/zet/articles/phoenix-project/","summary":"When I started my DevOps traineeship, I borrowed this book from my boss and read it from cover to cover. I loved the story and the characters; it helped me understand \u0026ldquo;the old way\u0026rdquo; of doing things and the merits of implementing DevOps principles.\nI reread the book ten months later. In the meantime, I\u0026rsquo;ve learned many new skills and technologies and started working full-time as a DevOps engineer.","title":"Book Notes: The Phoenix Project"},{"content":"I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily. I passed on my first try, and I did one session of killer.sh.\nMy preparation KodeKloud CKA Course Killer.sh Mock Exam Killercoda I kept track of the time I spent on this certification. In total, I spent 80 hours on study and practice.\nIn Hindsight I spent too much time repeating things during the KodeKloud course. This is the one thing I would do differently if I could start over. I went over some modules multiple times and kept meticulous notes. However, I have hardly used any of those notes. But they will be nice to have for the future.\nI learned most from the killer.sh exams. So I would advise you to go through the KodeKloud course and do all the exercises, but don\u0026rsquo;t spend too much time repeating stuff. If you don\u0026rsquo;t understand the topic at all, it is, of course, necessary to repeat it. But you don\u0026rsquo;t need to know all the details.\nKiller.sh After I finished the KodeKloud course, I purchased the exam voucher and started the killer.sh on Saturday morning. I wanted to simulate the exam experience as much as possible, so I set the timer and did not allow myself to stand up for two hours. My first round was humiliating. I only managed to get 24 out of 125 points. A little shocked by the experience, I spent the whole Saturday going through all the solutions of the exercises that killer.sh provides. The explanations they give are extensive, and I found them helpful. Saturday evening, I went out for dinner with friends, and on Sunday morning, I passed killer.sh. I spent the whole Sunday studying the solutions more and more, and on my last try on Sunday evening, I scored 115 out of 125.\nTips: I know tmux quite well and used it extensively during the killer.sh, but it was not necessary during the exam. No need to learn it if you don\u0026rsquo;t know it already.\nKnowing vim well will save you a lot of time at the exam. For example, dG to delete all lines until the end of the file from your current location. Run \u0026ldquo;vimtutor\u0026rdquo; on a Linux system to learn the basics.\nYou cannot use bookmarks. Learn how to search the docs efficiently. One handy one I figured out was to control + F and enter \u0026ldquo;kind: Pod\u0026rdquo; or \u0026ldquo;kind: PersistentVolume\u0026rdquo; to immediately go to the example YAML.\nmy exam environment did not need much extra configuration. All I added to my .bashrc was alias v=vim and export do=\u0026quot;\u0026ndash;dry-run=client -o yaml\u0026quot; so you can use \u0026ldquo;k run Nginx $do \u0026gt; Nginx.YAML\u0026rdquo;\nThe exam environment is not as bad as people make it out to be on the internet. There is a little delay while scrolling through the docs in the browser, but working in the terminal didn\u0026rsquo;t give me any problems. Get used to the environment on killer.sh, and there should not be any surprises in the real exam environment.\nSkip questions you cannot solve immediately. But don\u0026rsquo;t spend time reviewing all the questions, sorting by the highest % and doing those first. You will lose a lot of time evaluating all of these questions. It is much better to solve the questions during your first pass through and skip the ones you cannot immediately solve.\nWhen the 120-minute timer ran out, I was presented with a screen that said \u0026ldquo;quit\u0026rdquo; or \u0026ldquo;request more time.\u0026rdquo; I was pretty sure I could not get more time for this exam, so I just pressed \u0026ldquo;quit.\u0026rdquo; After I pressed quit, the application closed immediately, and there was no confirmation whatsoever that they received my exam results or anything. This was extremely disorienting, and I was left doubting if I had done it correctly. Eventually, I could see in the Linux Foundation portal that my exam was in Grading status.\nSpeed is of the essence. An hour before my exam, I used killercoda to get into the mood and get things up to speed. Learn to solve things quickly and don\u0026rsquo;t spend time having to arrange terminal windows on your screen or stumbling around in vim. You cannot afford to lose time on these things.\nFinally, this video is an excellent summary of all the necessary tips and information: ","permalink":"https://mischavandenburg.com/zet/articles/cka-tips/","summary":"I recently obtained my CKA certification. I started this certification journey with zero knowledge of Kubernetes. However, I was already working as a DevOps Engineer, and I know a fair bit of Linux. I daily drive Arch Linux and have LPIC-1 certification. It was handy to know where files are located on Linux systems and how to interact with systemd services. I also knew yaml quite well because I work with Ansible daily.","title":"Certified Kubernetes Administrator (CKA) Exam Tips"},{"content":"NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.\nintroduction I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since. There has not been a single use case where I needed to use Windows for anything at all.\nAs I was getting more used to daily driving Linux, I noticed how much I enjoyed the ability to customize my operating system and workflow. Finally, after spending a weekend going down the customization rabbit hole, I had a good-looking terminal and customized neovim to perform as I needed it.\nNot much later, I came across Arch Linux and the idea of building your own operating system from the ground up. I was instantly intrigued and knew I wanted to do the same. A few months have passed since I first came across Arch Linux, and now I am writing this blog post in neovim on my custom OS that I created from scratch. The font, the spacing, the colors, everything is exactly how I like it, and I love using it.\nWhen you first install Arch Linux, all you get is a black screen with a blinking cursor. However, the experience of creating a fully functional graphical environment from “nothing” has been extremely satisfying, and I learned so much about GNU/Linux in the process. I also realized that this could be seen as a creative activity, like a painter creating his masterpiece from a blank canvas or a sculptor carefully chipping away at a block of marble.\nnot just graphics When I say customization, I am not just referring to the visual aspects of the operating system. The things going on “under the hood” must also be carefully configured when you use a minimal distro such as Arch Linux.\nArch Linux comes with very few packages preinstalled, and every time you wish to add something to your system, you need to install it and enable the service in systemd. For example, after I did the installation and created my user account, I needed to run a command with root privileges. To my great surprise, even the “sudo” command was unavailable and needed to be installed.\nThis is the aspect I learned most from. Whenever I desired a certain functionality from my operating system, I needed to install and enable it. This has given me a much better understanding of the processes and daemons running on my system. It has also given me a greater appreciation of all the elements needed to provide a working environment.\ngraphic violence When you create an Arch Linux installation USB and boot it up, you are greeted with a command line and nothing else.\nWhen you install something more beginner-friendly, such as Ubuntu or Manjaro KDE, your installation will include a graphical desktop environment. But on Arch Linux, you must install and configure this yourself. Furthermore, to be able to render a graphical environment, you will also need to install and configure a display server such as Xorg.\nWhen I started on my journey, I intended to create something that used minimal resources with a minimal look. Having used GNOME on Manjaro for a few months, I was very satisfied, but I wanted to try a tiling window manager to shave down even more resource usage. After some research, I ended up with the Awesome Window Manager. Here are some screenshots of the final result:\nThis is what my desktop looks like when I boot up.\nHere I’m editing my window manager configuration file, while I have a browser open and keep an eye on my system resources\nmy music listening setup, using mpd + ncmpcpp, cava and sptlrx. the lyrics are shown in real time as the music is played.\ncreativity The Cambridge Dictionary defines creativity as “the ability to produce original and unusual ideas, or to make something new or imaginative.”\nWhen you embark on a journey, such as creating your operating system, you will probably start with a particular intention or a goal that you will work towards. With this goal in mind, you can start searching for the tools and color schemes you need to create the system that you have in mind. The result is a unique combination of tools, colors, fonts, and programs specifically tailored to your needs and wants and chosen by you.\nIs this any different from a painter starting with a blank canvas or a musician starting with a fragment of a melody, ending up with a complete symphony? Entering commands into a computer terminal might not strike everybody as a creative activity. Still, I have found that it is a very effective and satisfying way of expressing myself and creating something I love to use daily. As an IT professional, I spend most of my time behind my computer. Doesn’t it make sense to put effort into building something customized to your needs?\nresources If you want to start building your own OS, I recommend these resources:\nArch Linux\nArch Wiki\nr/unixporn – a subreddit about customization\n","permalink":"https://mischavandenburg.com/zet/articles/linux-creativity/","summary":"NOTE: In this article, I use a rather broad definition of “Operating System.” I do not intend to appear as if I wrote and compiled my own Linux kernel, nor do I understand the inner workings of the kernel written in C. Instead, with “building my OS from scratch,” I intend to convey that I used a minimal Linux distribution as a starting point and started building from there.\nintroduction I started using GNU/Linux as a daily driver about six months ago, and I have not regretted the decision ever since.","title":"Building my Own OS: Linux as a Creative Activity"},{"content":"Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.\nThis means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.\nSSH into the server and stop the service. Then get the latest upgrades for your server, which is good practice:\nservice jenkins stop apt-get update apt-get upgrade Depending on your setup, the apt-get upgrade command might upgrade Jenkins to the latest version that does not require Java 11+. In my case, that was 3.346.\nWhen you get a question about updating your current config file, take the default option. This option keeps your current configuration.\nHowever, if your Jenkins is installed from a binary or another source, you might need to upgrade Jenkins to 3.346 using the Jenkins.war file:\ncd /usr/share/jenkins mv jenkins.war jenkins.war.old wget https://updates.jenkins-ci.org/latest/jenkins.war service jenkins start When you start Jenkins, it will be updated to the latest version that does not require Java 11 or higher. You will notice that there will be a new folder called migrate in /usr/share/jenkins , and the jenkins.war is now located in /usr/share/java\nThis is where I got confused because it did not patch to the latest version, only up to 3.346 and the jenkins.war file was no longer being updated from the /usr/share/jenkins folder.\nThe reason is that this update moves the .war file to the /usr/share/java directory.\njava To get Jenkins to the latest version, we need to install or update Java and check if it has worked:\napt-get install default-jre java -version Now that you have updated the java version, you are ready to update Jenkins to the latest version.\nNotice that we use the /usr/share/java folder now, instead of /usr/share/jenkins\nservice jenkins stop cd /usr/share/java mv jenkins.war jenkins.war.old wget https://updates.jenkins-ci.org/latest/jenkins.war service jenkins start nodes When I accessed the Jenkins GUI, everything seemed fine, and my version was up to 3.358.\nHowever, I noticed that the build nodes were all offline. When inspecting the logs, I saw the following error:\njava.io.EOFException at java.base/java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2905) at java.base/java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3400) at java.base/java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:936) at java.base/java.io.ObjectInputStream.\u0026lt;init\u0026gt;(ObjectInputStream.java:379) at hudson.remoting.ObjectInputStreamEx.\u0026lt;init\u0026gt;(ObjectInputStreamEx.java:49) at hudson.remoting.Command.readFrom(Command.java:142) at hudson.remoting.Command.readFrom(Command.java:128) at hudson.remoting.AbstractSynchronousByteArrayCommandTransport.read(AbstractSynchronousByteArrayCommandTransport.java:35) at hudson.remoting.SynchronousCommandTransport$ReaderThread.run(SynchronousCommandTransport.java:61) Caused: java.io.IOException: Unexpected termination of the channel at hudson.remoting.SynchronousCommandTransport$ReaderThread.run(SynchronousCommandTransport.java:75) Observing that the error had something to do with Java, I ssh’d into the build nodes and updated Java there as well with the same command:\napt-get install default-jre After updating Java on the build node, head back to the GUI on the master node and restart the build node.\nIt should now be online again.\n","permalink":"https://mischavandenburg.com/how-to-upgrade-java-and-jenkins-on-ubuntu-18-04/","summary":"Last week I upgraded Jenkins to the latest version on the server infrastructure at work. Starting with the Jenkins 2.357 release, Java 11 or Java 17 will be required to run Jenkins. Also, the upcoming LTS release will require Java 11.\nThis means that I also needed to update Java on our Jenkins servers. Here are the steps that I did to perform the Jenkins and Java upgrade.\nSSH into the server and stop the service.","title":"How to Upgrade Java and Jenkins on Ubuntu 18.04"},{"content":"In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best. These are listed in no particular order.\n#1 job opportunities IT has always been a field with many job opportunities, and with the current movements of digitalization and automation, there is no sign that it will slow down.\nAccording to the U.S. Bureau of Labor Statistics, “Employment in computer and information technology occupations is projected to grow 13 percent from 2020 to 2030, faster than the average for all occupations. These occupations are projected to add about 667,600 new jobs. Demand for these workers will stem from greater emphasis on cloud computing, the collection and storage of big data, and information security.”\nThe situation is no different here in the Netherlands. Currently, there is a shortage of people in IT, and employers are much more willing to provide training to motivated individuals to make a change.\n#2 remote work I think remote work is one of the best parts of living in post-pandemic 2022. I am an introvert, and having a quiet, stable space without distractions, which is the same from day to day, is a huge boost to my productivity.\nSecondly, I think it is crucial to be mindful of your posture and body while working at a desk. For example, I am dependent on having a standing desk which I adjust more than ten times a day. I also need a chair suitable for my body type to avoid getting stiff and getting a sore back. Although some offices take care of providing these facilities to their employees, I think it is beneficial to invest in your own setup, which you can tailor to your own needs.\nThirdly, working from anywhere in the world is a massive advantage. I am not very interested in living a digital nomad lifestyle, working from a MacBook in coffee shops, but I think it’s great that you can spend some time abroad while working from that location.\n#3 personal interest This is a big one. You should not change your career to IT just because it earns well or because you think you can work from the beach in Thailand. I have been tinkering with computers and programming languages since I was a kid and have always enjoyed it. I always found myself “the computer guy” in groups of friends or colleagues.\nHowever, for some reason, I never managed to make my career out of it until now, and I get a lot of satisfaction from my work every day after I made the change.\n#4 high income It is no secret that tech jobs are some of the best paying jobs in the U.S., having a mean wage of $99,860. And if you work your way up into management, there are even higher salaries. Here in the Netherlands, it is also a financially sound choice, with a mean wage of €47.200\n#5 fast-changing field IT is a broad field with many little niches you can get into, and every niche is constantly developing. Being in IT means you will need to stay on board by continuing to learn the new technologies and languages to keep on track.\nThis is also what makes it exciting to me, being a life-long learner. There is always more to learn and some cutting-edge technology to become familiar with, which can improve your workflow and your deployments.\nlinks: Computer and Information Technology Occupations – US Bureau of Labor Statistics\nMay 2021 National Occupational Employment and Wage Estimates\nSalaries in IT - Dutch\n","permalink":"https://mischavandenburg.com/5-reasons-why-i-changed-my-career-to-it-in-my-thirties/","summary":"In this post, I’d like to share the five main reasons why I changed my career to IT in my thirties. Making a career change can be daunting, especially when you are past your twenties, and employers can get more skeptical of hiring and training you. However, when I passed my twenties and became more serious and intentional about my life and career, I decided to take the plunge and hope for the best.","title":"5 Reasons Why I Changed my Career to IT in my Thirties"},{"content":"I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.\nFirst, you need a correctly partitioned hard drive.\nTo wipe your drive clean and have a single partition on it, we’ll use GParted.\nSelect your disk in the upper right corner.\nThen go to Device and select Create Partition Table:\nFollow the wizard and use an ext4 filesystem. NTFS can cause problems because Steam cannot read it properly.\nmounting To use a disk or a partition in Linux, it needs to be mounted.\nList your devices and identify the one you wish to mount by using the “lsblk” command.\nIn my case, I wish to mount the drive sdc1\nOn Linux, all filesystems need to be mounted before they can be used. I wanted my whole disk to be available in the directory /mnt/data\nBefore mounting, I created the directory.\ncd /mnt\nsudo mkdir data\nWhen you make the directory by using sudo, the directory owner will be the root user. This means that you cannot access the directory and write to it from your own user.\nUse this command to change the ownership of the directory. Replace “mischa” with your username.\nsudo chmod mischa:mischa data\nVerify that the directory now has the correct ownership:\nNow you can mount your directory, so it is available for use.\nmount /dev/sdc1 /mnt/data\nmounting on boot For the mount to happen automatically on startup, you’ll need to add it to the /etc/fstab file. We start by finding the UUID of our disk.\nUse the following command:\nls -al /dev/disk/by-uuid/\nIn my case the UUID will be 50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6\n/etc/fstab open the file in your favorite editor. I like to use vim.\nsudo vim /etc/fstab\nAdd a new entry to your /etc/fstab file and use the following parameters:\nUUID=50d608bc-a7ad-4ff6-bf44-bb6f26efa4f6 /mnt/data ext4 defaults 0 0\nBefore we go further, verify that we did this correctly by using the following command:\nfindmnt --verify\nThis will verify the /etc/fstab file. Not meaning to scare you, but an incorrectly configured fstab may lead to an unbootable system.\nNow reboot your system and check if your disk is mounted automatically.\nIt is also a good idea to cd to your mounted directory and touch a file to see if you have write permissions.\nSteam Now it’s time to set things up in Steam. Open Steam and open your settings.\ngo to Downloads –\u0026gt; Steam Library Folders\nClick the + button and navigate to your mounted drive.\nTo test, install a game and reboot your system.\n","permalink":"https://mischavandenburg.com/how-to-move-your-steam-game-files-to-a-separate-hard-drive-on-linux/","summary":"I have installed my OS on a 240GB SSD, and I prefer to keep my data on a different disk to leave enough space to work with. I wanted to move my steam game files to a separate hard drive on Linux. I’ll show you what I did to make this happen in this article. I use Manjaro GNOME.\nFirst, you need a correctly partitioned hard drive.\nTo wipe your drive clean and have a single partition on it, we’ll use GParted.","title":"How To Move Steam Game Files To a Separate Hard Drive on Linux"},{"content":"After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.\nI was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me. So I decided I wanted to switch to Linux for my daily operating system. But a few things were holding me back. Will I be able to play my favorite games? Will I run into a lot of problems with my sound and microphone? Are all the apps I need for working from home available on Linux? Can I even edit videos on Linux?\npreparing to switch I made a dual boot install of Ubuntu to try things out to answer these questions. I started things off by setting small goals for myself. For example, I need to be able to work from home. Which programs do I need? And I started from there.\nI had no problems installing Slack, Zoom, Teams, and all the other programs I needed for my work. I was very surprised by how well all of the external hardware worked. My Bluetooth keyboard and mouse worked immediately without needing to install any drivers. Even my USB webcam worked instantly without any problems.\nTo put this into perspective, I spent an entire afternoon getting my keyboard to work correctly on my MacBook. It needed a lot of extra drivers. Still, there is a 4-second delay when I press my volume up/down keys when working on my MacBook. All of this works perfectly on my Linux OS without any delays and without any drivers or extra configuration.\nI kept my dual boot setup for a while as I was trying out different distros, and I eventually ended up with Manjaro as my distro of choice. I like it because it is based on Arch Linux, and it gives you access to the Arch User Repository (AUR). I want to use Arch eventually, and I figured this might be a good stepping stone.\nfate decided for me This dual boot setup continued for a while as I was warming up to the idea of completely abandoning Windows. I set Linux as my default boot option, and after a few weeks, I realized I hadn’t needed to boot into Windows for anything at all. However, I still didn’t feel quite ready to switch completely to Linux.\nOne evening I wanted to make another fresh install to check out the GNOME version of Manjaro. I was doing a lot of chores at the same time, and it was getting quite late, but I wanted to have the installer running as I was doing other tasks. Probably not my most brilliant move.\nYou probably know what is coming: in between my chores, I started the installation. In a moment of carelessness, I managed to point the installation to my Windows partition, and it was completely wiped and replaced with a sparkling fresh Linux install.\n“Well, I guess I am moving to Linux today!” I thought while I suppressed a hint of panic as I racked my brain to see if I had lost any important files. I knew that most of my important stuff was safely backed up in the cloud. But if I had formatted my Windows drive by choice, rather than by accident, I would have backed up a lot more files.\nfirst week without windows A week ago, I lost my complete Windows install, but there hasn’t been a single moment where I regretted making the switch. Fortunately, it also seems that I did not lose anything important.\nI am learning so much by forcing myself to use Linux as a daily driver. Most things are correctly configured out of the box. But sometimes, you have to do some work to get the configuration you like.\nFor instance, after installing Steam, I wanted to have the game files located on a different hard disk because my OS SSD is only 256GB. This required me to format my data SSD to an ext4 filesystem and mount it in a folder. I also needed to add it to my /etc/fstab file to make sure that it mounts automatically when I boot my PC.\nThese tasks have been great practice for the things I need to do on my servers at work, and they will make me approach these tasks with a little more confidence because I have done them before on my personal setup. This is the great advantage of having Linux as a daily driver if you are becoming a DevOps Engineer or Linux System Administrator.\n","permalink":"https://mischavandenburg.com/goodbye-windows-hello-linux-switching-to-linux-as-my-daily-driver/","summary":"After getting my LPIC-1 certification, my interest in Linux continued to grow. When I started my new job, I performed more sysadmin tasks, such as increasing the size of filesystems or removing backups, and it felt good to put the theory into practice.\nI was still using Windows in my personal setup, and I started running into limitations. Finally, I realized how much I appreciated the freedom and control over my system that Linux gave me.","title":"Goodbye Windows, Hello Linux! Switching to Linux as my Daily Driver"},{"content":"This week I learned about yadm: yet another dotfile manager. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.\nEven if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default. This is indicated by prefixing the file with a period. So the configuration file for the vim editor is .vimrc, and for zshell you use the .zshrc. This is why configuration files are referred to as dotfiles.\ncustomisation The more I work with Linux, the more I appreciate the ability to customize things. When I first started, I was pretty satisfied with the vanilla experience. You punch your commands into the terminal, and you do your tasks. What more could you need?\nThis started to change when I began working with senior engineers who built their custom setups over the years. I saw them opening 3 terminal windows in a perfect ratio with beautiful colorschemes or previewing files in a file browser directly in vim so they could split them vertically and edit them side by side.\nI wanted to create a similar setup by adding settings and plugins to the .vimrc and .zshrc files. However, before going down this rabbit hole, I asked myself the following question. How can I bring this configuration with me to other machines? What happens if my laptop gets stolen and I lose my precious configuration files?\nyet another dotfile manager Enter yadm. I had thought of putting my dotfiles in a GitHub repo, but this brings up a whole set of new challenges where you would need to create symbolic links across your system to have the files in their correct places. Yadm solves this problem.\nYadm turns your home directory ( ~/ ) into a Git repo which can be pushed to a host of your choice. You can add your files one by one, and yadm will track them. The best thing is that you can add the files from all over your system, and yadm will not bother with any of the other files in your home directory.\nyou want git for your dotfiles Setting up your configuration files in a git repository has a lot of advantages:\nconfiguration is saved in multiple places easily share your configuration across machines version control Version control is especially useful. You will always be able to trace back that one plugin you used a few years ago, but you cannot remember the name of. And it is fun to watch your configuration grow over time.\nsetting up yadm Installing yadm is a breeze. For my mac I just used\nbrew install yadm or you can use the apt-get or dnf install equivalents if you are on Linux.\nYou interact with yadm the same way you interact with git. You simply replace the word git with yadm in the commands.\nThen you navigate to your home directory and set up the repository. If you don’t have a repository yet:\nyadm init yadm add \u0026lt;important file\u0026gt; yadm commit yadm remote add origin \u0026lt;url\u0026gt; yadm push -u origin \u0026lt;local branch\u0026gt;:\u0026lt;remote branch\u0026gt; Or if you already have a dotfiles repository:\nyadm clone \u0026lt;url\u0026gt; yadm status And that’s it. Now add your configuration files and push them to your hosted repo:\nyadm add ~/.vimrc yadm add ~/.zshrc yadm commit -m \u0026#34;first commit\u0026#34; yadm push You will notice that yadm expects you to add all the files every time you want to make a new commit. Use this command to stage all the files you added previously:\nyadm add -u enjoy your synched customisation Having your dotfiles in a GitHub repo makes it easy to set up your preferred settings on a new machine or environment. So install yadm and pull your repo, and off you go!\nI hope you will enjoy it as much as I do. Crafting a customized setup takes a lot of time and effort, and now that I finally have an excellent solution to keep track of my files, I am ready to dive into the customization rabbit hole.\nDownload yadm here. Here you will also find all the necessary information to install and configure your yadm.\n","permalink":"https://mischavandenburg.com/yadm-keep-track-of-your-precious-dotfiles/","summary":"This week I learned about yadm: yet another dotfile manager. It is the perfect way to keep track of all your custom configuration files, known as dotfiles.\nEven if you have only a little bit of experience with Linux, you know that everything is managed in files. All configuration parameters are set or changed in text files stored on the hard disk. These files are usually located in your home directory and are hidden by default.","title":"Yadm: Keep Track of Your Precious Dotfiles"},{"content":"I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.\nBefore this certification, I had only a little bit of experience. I deployed LAMP stacks using Ansible and configured VMs to be able to communicate with each other using only the command line. I also did a “Linux Fundamentals” video training. I could navigate the filesystem, edit text files and work in the terminal, but that was about it.\nIs it hard? For a beginner: yes, it was hard! But if you are a Linux administrator with a few years of experience, these exams probably are not very difficult to pass. However, even if you are experienced, be prepared to do a lot of memorization. Even though the requirements on the website seem very basic and straightforward, when you dig into the study resources, you will soon discover that you need to learn a large host of commands and many of their accompanying parameters. For example, you will need to know what grep -H does precisely, the difference between passwd -l, chage -l, and chage -L, the location of the directory that contains all the timezones, and the directory that contains the printer configurations for CUPS.\nThe certification The LPIC-1 certificate requires candidates to pass the LPI 101-500 and 102-500 exams. These exams test the candidates on various subjects, such as file management, boot loaders, networking fundamentals, user and group management, file systems and partitioning, and much more.\nEach exam has a 600-page syllabus, so to get your LPIC-1 certification, you need to work through 1200 pages and memorize a few hundred commands and parameters. However, if you work as a Linux sysadmin, you’ll probably know many of these commands and concepts.\nStudy Materials I attended a 4-day course that covered both exams. However, because of the large amount of information that needs to be covered, the teacher could only address the subjects on a superficial level. Therefore, I would advise you to be suspicious of any courses that promise to prep you for the exams in 4 days if you are a beginner. I estimate that you need at least double that amount to get some proper explanation of the material.\nLPI Syllabus After finishing the course, it became clear that I needed a lot of studying to pass the exams. Fortunately, LPI has created a syllabus for each exam. These are available for free on the LPI.org website.\nLPI Practice Exams It is crucial to test your knowledge. This is the resource I used:\nLPIC-1 Linux Professional Institute Certification Practice Tests: Exam 101-500 and Exam 102-500 If you are a member of O’Reilly’s, you can read the book there. It contains around 90 practice questions for every chapter in the LPI syllabus. The questions test your knowledge in detail and are a great way to determine whether you have fully grasped the material.\nHowever, the book was written in 2019 and contains questions about certain subjects that have since been removed from the exams. So if you suddenly encounter questions that do not seem familiar at all, make sure to double-check that it is actually an exam objective.\nLastly, I used these practice exams on Udemy.\nMemorization As I have stated before, the exams require you to do a lot of memorization. Fortunately, we have some tools and techniques available to help us with this task.\nThe primary tool is Anki. If you are not familiar with it, Anki is a very simple and free program that allows you to create flashcards that you can use to study and test your knowledge. The best thing about Anki is that it implements spaced repetition. You can download and learn more about Anki here: https://apps.ankiweb.net\nSecondly, I am fond of memory techniques. You can remember things much more quickly by visualizing them in your mind or utilizing techniques such as Memory Palaces or the Method of Loci. If you are interested in learning more about memory techniques, I highly recommend Dr. Anthony Metivier’s YouTube channel.\nStrategy Here is the strategy that I used to pass the exams:\nRead through a chapter and take notes. Make Anki flashcards for all the commands and flags that you do not know yet Do the exercises at the end of the chapter Do the practice exam for your chapter from the exam book, which should give you a good indication of how well you have grasped the material. Make flashcards of all the questions that you answered wrong (trust me, there will be quite a few) Use Anki to test yourself and memorize all of the commands and exam questions Tips: Do your Anki reviews every day. On some days I was adding more than 100 new cards, which will lead to a lot of reviews in the coming days Although the syllabus for exam 101 explained things very well, the 102 syllabus sometimes is very meager in its explanations and you might need to supplement with reading man pages, YouTube videos, and other tutorials. For example, I needed to find quite a bit of supplementary material for chapter 109 Networking Fundamentals. Ask for help if you don’t understand a certain topic Don’t think you can get away with skipping a topic. You will be tested on absolutely everything that’s in the syllabus, trust me. Try doing it together with someone else. I was doing it together with my friend and colleague, and it was extremely useful to be able to share things I struggled with and to discuss things with him to understand them better. Thank you for the good times, Gino! My thoughts on the certification The subject matter is extensive, and I know my way around Linux much better now. Therefore, if I encounter a problem, I am better positioned to assess where the cause might be and then solve the problem from there. I also feel I have a much better grasp of basic networking concepts, which will prove to be very useful in many situations in my work as a DevOps Engineer.\nHowever, there are also a few drawbacks to this certification. I think there is too much emphasis on memorizing commands and their flags. I think it is not necessary to memorize all of the possible parameters of the chage command because, in the real world, I would take a quick look at the man page to find the parameter that I need. The exams force you to memorize many parameters in a short time, and to be honest, you will probably forget about them very quickly anyway.\nBut overall, I am pleased and grateful to my employer that I was able to obtain this certification, and it has made me hungry for more, and I am very eager to continue my learning in this domain.\n","permalink":"https://mischavandenburg.com/lpic-1-study-guide/","summary":"I recently obtained my LPIC-1 certification, and in this blog post, I’ll share the strategy and techniques I used to pass this exam and share my thoughts on the certification. Because I am a Linux novice, the exam was a pretty tough grind for me. This article offers a beginner’s perspective on the LPIC-1 certification. Is the LPIC-1 hard to pass? Keep reading to find out.\nBefore this certification, I had only a little bit of experience.","title":"LPIC-1 Study Guide"},{"content":"Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!\nProgramming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.\nGitHub is a place where many open source projects are hosted. Projects are hosted in “repositories” available to the public. Everyone can go in and take a look at the code. And the great thing about it is that everyone can contribute to the code as well.\nTwo months ago I knew almost nothing about GitHub. Surely, I had often downloaded software from GitHub, and I knew it had to do with version control. But I had no idea that it was such a powerful system of enabling collaboration for software projects.\nDuring an assignment in my DevOps Traineeship I spent some time learning about Github and the Git language. I learned about repositories, branches, commits and pull requests. Now I wanted to take it to the next level and make a contribution of my own somewhere.\nthe project As I have mentioned in other posts, I love game automation, and recently I discovered the Botty project, which is a bot written for the game Diablo 2: Resurrected. The bot is written in Python, which means that it is a great way of applying my Python learning to something I am passionate about.\nThe bot uses computer vision in order to recognise what is on the screen and run scripts accordingly. The monsters in the game drop items, and if you want the bot to pick up items, it will need to be taught which items it needs to pick up.\nThis is done by adding some images to its image database and adding the filenames to a list of items. When the bot scans the screen for items, it will look for a match in its image database, and when it matches, it will click the corresponding pixels on the screen to pick up the item.\nHere’s what an image in the database looks like:\nI am doing a Holy Grail project in this game, which means that I am collecting every item in the game. It is quite an undertaking as there are 506 items in the game, and some items have a drop chance of 1 : 1.000.000. Luckily I have a bot to help me with this project.\nNot surprisingly, many items were still missing from the bot because it is a fairly new project that is still in development. And as I needed my bot to pick up the items I needed, I decided to add these 46 missing items to the database.\nforks, commits and pull requests After doing the work I still needed to figure out how I should offer these items to the project. Luckily someone shared a few very helpful tutorials in the project’s discord. This is the tutorial I used for my first contribution:\nStep-by-step Guide to Contributing on Github\nYou begin with “forking” the project repository, which basically means making your own copy of all the code in the project. Then you add your contribution to the fork by cloning it to your local machine and making your changes to a new branch.\nWhen you have committed your changes and pushed your new branch to your fork, you are ready to make your first pull request. A pull request is a way of telling the project that you have something to contribute. You are sharing your version of the project repository including your proposed changes, and someone from the project will take a look at your suggestions and see if they are useful and compatible.\nmerged into master After a few days someone had a look at my contribution and requested me to make a few small adjustments. When I managed to incorporate those my contribution was accepted, and my changes were “merged into master”, which means that my contribution was brought into the main version of the project’s code.\nI learned a lot! As I am typing out this article, I am very satisfied with how much I have learned in the past few months already. I remember being very confused about all the GitHub terminology when I attended my first meetings during my DevOps traineeship.\nGoing through the process of making a contribution to open source on GitHub has been an enriching experience. It seemed quite intimidating at the start, but by following a good tutorial I managed to successfully submit my first pull request. I feel I have a much better understanding of Git, GitHub and the workflow.\nAnother valuable lesson I learned is that you don’t need to be a Senior Engineer in order to be able to contribute to open source. Although this project is written in Python, my contribution had very little to do with code, but I provided assets which were required by the code. So if you are a beginner at programming, you can look for other ways to contribute, such as fixing spelling mistakes in the documentation, providing images or writing wiki pages.\n","permalink":"https://mischavandenburg.com/my-first-contribution-to-open-source/","summary":"Two months ago I knew nothing about GitHub. This week my first pull request got merged into master!\nProgramming tutorials and books very often suggest that you should try to contribute to open source in order to practice your skills. Even though I am still on the beginner level in Python, I managed to find something I could contribute with. But there were a few things I needed to learn in order to be able to do so.","title":"My First Contribution to Open Source"},{"content":"The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life. In this case, I needed to write a program that parses log files from a bot so I could get a total number of runs. You can have a look at the final result in my Diablo 2 GitHub repo. Like I wrote in my journey into DevOps article, I love automating games. Diablo 2 is a game that was originally released in 2002 and which recently was remastered. Diablo 2 always had a very strong presence of bots in the online game, and it didn’t take long before I also joined the ride.\nA few months after the remaster the first bots have started emerging as well. There is a a particularly good one written in Python which is an open source project, which is a perfect opportunity for me to learn more about Python by trying to understand its code and solving problems. I was very excited to discover it because I was playing quite a few hours a week. Diablo 2 is a very grindy game and it takes a lot of time to find the needed gear. Now I could finally outsource my grinding to the computer again.\nthe problem Although the bot is very functional and does several tasks very well, there are still features missing because it is relatively early in its development. One of these features is keeping track of the total amount of runs that the bot has done. In Diablo, every time you play you start a “game” or an instance. In that game there are certain bosses you can kill, and when you are finished you exit your game. This is called a run. Then you create a new game and everything is reset, and you get another shot at killing the bosses to get the precious gear.\nBeing the nerd that I am, I like to keep track of the total amount of runs that the bot has done. On these numbers I like to apply some calculations to see how many items I get per xxxx runs and suchlike. The bot keeps track of the amount of runs it does per session and stores them in a log file. But there is no functionality of seeing the total amount of runs you have done, and when I discovered this, I realised I had my first little Python project.\nlog files Every time you close the bot after a session, a log file is created that looks like this:\nIt is formatted as a .txt file and shows information about the bosses that were run and the items that were found. Most importantly, it contains the amount of games that were done in the session. Even after only using the bot for a short time I had over 100 log files to go through, and that’s where I needed a script that would go through these files for me and add the numbers in order to get the total amount of runs.\nthe script After completing chapter 9 and 10 in the Automate the Boring Stuff book, I learned about file paths and opening files and reading from them. Now it was time to apply that knowledge. The process went pretty well and soon I had a script that would open the files for me.\nThings got a little bit more complicated when I needed to read information from the files. And of course, this operation would almost certainly involve the dreaded topic of regex. In the end it was not as bad, and I ended up with the following regex:\nregex_games = re.compile(r\u0026#34;Games:\u0026#34;) regex_dict = { \u0026#39;nihla\u0026#39;: \u0026#39;Nihl|Nihlatak\u0026#39;, \u0026#39;pindle\u0026#39;: \u0026#39;Pin|Pindle\u0026#39;, \u0026#39;eldritch\u0026#39;: \u0026#39;Eld\u0026#39; } As you will see afterwards, I needed a way to check every line for a certain statement. However, rather than hardcoding every operation, I wanted it to loop over a list of terms. This meant that I could easily go back to the code and add a few more search statements if I needed them. I ended up storing them in a dictionary as you can see above. I really like the way you can make dictionaries in Python and have every entry on a new line. It makes the code very readable and structured.\nAnd this is the actual looping sequence that I ended up with:\nfor folder_name, sub_folder, file_names in os.walk(source): for filename in file_names: p = PurePath(folder_name, filename) with open(p, \u0026#39;rt\u0026#39;) as my_file: # search for games number line for line in my_file: # find number of games and add to total games if regex_games.search(line): g = line.split() total_games += int(g[1]) f = my_file.read() # check which runs were done by using the regex dict for key in regex_dict: location = regex_dict[key] reg = re.compile(location) # if there is a match, add the numbers to the total variable if reg.search(f): var_name = key + \u0026#39;_total\u0026#39; globals()[var_name] += int(g[1]) This sequence loops through the folder, the subfolder, and opens each file one by one. When the file is opened it looks for the “Games: 25” line and adds the number to a variable. However, I was not only interested in the total number of games. I also wanted to get more insight in how many Pindle runs or Nihla runs I had done. So I set up another regex search and made sure that the number of games are added to a “pindle_total” or “nihla_total” variable.\nresult When running the script in the shell, the result looks like this:\nmischa@MischaMacBook stats_parser % python3 total_runs.py Total runs: 7159 Pindle runs: 6926 Eldritch + Shenk runs: 367 Nihla runs: 232 mischa@MischaMacBook stats_parser % Exactly what I wanted. Now I can just paste my stats files into a folder and see how many runs I’ve done. Maybe I’ll improve it by building a GUI. Another fun idea I have is to create a little pipeline where this script would be run once an hour and the stats would be uploaded to a webpage somewhere, so others could see the amount of runs of my bot. Not that anyone is interested in that, but it is a fun project for me to do. Let’s see what happens!\nFor now I am very happy with the result. It was a very satisfying experience to identify a problem that I had and to be able to come up with an automated solution. Of course it is still very rudimentary programming, and there is a long long way ahead of me, but it was fun to finally do something practical that solved a particular problem in my life.\nThe final result is in my Diablo 2 GitHub repo.\n","permalink":"https://mischavandenburg.com/my-first-useful-python-script/","summary":"The best part of learning Python is trying to identify things in my life which I can automate by writing a script. Learning a programming language involves doing a lot of exercises that sometimes lack a connection with the real world. But after I decided to go for it, I am always on the lookout for projects. Not only for my job as a DevOps Engineer, but also for my private life.","title":"My Mirst Useful Python Script"},{"content":"I am currently working through the book Automate the Boring Stuff by Al Sweigart . I can already highly recommend it to anybody who is learning Python.\nChapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.\nhere is the full assignment text: Mad Libs Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file. For example, a text file may look like this: The ADJECTIVE panda walked to the NOUN and then VERB. A nearby NOUN was unaffected by these events. The program would find these occurrences and prompt the user to replace them. Enter an adjective: silly Enter a noun: chandelier Enter a verb: screamed Enter a noun: pickup truck The following text file would then be created: The silly panda walked to the chandelier and then screamed. A nearby pickup truck was unaffected by these events. The results should be printed to the screen and saved to a new text file. Looks pretty simple, right? I went into it with a lot of zeal and started writing a long list of if statements. My first attempts at the solution involved matching the words NOUN and ADJECTIVE directly, like so:\nif word == \u0026#39;ADJECTIVE\u0026#39;: inv = input(\u0026#39;Enter an adjective: \u0026#39;) However, this is problematic because as you can see, the sentence can contain words with a period attached, such as “VERB.” in the above example.\nno, please no regex! I’ve understood that here is a general anxiety around regex. I have certainly noticed it in myself and some of my junior engineer friends. As soon as I read the word regex, or realise that an assignment is going to involve regex, I get a constricting feeling in my throat and a rise in my heart rate.\nI’ve had to struggle with it quite a bit during my freeCodeCamp Front End Development Certificate, and the memories are still fresh in my mind.\nSo, like any other ‘rational’ human being, I tried to approach this relatively simple assignment with all sorts of ways trying to account for a period ending the word:\ndef period_check(x): for letter in x: if letter == \u0026#34;.\u0026#34;: return True # loop over the array and prompt user for word in source_text: period = False if period_check(word): word = word.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;) period = True if word == \u0026#39;ADJECTIVE\u0026#39;: invoer = input(\u0026#39;Enter an adjective: \u0026#39;) if period: result_text.append(invoer + \u0026#34;.\u0026#34;) period = True else: result_text.append(invoer) elif word == \u0026#39;NOUN\u0026#39;: It’s quite funny to see the lengths we go through to avoid regular expressions. However, as you maybe deduce from the code above, it didn’t work, and after a couple of hours of fiddling I gave up, and like any other frustrated programmer, I started to google.\nI quickly found solutions to this assignment and they all involved regex, and I realised I could not walk away from my fears anymore.\nan elegant solution Eventually I ended up with the following result for the part of my assignment that needed to recognise and replace the words with the user input. Of course I heavily borrowed from my Google search results.\n# set up and match the regex grammar_regex = re.compile(r\u0026#39;ADJECTIVE|NOUN|VERB|ADVERB\u0026#39;) match_regex = grammar_regex.findall(source_text) # replace the matches with user input for i in match_regex: ask_user = input(\u0026#39;Please enter \u0026#39; + i + \u0026#39;: \u0026#39;) source_text = source_text.replace(i, ask_user, 1) When I say elegant, I mean elegant in total Python beginner terms. I’m sure there are enough Seniors out there who will burst out laughing when they see this. But to me, it was quite an eye-opening experience to see this little piece of code do exactly what I had intended to achieve with 3 different functions and long blocks of if statements.\nAlso, I was pleasantly surprised with how simple regex can be in Python. In this case there were no scary [Az ^**/!!${}aa{}aA{nF}] statements. We simply defined which words we wanted and called the findall() module to generate a list with all the matches.\nThen we iterate over the list of matches and for each match we ask the user for the desired word, and replace it in the source_text.\nfinal result Having sorted out the pattern matching and replacing part, it was only a matter of implementing reading from files and writing to a new file.\n# Automate the Boring Stuff chapter 9 # Mad Libs assignment # Mischa van den Burg from pathlib import Path import re # ask the user which file to open file_name = input(\u0026#39;Enter the filename. For example, grammar.txt: \u0026#39;) # my script and .txt file are located in ~/python/automatetheboringstuff/ text_file = open(Path.home() / \u0026#39;python\u0026#39; / \u0026#39;automatetheboringstuff\u0026#39; / file_name ) # read the file and store in variable \u0026amp; close source_text = text_file.read() text_file.close() # set up and match the regex grammar_regex = re.compile(r\u0026#39;ADJECTIVE|NOUN|VERB|ADVERB\u0026#39;) match_regex = grammar_regex.findall(source_text) # replace the matches with user input for i in match_regex: ask_user = input(\u0026#39;Enter \u0026#39; + i + \u0026#39;: \u0026#39;) source_text = source_text.replace(i, ask_user, 1) # write to the new file and print the result new_file = open(\u0026#39;new_\u0026#39; + file_name, \u0026#39;w\u0026#39;) new_file.write(source_text) new_file.close() print(source_text) I was getting into some better functionality, such as accounting for existing filenames, and making the pathing relative so it could be run from anywhere. But I decided to save that for a later assignment.\nThe assignment was clear and did not require such functionality. I need to learn to keep things simple, and I decided to do just what I was asked and not go into any other rabbit holes.\nLessons Learned All in all the assignment is pretty simple, but I learned surprisingly much from it. I decided I’ll need to change and learn to love regex rather than fear it, because it showed me how powerful it can be.\nAlso, I got some insight into my own mind and how I tend to work. I realised I have a tendency to make things much more complicated than they need to be. I need to learn to keep things simple.\n","permalink":"https://mischavandenburg.com/python-project-mad-libs/","summary":"I am currently working through the book Automate the Boring Stuff by Al Sweigart . I can already highly recommend it to anybody who is learning Python.\nChapter 9 is about reading and writing files, and there are two assignments at the end of the chapter. Here I’ll discuss my solution of the Mad Libs assignment.\nhere is the full assignment text: Mad Libs Create a Mad Libs program that reads in text files and lets the user add their own text anywhere the word ADJECTIVE, NOUN, ADVERB, or VERB appears in the text file.","title":"Python Project: Mad Libs"},{"content":"In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.\nI wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.\nYou can have a look at the Github repo with the result here. The repo is using the “ansible-galaxy init” role structure. You will find the playbooks as follows: roles/your_choice/tasks/main.yml\nDocker I was very excited to learn more about Docker and containerisation. I was familiar with the concept of virtualisation, which is creating virtual versions of fully functional machines on a host operating system. But the concept of containerisation was new to me.\nAs I understand it, containerisation differs drastically from virtualisation because containers are able to use resources from host directly. They do not need an entire operating system to run, and therefore they are a much more lightweight.\nThis means that resources can be used much more efficiently which eventually can mean cost reduction in your cloud infrastructure.\nDocker is a very popular platform for building and running containers. It seemed like the best option to get started with deploying my own containers.\nLEMP Stack My colleague recommended me this tutorial to become more familiar with Docker. It uses a LEMP stack as an example application. When I told friends about the fact that I was building a LEMP stack, they corrected me and said it was a LAMP stack.\nThe LAMP stack is a collection of software built out of these elements:\nL – Linux: the operating system\nA – Apace: webserver\nM – MySQL: database\nP – PHP: server scripting language\nHowever, in a LEMP stack, we use NGINX as a webserver, which is pronounced “Engine X”, hence the E in LEMP stack. Therefore, LEMP is the correct way to spell it, and it is used in all the tutorials that I have been using.\nI highly recommend the tutorial in order to learn how to deploy your first collection of containers. Deploying one container is relatively easy with Docker, but it gets a little more complicated when deploying several containers and making them communicate with each other in order to combine them into one application. But this tutorial does a great job at showing you how it’s done and it is especially good at explaining the steps along the way.\nDocker Compose vs. Ansible Docker Compose is a tool you can use to run multi-container applications. With the help of the tutorial, it was fairly easy to understand and hit the ground running by deploying multiple containers into one network.\nNow let’s have a look at how we actually set up the containers. In the Docker Compose file, the NGINX container was defined like this:\nversion: \u0026#39;3.8\u0026#39; #Services services: #Nginx Service nginx: image: nginx:1.19 ports: - 80:80 volumes: - ./src:/var/www/php - ./.docker/nginx/conf.d:/etc/nginx/conf.d depends_on: - php It looks pretty straightforward, right? Almost like pseudocode. We tell Docker which image to pull from the Docker Hub, and we tell it to route container’s port 80 to our host’s port 80. This ensures that the web server can be accessed from the outside, provided you have opened this port in the firewall.\nNext there is the volumes section: this mounts certain directories on the host into the container so it is accessible. In this case this was necessary to transfer the web server configuration and the index.php which we wanted to serve to the outside.\nHaving successfully deployed my LEMP using Docker Compose, the next step was to automate this process by using Ansible. Ansible is a very powerful tool which enables you to automate configuration management and application deployment by writing scripts called playbooks.\nWhy was it necessary to introduce Ansible? By using Docker Compose, you would need to have Docker and Docker Compose installed on the virtual machine before you could start running the containers.\nHowever, Ansible gives you the power to take a completely fresh virtual machine, configure it from scratch, and install Docker and its necessary dependencies, followed by deploying the containers.\nNow let’s take a look at the same container defined in Ansible: - name: start nginx docker_container: name: nginx image: nginx:1.19 detach: yes ports: - 80:80 networks: - name: network_one volumes: - /src:/var/www/php - /.docker/nginx/conf.d:/etc/nginx/conf.d Although there are some differences, they look very similar. Converting my Docker Compose file to an Ansible playbook was quite a natural and easy experience. It also helps that both are written in YAML and therefore use the same indentation conventions.\nA few differences we can observe:\nIn the Ansible playbook we invoke the docker_container module, whereas they are defined as services in the Docker Compose file. Another difference is that we need to set up the network ourselves. In the Docker Compose file, we just specified the containers and Docker Compose created a network automatically and made sure that all containers were connected to it.\nHowever, it isn’t very complicated in Ansible either:\n- name: setup network docker_network: name: network_one We simply call the docker_network module and tell it to make a network called network_one. All we need to do then is make sure to set the networks: parameter to network_one in the docker_container module as we saw above.\nThe last point to note is the detach parameter. This means that the container will keep running in the background after it is started.\nResult After some debugging here and there and making sure all of the elements were in place, eventually we get the satisfying message that everything went according to plan:\nThe result is a webpage being served on the server ip:\nI know, it is not the prettiest or most intricate design. But remember that I am working towards becoming a DevOps Engineer, not a Front End Developer 😉\nWe can also enter the phpMyAdmin dashboard by adding port 8080 to our ip in the browser:\nConclusion The assignment of deploying a LEMP stack in separate containers has been very useful and I learned a lot from the process. There were a few more modules that needed to be configured in Ansible as opposed to the Docker Compose method, but the tradeoff is that Ansible is much more powerful and enables you to configure the server from scratch. You can have a look at the code in the GitHub repo to see all of the changes I needed to do.\nThe only part that I needed to do by hand is to create the VM in the Microsoft Azure portal, open the ports and configure the SSH keys. The next step in my learning process will be to learn how I can automate this step as well. This means that I will need to learn Terraform.\nBy using Terraform I will be truly deploying this stack as Infrastructure as Code, but doing all of these steps with Ansible has given me a much better understanding of Infrastructure as Code already.\n","permalink":"https://mischavandenburg.com/docker-lemp-stack-deployed-with-ansible/","summary":"In order to learn more about Docker and Ansible I am working on an assignment to take an existing application and to break it down into containers. However, in order to be able to understand this process properly, I first needed to understand more about Docker and containerisation.\nI wrote a playbook that installs Docker and deploys a fully containerised LEMP stack on a virtual machine.\nYou can have a look at the Github repo with the result here.","title":"Docker LEMP Stack deployed with Ansible"},{"content":"I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.\nI intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect. Eventually I decided to just use my full name.\nAlthough I have zero connections with Russia, neither genetically nor culturally, my parents decided to give me a name of Russian origin: Mischa.\nAccording to this website, Mischa has the following meaning:\n“The name Mischa is primarily a gender-neutral name of Russian origin that means Who Is Like God“\nAlthough I admire my parents for giving me such an ambitious name, I must confess that I turned out to be of a much more earthy and less godly nature.\nWhen I was younger, an old gymnastics teacher once told me that Mischa meant “bear” in Russian, and I always liked that connotation much more. I also turned out to be more bear-like than god-like, being 190cm tall and having thick and bristly curly hair.\nIndeed, this post on Quora confirms that my name indeed has the meaning of bear:\n\u0026ldquo;In Russia Mishka sometimes also used to denote a bear, particularly a bear cub.\u0026rdquo; Misha Sivan, Born in USSR.\nAlthough I am conveniently ignoring the “cub” part of his explanation, I thought it was pretty cool that my actual name had connotations with our ursine friends.\nMoreover, I lived 9 years in Norway where I roamed the mountains for weeks at a time, just me and my tent and a fishing rod. I developed a very close connection with nature during those years. Not only its beauty, but also its merciless forces and awe-inspiring ingenuity.\nAlthough I never encountered a bear myself, they are most definitely present in the Norwegian nature. One time the newspapers told me that there was a bear sighted 4 kilometers from where I was camping one weekend.\nBut this is what the bear represents to me: the time I was allowed to spend in the North and the resulting connection with nature. The solitary character of the male, calmly roaming for many miles in search of sustenance. A force to be reckoned with when aggravated, but in general preferring to stay at a distance.\n","permalink":"https://mischavandenburg.com/why-i-chose-a-bear-for-my-logo/","summary":"I always struggled with finding an online nickname for myself. I was never given one, and over the years I used a few here and there, ranging from Nordic gods to my favourite fantasy character: Gandalf the Grey.\nI intended to create a tech blog to document my journey ever since I started my career change. But for months I was stopped by trying to decide on a name. (That’s me in a nutshell), because it needed to be perfect.","title":"Why I Chose a Bear as a Logo"},{"content":"In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.\nI was in a fortunate situation, because there were large shortages of people on the IT job market in The Netherlands. Consequently, employers were much more willing to train their employees to perform the roles that they required to fulfill.\nBots and Scripts I have always loved messing around with computers, and I learned to write small and simple programs at a very young age. At the same time I was an avid gamer, and spent many hours slaying monsters in online RPG’s. One day I came across the concept of a bot: a program that plays the computer game for you, and I was hooked. This is where I developed my ‘fetish’ for automation.\nI was very lucky that I had a friend who shared my interests, and together we built our own automation projects (called ‘botfarms’) in which we ran large amounts of bots that played a certain game for us. This army of bots generated in-game currency which we could sell for actual money. They weren’t huge profits, but it was an amazingly satisfying feeling to be the overlords of an army of automations that actually generated some income for us.\nFrom Bots to DevOps These botfarms were hosted on servers which set up ourselves. In order to save costs we rented Linux servers, and I spent many evenings figuring out how configure them via the command line. Often I would suddenly snap out of my flow at 3am and realise I had to go to work at 7 in the morning again.\nAlthough I did not manage to make these personal interests into a personal career, my friend eventually became a Data Engineer. After making the decision to make a career switch to IT I needed to figure out which direction I wanted to go in, because IT covers a very broad range of topics and skills. Based on on our shared interests and previous projects, he recommended me to become a DevOps Engineer.\nI very quickly realised that he was right on the money with his suggestion and I started to become very enthusiastic to learn how to make a living by working with computers and automation.\nTraineeship As I mentioned before, employers in The Netherlands are now willing to train potential candidates, and I used my hobby projects as a way to demonstrate my genuine interest and affinity with IT and automation. I was offered a traineeship to become a DevOps Engineer in 2021. After a period of training I started working for the City of Amsterdam and I\u0026rsquo;ve been part of an IT4IT operations team since.\n","permalink":"https://mischavandenburg.com/my-journey-into-devops-so-far/","summary":"In 2021 I had reached a certain stage in my life where I had the liberty to make a choice. I was 31 years old and had just finished an important chapter of my life, and was ready to begin a new one. After thinking about where I wanted to steer my professional career, I decided to follow my passion for programming and automation and attempt to make a career out of my interests in tech and IT.","title":"My Journey Into DevOps So Far"},{"content":"","permalink":"https://mischavandenburg.com/freedom/","summary":"","title":""},{"content":"","permalink":"https://mischavandenburg.com/onboarding/","summary":"","title":""},{"content":"","permalink":"https://mischavandenburg.com/screen/","summary":"","title":""},{"content":"","permalink":"https://mischavandenburg.com/skool/","summary":"","title":""},{"content":"","permalink":"https://mischavandenburg.com/startyourskool/","summary":"","title":""},{"content":"Let\u0026rsquo;s get in touch!\nIf you\u0026rsquo;re on mobile, use this link:\nhttps://calendar.app.google/rHynGWsKXLb6GJwM6\nSign Up For My Free Newsletter Email You may also email me: info [at] kubecraft.nl\nMentoring 60 minute call You can also use this link\n30 minute call Or use this link\nSocial Media 📚 My Skool Community\n🐦 Twitter - X\n💻 LinkedIn\n💾 GitHub\n🎥 YouTube\n","permalink":"https://mischavandenburg.com/contact/","summary":"Let\u0026rsquo;s get in touch!\nIf you\u0026rsquo;re on mobile, use this link:\nhttps://calendar.app.google/rHynGWsKXLb6GJwM6\nSign Up For My Free Newsletter Email You may also email me: info [at] kubecraft.nl\nMentoring 60 minute call You can also use this link\n30 minute call Or use this link\nSocial Media 📚 My Skool Community\n🐦 Twitter - X\n💻 LinkedIn\n💾 GitHub\n🎥 YouTube","title":"Book a Meeting"},{"content":"Accelerate Your Tech Career with Personalized Coaching\nAre you ready to take your tech career to the next level? As a Microsoft MVP, Cloud Native Engineer, and successful career changer, I offer personalized coaching to help you achieve your goals in the world of DevOps, Cloud Computing, and Kubernetes. What I Offer:\nTailored Career Roadmaps: Together, we\u0026rsquo;ll create a personalized roadmap to guide your tech career journey, whether you\u0026rsquo;re just starting out or looking to level up. Hands-on Kubernetes and Cloud Native Expertise: Benefit from my deep knowledge of Kubernetes, AKS, and cloud-native technologies to accelerate your learning and practical skills. Productivity Mastery: Learn how to build your own Second Brain and Zettelkasten system, optimizing your learning and information management for long-term success. Certification Strategy: Leverage my experience in obtaining key certifications (CKA, AZ-104, AZ-400) to create an effective study plan and boost your credentials. Real-world DevOps Insights: Gain practical insights from my experience as a DevOps Engineer, helping you bridge the gap between theory and industry practice. Mindset and Motivation: Develop the mindset of continuous learning and improvement that\u0026rsquo;s crucial for thriving in the fast-paced tech world. Why Choose My Coaching? Proven Track Record: I\u0026rsquo;ve successfully transitioned from a non-tech background to a thriving career in Cloud and DevOps. Practical Approach: My coaching is based on real-world experience, not just theory. Holistic Development: We\u0026rsquo;ll focus on technical skills, career strategy, and personal productivity. Personalized Attention: Get tailored advice and support that goes beyond what\u0026rsquo;s possible in group settings or courses. Whether you\u0026rsquo;re looking to break into the tech industry, master Kubernetes, or enhance your DevOps skills, my personalized coaching will provide the guidance and support you need to succeed. Book a session now and let\u0026rsquo;s accelerate your tech career together!\n60 minute call Pre-Coaching Session Questionnaire\nTo prepare for our session(s) optimally, please fill out the following template and include it in the booking.\nYou do not need to fill out everything.\nThese questions are crafted to help you get clarity. I want you to get the most out of our time together.\nName:\nCurrent Role/Position:\nYears of Experience in Tech:\nWhat are your top 3 personal \u0026amp; career goals for the next 6-12 months?\na)\nb)\nc)\nWhich specific areas of technology are you most interested in developing? (e.g., Kubernetes, Azure, DevOps practices)\nWhat courses or modules from our community have you completed so far?\nWhat aspects of the course material are you finding most challenging to implement or understand?\nDescribe your current study routine or learning process:\nWhat obstacles or challenges are you currently facing in your tech career or learning journey?\nHave you attempted to build your own Zettelkasten or Second Brain system? If yes, what challenges are you facing?\nOn a scale of 1-10, how confident do you feel about your current career trajectory? Why?\nWhat specific skills or knowledge do you feel you\u0026rsquo;re missing to reach your next career goal?\nAre there any particular projects or certifications you\u0026rsquo;re working towards?\nHow much time can you realistically dedicate to learning and skill development each week?\nWhat\u0026rsquo;s your preferred learning style? (e.g., hands-on practice, theoretical study, project-based learning)\nAre there any specific topics or questions you\u0026rsquo;d like to address in our coaching session?\nWhat does success look like to you after our coaching sessions?\nIs there anything else you\u0026rsquo;d like me to know before our session?\nLooking forward to our session!\nMischa van den Burg\nYou can also use this link\n30 minute call Or use this link\n","permalink":"https://mischavandenburg.com/coaching/","summary":"Accelerate Your Tech Career with Personalized Coaching\nAre you ready to take your tech career to the next level? As a Microsoft MVP, Cloud Native Engineer, and successful career changer, I offer personalized coaching to help you achieve your goals in the world of DevOps, Cloud Computing, and Kubernetes. What I Offer:\nTailored Career Roadmaps: Together, we\u0026rsquo;ll create a personalized roadmap to guide your tech career journey, whether you\u0026rsquo;re just starting out or looking to level up.","title":"Coaching"},{"content":"I create video courses about Kubernetes, DevOps, Neovim, Terminal Workflow, Zettelkasten, Notetaking and productivity.\nYou can gain unlimited access to all of my courses by signing up to my Skool community.\nUdemy Alternatively, some of my courses are available on Udemy.\nKubernetes Masterclass for Beginners\nAdvanced Notetaking Techniques: Goal Setting with Obsidian\nStart Taking Notes in Obsidian in 1 Hour\nCommunity But why waste money on individual courses if you can gain UNLIMITED access to all of my courses and a community of likeminded learners?\nWhat my community is about:\n🤖 DevOps \u0026amp; Kubernetes\n💎 Next-level Productivity \u0026amp; Note Taking\n🚀 Accelerating your Career \u0026amp; Salary\n🤝 Enjoying the journey with new friends\nWhat You Get:\n💰 ALL Premium Courses: 25h of DevOps, Kubernetes, Productivity, Career ($2499)\n💰 24/7 Expert Support: Stop Googling, get real answers ($499)\n💰 3x Weekly Q\u0026amp;A Calls: Direct access to industry experts ($599)\n💰Vibrant Community: Grow and have fun with like-minded people (Priceless)\nBonuses:\n🎁 Get 1:1 Support for landing your next, or improving your job \u0026amp; salary from top 1% earners ($999)\n🎁 LinkedIn Connections: Network with 150+ members\nOver $4500 in value, Invest in yourself for less than $1/day.\nCourse Portfolio More being added every week Join Today You can get unlimited access to all of these courses, 20+ hours of Q\u0026amp;A recordings and a thriving community of learners and experts alike.\nWhat are you waiting for? You can get all of this value for less than $1 a day.\nCancel anytime.\nJoin Now.\n","permalink":"https://mischavandenburg.com/courses/","summary":"I create video courses about Kubernetes, DevOps, Neovim, Terminal Workflow, Zettelkasten, Notetaking and productivity.\nYou can gain unlimited access to all of my courses by signing up to my Skool community.\nUdemy Alternatively, some of my courses are available on Udemy.\nKubernetes Masterclass for Beginners\nAdvanced Notetaking Techniques: Goal Setting with Obsidian\nStart Taking Notes in Obsidian in 1 Hour\nCommunity But why waste money on individual courses if you can gain UNLIMITED access to all of my courses and a community of likeminded learners?","title":"Courses"},{"content":"On this page I share the things I\u0026rsquo;m currently focused on, which habits I\u0026rsquo;m tracking, and what my goals are.\nReading Currenlty reading The Longevity Diet\nAlso started reading the Polity series by Neal Asher, now reading the first book Gridlinked.\nI have never read much science fiction but I\u0026rsquo;m broadening my horizon a bit.\nWith the current AI craze going on it seemed like an interesting AI themed series.\nHere is a list of all the books I\u0026rsquo;ve read\nWeekly Activity Weight Loss I\u0026rsquo;ve been on a weight loss regime sinds late 2021. I\u0026rsquo;ve maintained a caloric deficit since I started, but in some periods life got in the way and I had breaks from the caloric deficit.\nSee:\n🍎 My Weight Loss Journey And Current Goal 📉\nDiet I eat a whole foods plant based diet. I\u0026rsquo;ve been vegan for almost 7 years, but I\u0026rsquo;ve had periods where I had a vegetarian diet. Sometimes I even ate meat for a short period during the winter. But I always feel best when I go full plant based.\nI\u0026rsquo;ve narrowed my diet down to fruits and vegetables, beans, brown rice, nuts and seeds.\nMacro division:\nCarbs: 33%\nProtein: 20%\nFat: 47%\nFasting I\u0026rsquo;ve been doing some form of Intermittent Fasting for over 7 years now.\nAfter a period of eating One Meal A Day (OMAD) for a few months, I\u0026rsquo;m back on an 8 hour eating window, starting at noon and ending at 8pm.\nDaily Habits Morning Light Viewing Popularized by Andrew Huberman. Get at least 10 minutes of sunlight directly into the eyes within 2 hours of waking up.\nExercise Goal is to get at least 30 minutes of exercise everyday, but I\u0026rsquo;m averaging much more. I averaged 66 minutes over the past 6 months, and 90 minutes over the past two months (as per 2023-05-26)\nCurrently I go for a fasted run right after I wake up in the morning. I aim to do this 6 times a week. After my run I do 10 minutes of pushups and other bodyweight exercises.\nIn the afternoon I do a session of Ashtanga Yoga mostly focussing on stretching my back and hamstrings.\nAfter dinner I go for a 20 minute walk.\nMeditation I practice formal sitting meditation 30-60 minutes a day. Sometimes I\u0026rsquo;ll go for several hours a day.\nThroughout my daily activities I strive to maintain continuous mindfulness.\nRunning has become a meditation as well, and I do that for about 45 minutes a day too.\nCleaning I keep my environment clean and clutter free because it aids in mental calm, focus and well being. I want my apartment to always be presentable to visitors at any time of the day.\nWork I\u0026rsquo;ve fallen in love with Kubernetes after I obtained my CKA.\nI\u0026rsquo;m currently getting a lot of Cloud Engineer experience in an Azure environment. I focus on Azure Kubernetes Service, but I also contribute with deploying and maintaining other resources such as Virtual machines, firewalls and Application Gateways.\nAfter getting my AZ104 and AZ400 during February, I don\u0026rsquo;t have any certifications on the horizon currently because I discovered the joy of running and I\u0026rsquo;m focusing more on health and exercise in my free time. But this will change soon, everything goes in phases with me. When I feel more established in my training routines there will be mental space for learning again.\nHealth Sleep I aim to sleep 7 hours a day. My 6 month average as per 2023-05-26 is 8 hours and 4 minutes.\nExercise See above :)\nSobriety I don\u0026rsquo;t drink alcohol or use any other mind-altering drugs. On 2023-07-04 I\u0026rsquo;ll be celebrating 5 years of sobriety.\nSpirituality I\u0026rsquo;m a practicing Buddhist. I found my religion in Therevadan Buddhism, and I found that the Thai Forest Tradition of Ajahn Chah suits me best. I\u0026rsquo;ve been on the 5 precepts for nearly 5 years now.\nQuotes that Inspire me \u0026ldquo;“I’m only a scribe,” I mumbled. “All I know is clay tablets.” “Nonsense!” he interjected. “You’re skilled in language and communication, which puts you far ahead of most. And remember this—the best investment you can make is in yourself. Continue to educate yourself and seek out wisdom, adding new skills and experiences that will make you more valuable.\u0026rdquo; - location: 169 ^ref-57645 - The Richest Man in Babylon\n\u0026ldquo;The Fifth Law of Wealth: Invest in yourself. Gain knowledge and skills to increase your earning power.\u0026rdquo; - location: 184 ^ref-12741 - The Richest Man in Babylon\n","permalink":"https://mischavandenburg.com/current/","summary":"On this page I share the things I\u0026rsquo;m currently focused on, which habits I\u0026rsquo;m tracking, and what my goals are.\nReading Currenlty reading The Longevity Diet\nAlso started reading the Polity series by Neal Asher, now reading the first book Gridlinked.\nI have never read much science fiction but I\u0026rsquo;m broadening my horizon a bit.\nWith the current AI craze going on it seemed like an interesting AI themed series.","title":"Current"},{"content":"As a Senior DevOps Engineer I have experience with setting up cloud native environments in large organizations.\nI have mentored junior engineers and enjoy sharing my knowledge with my team.\nI can help you with:\nDeploying and maintaning Kubernetes clusters Implementing GitOps workflows using ArgoCD \u0026amp; Flux Improving developer productivity through cloud-based workspaces Creating Azure Landing Zones Provisioning Azure Infrastructure through Bicep \u0026amp; Terraform Setting up or improving CI/CD pipelines with GitHub Actions \u0026amp; Azure DevOps pipelines Linux System Administration Let\u0026rsquo;s get in touch!\nIf you\u0026rsquo;re on mobile, use this link:\nhttps://calendar.app.google/rHynGWsKXLb6GJwM6\n","permalink":"https://mischavandenburg.com/engineering/","summary":"As a Senior DevOps Engineer I have experience with setting up cloud native environments in large organizations.\nI have mentored junior engineers and enjoy sharing my knowledge with my team.\nI can help you with:\nDeploying and maintaning Kubernetes clusters Implementing GitOps workflows using ArgoCD \u0026amp; Flux Improving developer productivity through cloud-based workspaces Creating Azure Landing Zones Provisioning Azure Infrastructure through Bicep \u0026amp; Terraform Setting up or improving CI/CD pipelines with GitHub Actions \u0026amp; Azure DevOps pipelines Linux System Administration Let\u0026rsquo;s get in touch!","title":"Engineering"},{"content":"This is an overview of the services I provide.\n1 on 1 Coaching \u0026amp; Mentoring I\u0026rsquo;m available for 1 on 1 coaching \u0026amp; mentoring\nVisit this page for more information.\nInterim Engineering I\u0026rsquo;m available for interim contracts for DevOps Engineering, Cloud Engineering and Kubernetes Engineering. Click here to learn more.\nTraining I provide virtual and on-site training. Book a meeting for more information.\n","permalink":"https://mischavandenburg.com/services/","summary":"This is an overview of the services I provide.\n1 on 1 Coaching \u0026amp; Mentoring I\u0026rsquo;m available for 1 on 1 coaching \u0026amp; mentoring\nVisit this page for more information.\nInterim Engineering I\u0026rsquo;m available for interim contracts for DevOps Engineering, Cloud Engineering and Kubernetes Engineering. Click here to learn more.\nTraining I provide virtual and on-site training. Book a meeting for more information.","title":"Services"},{"content":"Due to a lack of interaction, the free community has been closed.\nIf you are interested in getting direct access to Mischa and his fantastic community, check out the link below:\nhttps://mischavandenburg.com/skool\n","permalink":"https://mischavandenburg.com/free/","summary":"Due to a lack of interaction, the free community has been closed.\nIf you are interested in getting direct access to Mischa and his fantastic community, check out the link below:\nhttps://mischavandenburg.com/skool","title":"Skool Community"},{"content":"\nAbout me I’m Mischa. I’ve worked across various fields ranging from project management in the oil industry to health care, but my driving passion has always been IT. Fascinated since childhood, I’ve always passionately worked on projects that leveraged computers and automation to reach my goals.\nIn 2007 I started learning to code to automate the video games I used to play and to build my own websites. In 2014 I taught myself to use Linux and utilized these skills to run large-scale video game and social media automation projects to drive traffic to the websites I coded at night. I lived in Norway for 9 years, and when I moved back home to the Netherlands, I succeeded in following my dream and turning my passion into my career, and I’ve been working as a full-time DevOps Engineer since 2021.\nI fell completely in love with Kubernetes the first time I used it, and I am obessed with learning as much about Kubernetes as possible and sharing it with my audience. I passionately believe that the future of software deployment lies in containers and I specialize in building platforms and environments that allow efficient deployment and maintenance of containerized workloads.\nI\u0026rsquo;m in love with my work. After I finish my day\u0026rsquo;s work, I usually sit down to study more about my field. There is always something new to learn. Learning can take many forms. Very often I read or watch videos and courses and take notes. But you cannot learn everything from the books alone. You also need to learn by doing. I work on coding projects or building my Kubernetes home lab. I created a lab repo where I store all the files I use to explore these topics.\nI also run an extensive Kubernetes homelab which I document on my YouTube channel and all GitOps code can be found in this repo\nIn 2024 I was awared the Microsoft MVP title for my efforts of sharing knowledge with the community.\nAppearances Podcasts Azure Cloud Recruiting Simplified: Serverless, Kubernetes, and the Buzz with Mischa van den Burg\nhttps://www.youtube.com/watch?v=Uvcv4TVJwP0\nFavorites These articles came out particularly well, and might provide some value to you, or help you get to know me a little better.\nI\u0026rsquo;m In Love with my Work: Lessons from a Japanese Sushi Master My Neovim Zettelkasten: How I Take Notes in Markdown Using Vim and Bash How and Why I Started Using Vim Living On The Command Line I\u0026rsquo;m a nerd. I use neovim for all of my text editing. I have a fully customized setup with dozens of custom keybindings so I can work a little more effectively, and this blog is entirely written on the command line using vim.\nI love UNIX based operating systems and I\u0026rsquo;ve built my custom setup in Arch Linux. I wrote an article about how customizing and maintaining my tailored operating system is a creative activity to me.\nIn the words of Rob Muhlenstein, \u0026ldquo;Using the Linux Bash terminal command-line is coding. Every command you type on every line is part of an interactive, ongoing program interpreted by Bash and processed by Linux.\u0026rdquo; Having this dialogue with the computer through commands is one of the most fascinating things on this earth to me, and I feel fortunate that I can work with this every day as a DevOps engineer.\nAbout This Website When I changed my career to DevOps, I started this blog to document the journey I was embarking on. I\u0026rsquo;ve been sharing my notes on this blog and started writing articles about the stuff I was learning, and found it to be a very enjoyable practice.\nMy style is a mixture of micro-blogging where I share useful things and tricks I learn and long-form articles which are more refined.\nWhy bother with a personal website? Why not just write on social media?\nThis post captures it beautifully.\nWhy Take Notes? “Your professional success and quality of life depend directly on your ability to manage information effectively.”\nTiago Forte, Building A Second Brain\nIn the modern age we consume very large amounts of information, much more than we can remember. It is therefore very important to manage your intake of information, but also the retention of that information. I find that taking notes on the topics that may be of interest to me at any given time helps me understand the subject much better, because it forces me to clearly formulate my thoughts and convert them to written form.\nWhen I want to reflect on a certain topic or to write an article, I go through my collection of notes and link them together. Very often I find that the creation of these links will stimulate even more new connections and associations, and I end up with new ideas for even more topics of study or things that I want to write about. I use my Zettelkasten as a vehicle for reflection, learning and creativity.\nMany of these notes are avaialble in my public Zettelkasten,\nCode If you\u0026rsquo;re curious about the writing and publishing process of this blog, see this note.\nYou can find the source code of the blog at github.com/mischavandenburg/blog, and the contents of the zettelkasten are hosted in the zet directory at /blog/content/zet/\n","permalink":"https://mischavandenburg.com/aboutme/","summary":"About me I’m Mischa. I’ve worked across various fields ranging from project management in the oil industry to health care, but my driving passion has always been IT. Fascinated since childhood, I’ve always passionately worked on projects that leveraged computers and automation to reach my goals.\nIn 2007 I started learning to code to automate the video games I used to play and to build my own websites. In 2014 I taught myself to use Linux and utilized these skills to run large-scale video game and social media automation projects to drive traffic to the websites I coded at night.","title":"Welcome!"},{"content":"\nHi, I’m Mischa, a Cloud Native Engineer from Amsterdam. I\u0026rsquo;m a Microsoft MVP and managed to turn my career from nurse to DevOps Engineer in record time. I\u0026rsquo;ve documented my journey on my blog and YouTube channel, and I\u0026rsquo;ve started a Skool community where I teach the skills I needed to aqcuire to realize my dream of a career in IT.\nClick here to visit my Skool community!\n👋 Join us to master DevOps, Kubernetes, Note Taking and the art of getting hired with a high salary. Learn my 10x Productivity workflow that took me from Nurse to Microsoft MVP \u0026amp; soon Kubestronaut in just 2 years. This Skool combines educational material with an active, thriving community unlike anything else online.\nPremium Community gets:\n20h+ of Premium Courses: Note Taking, DevOps, Kubernetes (value $1997)\nQuick \u0026amp; reliable answers to your questions 24/7 (value $747)\n3x Weekly Q\u0026amp;A sessions with Mischa (value $1800)\nA searchable Knowledge Base (value $997)\nAccess to a vibrant community of experts \u0026amp; learners (Priceless)\nBonus:\n🎁 Mastering Soft Skills by Sales Expert; Get Hired, Maximize your Salary, Communication and LinkedIn (value $997)\n🎁 1:1 support for getting a (better) job (value $997)\n🎁 Connect with 140+ Members on LinkedIn!\nTotal value - $7,535. Lock in forever at just $19/mo.\n❗Price increasing soon to $29/mo (in a few weeks)\n❗At 500 memberships will be closed.\nClick here to visit my Skool community!\nAre you looking to break into the industry? I don\u0026rsquo;t have a CS degree, but I managed to make it in the industry anyway. Through my YouTube channel I\u0026rsquo;ve received countless of comments where people tell me that they find my journey inspiring.\nIf you are looking to break into the industry, this is the community for you.\nI have walked the path to change my career from nurse to Cloud Engineer. I know what to do and what you need to learn.\nWhen you look over a job posting for a DevOps engineer, it\u0026rsquo;s likely you\u0026rsquo;ll feel overwhelmed with all of the stuff you need to learn. In these moments you need someone to point you in the right direction. Me and the community are able to analyze your current situation and recommend the next step for you.\nWe are a community that has a healthy mix of seniors who are eager to help and juniors who are motivated to learn.\nConnect with fellow notetakers It\u0026rsquo;s likely that you know me from my videos on productivity and notetaking. Many people who join my community are engineers, but you don\u0026rsquo;t need to be an engineer to join.\nOne thing many of us in the community have in common is a fascination with productivity and notetaking, we regularly share our ideas with each other. Almost weekly someone will discover someone new and share it with the community.\nNot just Kubernetes Our community is such a nice place to hang out in, that people wish to join and stay even if they are not into DevOps and Kubernetes.\nDo you wish to level up your game as an engineer? My courses on Notetaking, productivity and soft skills can help you lift your career to the next level.\nEverybody has their weak points, and we can help you to address them. You might excel everyone around you on a technological level, but if you\u0026rsquo;re unable to communicate with your team members and stakeholders or if you are regarded as difficult to work with, you will stagnate and you will be stuck somewhere comfortable and stop growing.\nWe are here to help you with that.\nConnect with likeminded engineers. Get feedback on your solutions.\nClick here to visit my Skool community!\n","permalink":"https://mischavandenburg.com/test/","summary":"Hi, I’m Mischa, a Cloud Native Engineer from Amsterdam. I\u0026rsquo;m a Microsoft MVP and managed to turn my career from nurse to DevOps Engineer in record time. I\u0026rsquo;ve documented my journey on my blog and YouTube channel, and I\u0026rsquo;ve started a Skool community where I teach the skills I needed to aqcuire to realize my dream of a career in IT.\nClick here to visit my Skool community!\n👋 Join us to master DevOps, Kubernetes, Note Taking and the art of getting hired with a high salary.","title":"Welcome!"}]